{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Artificial Neural Networks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input(x) * Weight(w) * Bias(b) = output(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variants of Gradient Descent**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Batch Gradient Descent: Uses the entire dataset to compute the gradient of the cost function. This is slow as you have to compute the gradient of the entire dataset in order to perform a single update.\n",
    "2. Stochastic Gradient Descent: The gradient of the cost function is computed from a single training example in every iteration.Faster\n",
    "3. Mini-Batch Gradient Descent: The sample of the training data is used to compute the gradient of the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deep Learning -Jose Portilla Notes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link to Notes: https://docs.google.com/presentation/d/12oUP2g7gqpPBdZcmzuqH8_ttnzosOKA2cZKbOFJyPKU/edit#slide=id.g73ebe5debd_0_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tensor is an n-dimensional matrix\n",
    "We can create a simple perceptron model replicating the core concepts behind a neuron.\n",
    "\n",
    "A single pereceptron is not enough to learn complicated systems. They can however be expanded to create a multi-layer perceptron model.\n",
    "\n",
    "Feedfoward- all the info goes from input layer to output layer.\n",
    "Hidden layers are the layers between the input and output layers. They are difficult to interpret due to their high interconnectivity and distance away from known input or output values.\n",
    "\n",
    "A Neural Network becomes a 'deep neural network' when it contains 2 or more hidden layers.\n",
    "\n",
    "**Activation functions** set boundaries to output values from the neuron.\n",
    "1. Step function\n",
    "2. sigmoid function\n",
    "3. Hyperbolic Tangent (Between -1 and 1)\n",
    "4. Rectified Linear Unit(ReLU): Below 0 outputs 0 but above ) outputs the actual value. They have been found to have very good performance, especially when dealing with the issue of vanishing gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one-hot-encoding or dummy variables\n",
    "Mutually Exclusive Classes- each data point can only have one class assigned to it.\n",
    "Non_Exclusive Class: A data point may have more than one class assigned to it.\n",
    "\n",
    "softmax function:- used for mutually exclusive classes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cost Functions and Gradient Descent**\n",
    "\n",
    "The cost functions are sometimes called loss functions or error functions. -Measured during every epoch of training.\n",
    "\n",
    "Adaptive gradient descent: Starting with larger learning rate then adjusting to smaller rates as the slope gets closer to zero.\n",
    "\n",
    "'Adam': A method for stochastic optimization. It is more efficient for searching for minimums.-used as an optimizer- uses adaptive step size.\n",
    "\n",
    "For classification problems, cross entropy loss function is often used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Backpropagation**\n",
    "\n",
    "Moving backwards in a network to update the weights and biases. The main idea here is that we can use the gradient to go back through the network and adjust our weights and biases to minimize the output of the error vector on the last output layer.\n",
    "\n",
    "Hadamard Product - Element by element multiplication.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TensorFlow vs Keras**\n",
    "\n",
    "TensorFlow is an open-source deep learning library develped by Google. TF 2.0 released in late 2019. It has a large ecosystem of related components, including libraries like Tensorboard, Deployment and Production APIs, and support for various programming languages.\n",
    "\n",
    "Keras is a high-level python library that can use a variety of deep learning libraries underneath, such as: TensorFlow, CNTK or Theano. The official API for TF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification using Sckit_learn Neural Networks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary dependencies\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading dataset\n",
    "df = pd.read_csv('transfusion.data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Recency (months)</th>\n",
       "      <th>Frequency (times)</th>\n",
       "      <th>Monetary (c.c. blood)</th>\n",
       "      <th>Time (months)</th>\n",
       "      <th>whether he/she donated blood in March 2007</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>12500</td>\n",
       "      <td>98</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>3250</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>4000</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>5000</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>6000</td>\n",
       "      <td>77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Recency (months)  Frequency (times)  Monetary (c.c. blood)  Time (months)  \\\n",
       "0                 2                 50                  12500             98   \n",
       "1                 0                 13                   3250             28   \n",
       "2                 1                 16                   4000             35   \n",
       "3                 2                 20                   5000             45   \n",
       "4                 1                 24                   6000             77   \n",
       "\n",
       "   whether he/she donated blood in March 2007  \n",
       "0                                           1  \n",
       "1                                           1  \n",
       "2                                           1  \n",
       "3                                           1  \n",
       "4                                           0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping the target variable\n",
    "X = df.drop('whether he/she donated blood in March 2007',axis = 1)\n",
    "y = df['whether he/she donated blood in March 2007']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing or scaling the data\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instance of the standard scaler\n",
    "sc = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scalling - Fitting the dataset the dataset\n",
    "#X_train = sc.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transforming- combines both fitting and transforming\n",
    "X_train = sc.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tranforming the test set\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Multi Layer Perceptron\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creatinga a model\n",
    "model = MLPClassifier(solver = 'adam',max_iter = 1000,verbose = True, \n",
    "                     hidden_layer_sizes = (5,5,5), random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.53613580\n",
      "Iteration 2, loss = 0.53551626\n",
      "Iteration 3, loss = 0.53483204\n",
      "Iteration 4, loss = 0.53433431\n",
      "Iteration 5, loss = 0.53367944\n",
      "Iteration 6, loss = 0.53331657\n",
      "Iteration 7, loss = 0.53279427\n",
      "Iteration 8, loss = 0.53242413\n",
      "Iteration 9, loss = 0.53198078\n",
      "Iteration 10, loss = 0.53169584\n",
      "Iteration 11, loss = 0.53126880\n",
      "Iteration 12, loss = 0.53099004\n",
      "Iteration 13, loss = 0.53064449\n",
      "Iteration 14, loss = 0.53034499\n",
      "Iteration 15, loss = 0.53005366\n",
      "Iteration 16, loss = 0.52975277\n",
      "Iteration 17, loss = 0.52945517\n",
      "Iteration 18, loss = 0.52919865\n",
      "Iteration 19, loss = 0.52892462\n",
      "Iteration 20, loss = 0.52862781\n",
      "Iteration 21, loss = 0.52833875\n",
      "Iteration 22, loss = 0.52807558\n",
      "Iteration 23, loss = 0.52780319\n",
      "Iteration 24, loss = 0.52752785\n",
      "Iteration 25, loss = 0.52727044\n",
      "Iteration 26, loss = 0.52700023\n",
      "Iteration 27, loss = 0.52673409\n",
      "Iteration 28, loss = 0.52646598\n",
      "Iteration 29, loss = 0.52621788\n",
      "Iteration 30, loss = 0.52593142\n",
      "Iteration 31, loss = 0.52565702\n",
      "Iteration 32, loss = 0.52537885\n",
      "Iteration 33, loss = 0.52508942\n",
      "Iteration 34, loss = 0.52482855\n",
      "Iteration 35, loss = 0.52453885\n",
      "Iteration 36, loss = 0.52425643\n",
      "Iteration 37, loss = 0.52394646\n",
      "Iteration 38, loss = 0.52366621\n",
      "Iteration 39, loss = 0.52336459\n",
      "Iteration 40, loss = 0.52306851\n",
      "Iteration 41, loss = 0.52273868\n",
      "Iteration 42, loss = 0.52245855\n",
      "Iteration 43, loss = 0.52216019\n",
      "Iteration 44, loss = 0.52182397\n",
      "Iteration 45, loss = 0.52151752\n",
      "Iteration 46, loss = 0.52118986\n",
      "Iteration 47, loss = 0.52084621\n",
      "Iteration 48, loss = 0.52049561\n",
      "Iteration 49, loss = 0.52014185\n",
      "Iteration 50, loss = 0.51980023\n",
      "Iteration 51, loss = 0.51941358\n",
      "Iteration 52, loss = 0.51907375\n",
      "Iteration 53, loss = 0.51867733\n",
      "Iteration 54, loss = 0.51830946\n",
      "Iteration 55, loss = 0.51789937\n",
      "Iteration 56, loss = 0.51749562\n",
      "Iteration 57, loss = 0.51711043\n",
      "Iteration 58, loss = 0.51663488\n",
      "Iteration 59, loss = 0.51621533\n",
      "Iteration 60, loss = 0.51581743\n",
      "Iteration 61, loss = 0.51537942\n",
      "Iteration 62, loss = 0.51496232\n",
      "Iteration 63, loss = 0.51447565\n",
      "Iteration 64, loss = 0.51401242\n",
      "Iteration 65, loss = 0.51354126\n",
      "Iteration 66, loss = 0.51307388\n",
      "Iteration 67, loss = 0.51256913\n",
      "Iteration 68, loss = 0.51207235\n",
      "Iteration 69, loss = 0.51153679\n",
      "Iteration 70, loss = 0.51097693\n",
      "Iteration 71, loss = 0.51042231\n",
      "Iteration 72, loss = 0.50991029\n",
      "Iteration 73, loss = 0.50927699\n",
      "Iteration 74, loss = 0.50871796\n",
      "Iteration 75, loss = 0.50815191\n",
      "Iteration 76, loss = 0.50752236\n",
      "Iteration 77, loss = 0.50691384\n",
      "Iteration 78, loss = 0.50635343\n",
      "Iteration 79, loss = 0.50565779\n",
      "Iteration 80, loss = 0.50502240\n",
      "Iteration 81, loss = 0.50439465\n",
      "Iteration 82, loss = 0.50377300\n",
      "Iteration 83, loss = 0.50314829\n",
      "Iteration 84, loss = 0.50248827\n",
      "Iteration 85, loss = 0.50185663\n",
      "Iteration 86, loss = 0.50119477\n",
      "Iteration 87, loss = 0.50048122\n",
      "Iteration 88, loss = 0.49985029\n",
      "Iteration 89, loss = 0.49921856\n",
      "Iteration 90, loss = 0.49852775\n",
      "Iteration 91, loss = 0.49774441\n",
      "Iteration 92, loss = 0.49724148\n",
      "Iteration 93, loss = 0.49639215\n",
      "Iteration 94, loss = 0.49580082\n",
      "Iteration 95, loss = 0.49519815\n",
      "Iteration 96, loss = 0.49447064\n",
      "Iteration 97, loss = 0.49386351\n",
      "Iteration 98, loss = 0.49317714\n",
      "Iteration 99, loss = 0.49256553\n",
      "Iteration 100, loss = 0.49185453\n",
      "Iteration 101, loss = 0.49126227\n",
      "Iteration 102, loss = 0.49055664\n",
      "Iteration 103, loss = 0.48979460\n",
      "Iteration 104, loss = 0.48913751\n",
      "Iteration 105, loss = 0.48852783\n",
      "Iteration 106, loss = 0.48780046\n",
      "Iteration 107, loss = 0.48717673\n",
      "Iteration 108, loss = 0.48642575\n",
      "Iteration 109, loss = 0.48573069\n",
      "Iteration 110, loss = 0.48514060\n",
      "Iteration 111, loss = 0.48430657\n",
      "Iteration 112, loss = 0.48367350\n",
      "Iteration 113, loss = 0.48279392\n",
      "Iteration 114, loss = 0.48194319\n",
      "Iteration 115, loss = 0.48101731\n",
      "Iteration 116, loss = 0.47981172\n",
      "Iteration 117, loss = 0.47893132\n",
      "Iteration 118, loss = 0.47771971\n",
      "Iteration 119, loss = 0.47671803\n",
      "Iteration 120, loss = 0.47560240\n",
      "Iteration 121, loss = 0.47464849\n",
      "Iteration 122, loss = 0.47372980\n",
      "Iteration 123, loss = 0.47281529\n",
      "Iteration 124, loss = 0.47193696\n",
      "Iteration 125, loss = 0.47110187\n",
      "Iteration 126, loss = 0.47019113\n",
      "Iteration 127, loss = 0.46925577\n",
      "Iteration 128, loss = 0.46846508\n",
      "Iteration 129, loss = 0.46760073\n",
      "Iteration 130, loss = 0.46694921\n",
      "Iteration 131, loss = 0.46605402\n",
      "Iteration 132, loss = 0.46530121\n",
      "Iteration 133, loss = 0.46444494\n",
      "Iteration 134, loss = 0.46371290\n",
      "Iteration 135, loss = 0.46296569\n",
      "Iteration 136, loss = 0.46222403\n",
      "Iteration 137, loss = 0.46143134\n",
      "Iteration 138, loss = 0.46067502\n",
      "Iteration 139, loss = 0.45989206\n",
      "Iteration 140, loss = 0.45905582\n",
      "Iteration 141, loss = 0.45831888\n",
      "Iteration 142, loss = 0.45773546\n",
      "Iteration 143, loss = 0.45676369\n",
      "Iteration 144, loss = 0.45621169\n",
      "Iteration 145, loss = 0.45533161\n",
      "Iteration 146, loss = 0.45482089\n",
      "Iteration 147, loss = 0.45402167\n",
      "Iteration 148, loss = 0.45325810\n",
      "Iteration 149, loss = 0.45264314\n",
      "Iteration 150, loss = 0.45202277\n",
      "Iteration 151, loss = 0.45131516\n",
      "Iteration 152, loss = 0.45073432\n",
      "Iteration 153, loss = 0.45000904\n",
      "Iteration 154, loss = 0.44941054\n",
      "Iteration 155, loss = 0.44887979\n",
      "Iteration 156, loss = 0.44833254\n",
      "Iteration 157, loss = 0.44776313\n",
      "Iteration 158, loss = 0.44717898\n",
      "Iteration 159, loss = 0.44669909\n",
      "Iteration 160, loss = 0.44613197\n",
      "Iteration 161, loss = 0.44551642\n",
      "Iteration 162, loss = 0.44509961\n",
      "Iteration 163, loss = 0.44456516\n",
      "Iteration 164, loss = 0.44409592\n",
      "Iteration 165, loss = 0.44380250\n",
      "Iteration 166, loss = 0.44330911\n",
      "Iteration 167, loss = 0.44284759\n",
      "Iteration 168, loss = 0.44243237\n",
      "Iteration 169, loss = 0.44205367\n",
      "Iteration 170, loss = 0.44170785\n",
      "Iteration 171, loss = 0.44160353\n",
      "Iteration 172, loss = 0.44089080\n",
      "Iteration 173, loss = 0.44055012\n",
      "Iteration 174, loss = 0.44021795\n",
      "Iteration 175, loss = 0.43986334\n",
      "Iteration 176, loss = 0.43950792\n",
      "Iteration 177, loss = 0.43921792\n",
      "Iteration 178, loss = 0.43887192\n",
      "Iteration 179, loss = 0.43857354\n",
      "Iteration 180, loss = 0.43830324\n",
      "Iteration 181, loss = 0.43799426\n",
      "Iteration 182, loss = 0.43765542\n",
      "Iteration 183, loss = 0.43737844\n",
      "Iteration 184, loss = 0.43702296\n",
      "Iteration 185, loss = 0.43676528\n",
      "Iteration 186, loss = 0.43651072\n",
      "Iteration 187, loss = 0.43618905\n",
      "Iteration 188, loss = 0.43590998\n",
      "Iteration 189, loss = 0.43567051\n",
      "Iteration 190, loss = 0.43537453\n",
      "Iteration 191, loss = 0.43507914\n",
      "Iteration 192, loss = 0.43485709\n",
      "Iteration 193, loss = 0.43468574\n",
      "Iteration 194, loss = 0.43440658\n",
      "Iteration 195, loss = 0.43423098\n",
      "Iteration 196, loss = 0.43396410\n",
      "Iteration 197, loss = 0.43380606\n",
      "Iteration 198, loss = 0.43357382\n",
      "Iteration 199, loss = 0.43351710\n",
      "Iteration 200, loss = 0.43322479\n",
      "Iteration 201, loss = 0.43296506\n",
      "Iteration 202, loss = 0.43282441\n",
      "Iteration 203, loss = 0.43263998\n",
      "Iteration 204, loss = 0.43245325\n",
      "Iteration 205, loss = 0.43226722\n",
      "Iteration 206, loss = 0.43213899\n",
      "Iteration 207, loss = 0.43188299\n",
      "Iteration 208, loss = 0.43171916\n",
      "Iteration 209, loss = 0.43152385\n",
      "Iteration 210, loss = 0.43131018\n",
      "Iteration 211, loss = 0.43121963\n",
      "Iteration 212, loss = 0.43108563\n",
      "Iteration 213, loss = 0.43082638\n",
      "Iteration 214, loss = 0.43070765\n",
      "Iteration 215, loss = 0.43051633\n",
      "Iteration 216, loss = 0.43040369\n",
      "Iteration 217, loss = 0.43031532\n",
      "Iteration 218, loss = 0.43020158\n",
      "Iteration 219, loss = 0.42999862\n",
      "Iteration 220, loss = 0.42986763\n",
      "Iteration 221, loss = 0.42974599\n",
      "Iteration 222, loss = 0.42970786\n",
      "Iteration 223, loss = 0.42959325\n",
      "Iteration 224, loss = 0.42945914\n",
      "Iteration 225, loss = 0.42943092\n",
      "Iteration 226, loss = 0.42928709\n",
      "Iteration 227, loss = 0.42918720\n",
      "Iteration 228, loss = 0.42913265\n",
      "Iteration 229, loss = 0.42895722\n",
      "Iteration 230, loss = 0.42887928\n",
      "Iteration 231, loss = 0.42884360\n",
      "Iteration 232, loss = 0.42884246\n",
      "Iteration 233, loss = 0.42878571\n",
      "Iteration 234, loss = 0.42867979\n",
      "Iteration 235, loss = 0.42860197\n",
      "Iteration 236, loss = 0.42856387\n",
      "Iteration 237, loss = 0.42850281\n",
      "Iteration 238, loss = 0.42844048\n",
      "Iteration 239, loss = 0.42840527\n",
      "Iteration 240, loss = 0.42839900\n",
      "Iteration 241, loss = 0.42834071\n",
      "Iteration 242, loss = 0.42830007\n",
      "Iteration 243, loss = 0.42821623\n",
      "Iteration 244, loss = 0.42817669\n",
      "Iteration 245, loss = 0.42813996\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(5, 5, 5), max_iter=1000, random_state=42,\n",
       "              verbose=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fitting the model\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making predictions\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7489878542510121"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#performance metrics\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "accuracy_score(predictions,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.77      0.85       219\n",
      "           1       0.24      0.57      0.34        28\n",
      "\n",
      "    accuracy                           0.75       247\n",
      "   macro avg       0.59      0.67      0.59       247\n",
      "weighted avg       0.86      0.75      0.79       247\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#classification report\n",
    "print(classification_report(predictions,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regression in Scikit_Learn Neural Networks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing necessary dependencies\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature 1</th>\n",
       "      <th>Feature 2</th>\n",
       "      <th>Feature 3</th>\n",
       "      <th>Feature 4</th>\n",
       "      <th>Feature 5</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>561.985028</td>\n",
       "      <td>732.353911</td>\n",
       "      <td>617.229697</td>\n",
       "      <td>557.960379</td>\n",
       "      <td>433.822515</td>\n",
       "      <td>17626.615604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>661.395572</td>\n",
       "      <td>448.089304</td>\n",
       "      <td>913.175553</td>\n",
       "      <td>986.790667</td>\n",
       "      <td>392.644115</td>\n",
       "      <td>22918.339944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>810.726439</td>\n",
       "      <td>541.588398</td>\n",
       "      <td>581.677631</td>\n",
       "      <td>947.810958</td>\n",
       "      <td>72.740924</td>\n",
       "      <td>19844.766068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>89.220403</td>\n",
       "      <td>20.703639</td>\n",
       "      <td>852.602722</td>\n",
       "      <td>796.832513</td>\n",
       "      <td>890.892440</td>\n",
       "      <td>14745.282522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1002.105182</td>\n",
       "      <td>818.338370</td>\n",
       "      <td>472.554867</td>\n",
       "      <td>799.261877</td>\n",
       "      <td>121.113012</td>\n",
       "      <td>15104.316428</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Feature 1   Feature 2   Feature 3   Feature 4   Feature 5         Price\n",
       "0   561.985028  732.353911  617.229697  557.960379  433.822515  17626.615604\n",
       "1   661.395572  448.089304  913.175553  986.790667  392.644115  22918.339944\n",
       "2   810.726439  541.588398  581.677631  947.810958   72.740924  19844.766068\n",
       "3    89.220403   20.703639  852.602722  796.832513  890.892440  14745.282522\n",
       "4  1002.105182  818.338370  472.554867  799.261877  121.113012  15104.316428"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('sales.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Feature 1    float64\n",
       "Feature 2    float64\n",
       "Feature 3    float64\n",
       "Feature 4    float64\n",
       "Feature 5    float64\n",
       "Price        float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There are no categorical features present in the dataset : Therefore there is no need for hot-encoding\n",
    "#creating the features and target varibles dataframes\n",
    "X = df.drop(['Price'],axis = 1)\n",
    "y = df['Price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fiting and transforming the training data\n",
    "X_train = sc.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tranforming the test set\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building model\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "#picking hidden layers closer to the number of features present\n",
    "model = MLPRegressor(max_iter = 1000, solver = 'lbfgs',hidden_layer_sizes = (5,5,5),random_state = 101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPRegressor(hidden_layer_sizes=(5, 5, 5), max_iter=1000, random_state=101,\n",
       "             solver='lbfgs')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fitting the model\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#performance metrics\n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error,RocCurveDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MAE is:  627.1930941547544\n",
      "The MSE is:  628522.5931459988\n",
      "The RMSE is:  792.794168208873\n"
     ]
    }
   ],
   "source": [
    "print('The MAE is: ',mean_absolute_error(y_test,predictions))\n",
    "print('The MSE is: ',mean_squared_error(y_test,predictions))\n",
    "print('The RMSE is: ',np.sqrt(mean_squared_error(y_test,predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tensorflow Introduction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the necessary dependencies\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature 1</th>\n",
       "      <th>Feature 2</th>\n",
       "      <th>Feature 3</th>\n",
       "      <th>Feature 4</th>\n",
       "      <th>Feature 5</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>561.985028</td>\n",
       "      <td>732.353911</td>\n",
       "      <td>617.229697</td>\n",
       "      <td>557.960379</td>\n",
       "      <td>433.822515</td>\n",
       "      <td>17626.615604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>661.395572</td>\n",
       "      <td>448.089304</td>\n",
       "      <td>913.175553</td>\n",
       "      <td>986.790667</td>\n",
       "      <td>392.644115</td>\n",
       "      <td>22918.339944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>810.726439</td>\n",
       "      <td>541.588398</td>\n",
       "      <td>581.677631</td>\n",
       "      <td>947.810958</td>\n",
       "      <td>72.740924</td>\n",
       "      <td>19844.766068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>89.220403</td>\n",
       "      <td>20.703639</td>\n",
       "      <td>852.602722</td>\n",
       "      <td>796.832513</td>\n",
       "      <td>890.892440</td>\n",
       "      <td>14745.282522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1002.105182</td>\n",
       "      <td>818.338370</td>\n",
       "      <td>472.554867</td>\n",
       "      <td>799.261877</td>\n",
       "      <td>121.113012</td>\n",
       "      <td>15104.316428</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Feature 1   Feature 2   Feature 3   Feature 4   Feature 5         Price\n",
       "0   561.985028  732.353911  617.229697  557.960379  433.822515  17626.615604\n",
       "1   661.395572  448.089304  913.175553  986.790667  392.644115  22918.339944\n",
       "2   810.726439  541.588398  581.677631  947.810958   72.740924  19844.766068\n",
       "3    89.220403   20.703639  852.602722  796.832513  890.892440  14745.282522\n",
       "4  1002.105182  818.338370  472.554867  799.261877  121.113012  15104.316428"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading the dataset\n",
    "df = pd.read_csv('sales.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scales the dataset such that all the records are between 0 and 1\n",
    "sc = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MinMaxScaler()"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tranform\n",
    "X_train = sc.transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Building the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the necessary dependecies\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential #initializes the layers\n",
    "from tensorflow.keras.layers import Dense #used to create the layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature 1</th>\n",
       "      <th>Feature 2</th>\n",
       "      <th>Feature 3</th>\n",
       "      <th>Feature 4</th>\n",
       "      <th>Feature 5</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>561.985028</td>\n",
       "      <td>732.353911</td>\n",
       "      <td>617.229697</td>\n",
       "      <td>557.960379</td>\n",
       "      <td>433.822515</td>\n",
       "      <td>17626.615604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>661.395572</td>\n",
       "      <td>448.089304</td>\n",
       "      <td>913.175553</td>\n",
       "      <td>986.790667</td>\n",
       "      <td>392.644115</td>\n",
       "      <td>22918.339944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>810.726439</td>\n",
       "      <td>541.588398</td>\n",
       "      <td>581.677631</td>\n",
       "      <td>947.810958</td>\n",
       "      <td>72.740924</td>\n",
       "      <td>19844.766068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>89.220403</td>\n",
       "      <td>20.703639</td>\n",
       "      <td>852.602722</td>\n",
       "      <td>796.832513</td>\n",
       "      <td>890.892440</td>\n",
       "      <td>14745.282522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1002.105182</td>\n",
       "      <td>818.338370</td>\n",
       "      <td>472.554867</td>\n",
       "      <td>799.261877</td>\n",
       "      <td>121.113012</td>\n",
       "      <td>15104.316428</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Feature 1   Feature 2   Feature 3   Feature 4   Feature 5         Price\n",
       "0   561.985028  732.353911  617.229697  557.960379  433.822515  17626.615604\n",
       "1   661.395572  448.089304  913.175553  986.790667  392.644115  22918.339944\n",
       "2   810.726439  541.588398  581.677631  947.810958   72.740924  19844.766068\n",
       "3    89.220403   20.703639  852.602722  796.832513  890.892440  14745.282522\n",
       "4  1002.105182  818.338370  472.554867  799.261877  121.113012  15104.316428"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method1: of creating the model\n",
    "#initialize sequential\n",
    "my_model = Sequential()\n",
    "#picking number in between number of features and the output\n",
    "my_model.add(Dense(5,activation = 'relu',input_dim=5))\n",
    "my_model.add(Dense(5,activation = 'relu'))\n",
    "my_model.add(Dense(5,activation = 'relu'))\n",
    "\n",
    "#including the output layer- output 1 because we are only predicting price\n",
    "my_model.add(Dense(1, activation = keras.activations.linear))  #linear activation-regression problem\n",
    "#compiling the model- application of the gradient descent\n",
    "my_model.compile(optimizer='adam',loss='mean_squared_error',metrics=['mse'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method 2\n",
    "model = Sequential(\n",
    "    [\n",
    "        Dense(5,activation='relu',input_dim=5),\n",
    "        Dense(5,activation = 'relu'),\n",
    "        Dense(5,activation = 'relu'),\n",
    "        Dense(1,activation = keras.activations.linear)\n",
    "    ]\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss = 'mean_squared_error',metrics=['mse'])\n",
    "#can also update the batch size: Number of samples per gradient update, defualt set as 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 238301520.0000 - mse: 238301520.0000 - val_loss: 244323632.0000 - val_mse: 244323632.0000\n",
      "Epoch 2/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 236845024.0000 - mse: 236845024.0000 - val_loss: 241022144.0000 - val_mse: 241022144.0000\n",
      "Epoch 3/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 230154272.0000 - mse: 230154272.0000 - val_loss: 229546048.0000 - val_mse: 229546048.0000\n",
      "Epoch 4/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 212257120.0000 - mse: 212257120.0000 - val_loss: 203517504.0000 - val_mse: 203517504.0000\n",
      "Epoch 5/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 178297120.0000 - mse: 178297120.0000 - val_loss: 160214352.0000 - val_mse: 160214352.0000\n",
      "Epoch 6/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 129449944.0000 - mse: 129449944.0000 - val_loss: 105535432.0000 - val_mse: 105535432.0000\n",
      "Epoch 7/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 76691712.0000 - mse: 76691712.0000 - val_loss: 55451300.0000 - val_mse: 55451300.0000\n",
      "Epoch 8/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 36559584.0000 - mse: 36559584.0000 - val_loss: 25223804.0000 - val_mse: 25223804.0000\n",
      "Epoch 9/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 17267184.0000 - mse: 17267184.0000 - val_loss: 14607835.0000 - val_mse: 14607835.0000\n",
      "Epoch 10/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 12024542.0000 - mse: 12024542.0000 - val_loss: 12528851.0000 - val_mse: 12528851.0000\n",
      "Epoch 11/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 11167120.0000 - mse: 11167120.0000 - val_loss: 12181296.0000 - val_mse: 12181296.0000\n",
      "Epoch 12/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 10965319.0000 - mse: 10965319.0000 - val_loss: 12004915.0000 - val_mse: 12004915.0000\n",
      "Epoch 13/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 10810742.0000 - mse: 10810742.0000 - val_loss: 11833073.0000 - val_mse: 11833073.0000\n",
      "Epoch 14/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 10649913.0000 - mse: 10649913.0000 - val_loss: 11659994.0000 - val_mse: 11659994.0000\n",
      "Epoch 15/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 10484639.0000 - mse: 10484639.0000 - val_loss: 11472548.0000 - val_mse: 11472548.0000\n",
      "Epoch 16/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 10312203.0000 - mse: 10312203.0000 - val_loss: 11283991.0000 - val_mse: 11283991.0000\n",
      "Epoch 17/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 10134935.0000 - mse: 10134935.0000 - val_loss: 11086075.0000 - val_mse: 11086075.0000\n",
      "Epoch 18/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 9955245.0000 - mse: 9955245.0000 - val_loss: 10888177.0000 - val_mse: 10888177.0000\n",
      "Epoch 19/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 9768378.0000 - mse: 9768378.0000 - val_loss: 10687987.0000 - val_mse: 10687987.0000\n",
      "Epoch 20/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 9582279.0000 - mse: 9582279.0000 - val_loss: 10488946.0000 - val_mse: 10488946.0000\n",
      "Epoch 21/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 9394343.0000 - mse: 9394343.0000 - val_loss: 10290269.0000 - val_mse: 10290269.0000\n",
      "Epoch 22/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 9207247.0000 - mse: 9207247.0000 - val_loss: 10071297.0000 - val_mse: 10071297.0000\n",
      "Epoch 23/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 9018162.0000 - mse: 9018162.0000 - val_loss: 9863050.0000 - val_mse: 9863050.0000\n",
      "Epoch 24/100\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 8829541.0000 - mse: 8829541.0000 - val_loss: 9660132.0000 - val_mse: 9660132.0000\n",
      "Epoch 25/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 8657371.0000 - mse: 8657371.0000 - val_loss: 9460899.0000 - val_mse: 9460899.0000\n",
      "Epoch 26/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 8475385.0000 - mse: 8475385.0000 - val_loss: 9265882.0000 - val_mse: 9265882.0000\n",
      "Epoch 27/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 8297366.5000 - mse: 8297366.5000 - val_loss: 9073374.0000 - val_mse: 9073374.0000\n",
      "Epoch 28/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 8128743.5000 - mse: 8128743.5000 - val_loss: 8900411.0000 - val_mse: 8900411.0000\n",
      "Epoch 29/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 7964180.0000 - mse: 7964180.0000 - val_loss: 8710695.0000 - val_mse: 8710695.0000\n",
      "Epoch 30/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 7808909.5000 - mse: 7808909.5000 - val_loss: 8540792.0000 - val_mse: 8540792.0000\n",
      "Epoch 31/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 7660910.0000 - mse: 7660910.0000 - val_loss: 8377889.0000 - val_mse: 8377889.0000\n",
      "Epoch 32/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 7519603.5000 - mse: 7519603.5000 - val_loss: 8226481.5000 - val_mse: 8226481.5000\n",
      "Epoch 33/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 7390208.0000 - mse: 7390208.0000 - val_loss: 8078333.5000 - val_mse: 8078333.5000\n",
      "Epoch 34/100\n",
      "168/168 [==============================] - 0s 3ms/step - loss: 7265162.5000 - mse: 7265162.5000 - val_loss: 7944694.0000 - val_mse: 7944694.0000\n",
      "Epoch 35/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 7155105.0000 - mse: 7155105.0000 - val_loss: 7815091.0000 - val_mse: 7815091.0000\n",
      "Epoch 36/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 7044276.5000 - mse: 7044276.5000 - val_loss: 7702190.0000 - val_mse: 7702190.0000\n",
      "Epoch 37/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 6953550.5000 - mse: 6953550.5000 - val_loss: 7592265.5000 - val_mse: 7592265.5000\n",
      "Epoch 38/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 6858654.0000 - mse: 6858654.0000 - val_loss: 7497982.5000 - val_mse: 7497982.5000\n",
      "Epoch 39/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 6785659.0000 - mse: 6785659.0000 - val_loss: 7410099.0000 - val_mse: 7410099.0000\n",
      "Epoch 40/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 6712848.0000 - mse: 6712848.0000 - val_loss: 7334423.5000 - val_mse: 7334423.5000\n",
      "Epoch 41/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 6651854.5000 - mse: 6651854.5000 - val_loss: 7258229.0000 - val_mse: 7258229.0000\n",
      "Epoch 42/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 6592312.0000 - mse: 6592312.0000 - val_loss: 7196838.5000 - val_mse: 7196838.5000\n",
      "Epoch 43/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 6545601.5000 - mse: 6545601.5000 - val_loss: 7145770.0000 - val_mse: 7145770.0000\n",
      "Epoch 44/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 6498064.0000 - mse: 6498064.0000 - val_loss: 7084875.5000 - val_mse: 7084875.5000\n",
      "Epoch 45/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 6461036.5000 - mse: 6461036.5000 - val_loss: 7041390.0000 - val_mse: 7041390.0000\n",
      "Epoch 46/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 6427953.5000 - mse: 6427953.5000 - val_loss: 7020125.0000 - val_mse: 7020125.0000\n",
      "Epoch 47/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 6404327.5000 - mse: 6404327.5000 - val_loss: 6970945.5000 - val_mse: 6970945.5000\n",
      "Epoch 48/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 6372249.0000 - mse: 6372249.0000 - val_loss: 6933993.0000 - val_mse: 6933993.0000\n",
      "Epoch 49/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 6346531.5000 - mse: 6346531.5000 - val_loss: 6906844.0000 - val_mse: 6906844.0000\n",
      "Epoch 50/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 6324104.0000 - mse: 6324104.0000 - val_loss: 6883928.0000 - val_mse: 6883928.0000\n",
      "Epoch 51/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 6307998.5000 - mse: 6307998.5000 - val_loss: 6859810.5000 - val_mse: 6859810.5000\n",
      "Epoch 52/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 6292476.0000 - mse: 6292476.0000 - val_loss: 6828913.0000 - val_mse: 6828913.0000\n",
      "Epoch 53/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 6272668.5000 - mse: 6272668.5000 - val_loss: 6808953.5000 - val_mse: 6808953.5000\n",
      "Epoch 54/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 6261947.0000 - mse: 6261947.0000 - val_loss: 6792764.5000 - val_mse: 6792764.5000\n",
      "Epoch 55/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 6246043.0000 - mse: 6246043.0000 - val_loss: 6781117.5000 - val_mse: 6781117.5000\n",
      "Epoch 56/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 6235757.5000 - mse: 6235757.5000 - val_loss: 6765814.0000 - val_mse: 6765814.0000\n",
      "Epoch 57/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 6227254.5000 - mse: 6227254.5000 - val_loss: 6747548.5000 - val_mse: 6747548.5000\n",
      "Epoch 58/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 6215502.5000 - mse: 6215502.5000 - val_loss: 6741440.5000 - val_mse: 6741440.5000\n",
      "Epoch 59/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 6207723.5000 - mse: 6207723.5000 - val_loss: 6739522.5000 - val_mse: 6739522.5000\n",
      "Epoch 60/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 6200120.5000 - mse: 6200120.5000 - val_loss: 6710961.5000 - val_mse: 6710961.5000\n",
      "Epoch 61/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 6197703.5000 - mse: 6197703.5000 - val_loss: 6703678.0000 - val_mse: 6703678.0000\n",
      "Epoch 62/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 6188240.0000 - mse: 6188240.0000 - val_loss: 6688616.5000 - val_mse: 6688616.5000\n",
      "Epoch 63/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 6177229.0000 - mse: 6177229.0000 - val_loss: 6682540.0000 - val_mse: 6682540.0000\n",
      "Epoch 64/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 6172532.0000 - mse: 6172532.0000 - val_loss: 6682611.0000 - val_mse: 6682611.0000\n",
      "Epoch 65/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 6172590.0000 - mse: 6172590.0000 - val_loss: 6670107.5000 - val_mse: 6670107.5000\n",
      "Epoch 66/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 6164114.5000 - mse: 6164114.5000 - val_loss: 6663255.0000 - val_mse: 6663255.0000\n",
      "Epoch 67/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 6161388.5000 - mse: 6161388.5000 - val_loss: 6650563.5000 - val_mse: 6650563.5000\n",
      "Epoch 68/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 6154134.0000 - mse: 6154134.0000 - val_loss: 6646817.0000 - val_mse: 6646817.0000\n",
      "Epoch 69/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 6149968.0000 - mse: 6149968.0000 - val_loss: 6655617.0000 - val_mse: 6655617.0000\n",
      "Epoch 70/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 6150787.0000 - mse: 6150787.0000 - val_loss: 6635454.5000 - val_mse: 6635454.5000\n",
      "Epoch 71/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 6143668.0000 - mse: 6143668.0000 - val_loss: 6634126.0000 - val_mse: 6634126.0000\n",
      "Epoch 72/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 6143673.5000 - mse: 6143673.5000 - val_loss: 6631950.5000 - val_mse: 6631950.5000\n",
      "Epoch 73/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 6140943.5000 - mse: 6140943.5000 - val_loss: 6638908.5000 - val_mse: 6638908.5000\n",
      "Epoch 74/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 6133462.5000 - mse: 6133462.5000 - val_loss: 6638445.5000 - val_mse: 6638445.5000\n",
      "Epoch 75/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 6130555.5000 - mse: 6130555.5000 - val_loss: 6622216.5000 - val_mse: 6622216.5000\n",
      "Epoch 76/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 6126818.0000 - mse: 6126818.0000 - val_loss: 6615780.5000 - val_mse: 6615780.5000\n",
      "Epoch 77/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 6129735.5000 - mse: 6129735.5000 - val_loss: 6605426.5000 - val_mse: 6605426.5000\n",
      "Epoch 78/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 6127876.0000 - mse: 6127876.0000 - val_loss: 6604816.0000 - val_mse: 6604816.0000\n",
      "Epoch 79/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 6126223.5000 - mse: 6126223.5000 - val_loss: 6603553.5000 - val_mse: 6603553.5000\n",
      "Epoch 80/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 6126154.0000 - mse: 6126154.0000 - val_loss: 6613258.0000 - val_mse: 6613258.0000\n",
      "Epoch 81/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 6125530.0000 - mse: 6125530.0000 - val_loss: 6593785.0000 - val_mse: 6593785.0000\n",
      "Epoch 82/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 6120239.0000 - mse: 6120239.0000 - val_loss: 6612505.0000 - val_mse: 6612505.0000\n",
      "Epoch 83/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 6122304.0000 - mse: 6122304.0000 - val_loss: 6591304.0000 - val_mse: 6591304.0000\n",
      "Epoch 84/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 6118058.0000 - mse: 6118058.0000 - val_loss: 6589177.0000 - val_mse: 6589177.0000\n",
      "Epoch 85/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 6121957.5000 - mse: 6121957.5000 - val_loss: 6605890.5000 - val_mse: 6605890.5000\n",
      "Epoch 86/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 6119356.5000 - mse: 6119356.5000 - val_loss: 6597017.0000 - val_mse: 6597017.0000\n",
      "Epoch 87/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 6114205.0000 - mse: 6114205.0000 - val_loss: 6582778.5000 - val_mse: 6582778.5000\n",
      "Epoch 88/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 6118068.5000 - mse: 6118068.5000 - val_loss: 6597318.0000 - val_mse: 6597318.0000\n",
      "Epoch 89/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 6114639.0000 - mse: 6114639.0000 - val_loss: 6581900.5000 - val_mse: 6581900.5000\n",
      "Epoch 90/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 6108315.0000 - mse: 6108315.0000 - val_loss: 6583778.0000 - val_mse: 6583778.0000\n",
      "Epoch 91/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 6117687.0000 - mse: 6117687.0000 - val_loss: 6578539.5000 - val_mse: 6578539.5000\n",
      "Epoch 92/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 6113268.0000 - mse: 6113268.0000 - val_loss: 6596254.0000 - val_mse: 6596254.0000\n",
      "Epoch 93/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 6114596.5000 - mse: 6114596.5000 - val_loss: 6575326.5000 - val_mse: 6575326.5000\n",
      "Epoch 94/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 6115575.0000 - mse: 6115575.0000 - val_loss: 6574353.0000 - val_mse: 6574353.0000\n",
      "Epoch 95/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 6115644.0000 - mse: 6115644.0000 - val_loss: 6576639.0000 - val_mse: 6576639.0000\n",
      "Epoch 96/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 6102157.5000 - mse: 6102157.5000 - val_loss: 6604447.0000 - val_mse: 6604447.0000\n",
      "Epoch 97/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 6115088.5000 - mse: 6115088.5000 - val_loss: 6578031.0000 - val_mse: 6578031.0000\n",
      "Epoch 98/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 6108019.5000 - mse: 6108019.5000 - val_loss: 6572235.5000 - val_mse: 6572235.5000\n",
      "Epoch 99/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 6111233.0000 - mse: 6111233.0000 - val_loss: 6569612.5000 - val_mse: 6569612.5000\n",
      "Epoch 100/100\n",
      "168/168 [==============================] - 0s 2ms/step - loss: 6110264.0000 - mse: 6110264.0000 - val_loss: 6569648.5000 - val_mse: 6569648.5000\n"
     ]
    }
   ],
   "source": [
    "#fitting the model\n",
    "#epochs : How many times the dataset is to be fitted to the network\n",
    "#Validation_split: used to validate how the model is performing\n",
    "history = model.fit(X_train,y_train,epochs = 100,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluating the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making predictions\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MAE is:  1923.716789238003\n",
      "The MSE is:  6166875.720573046\n",
      "The RMSE is:  2483.3194962736966\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error,mean_squared_error\n",
    "print('The MAE is: ',mean_absolute_error(y_test,predictions))\n",
    "print('The MSE is: ',mean_squared_error(y_test,predictions))\n",
    "print('The RMSE is: ',np.sqrt(mean_squared_error(y_test,predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3300,)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3300, 1)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In C:\\Users\\user\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The text.latex.preview rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\user\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The mathtext.fallback_to_cm rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\user\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: Support for setting the 'mathtext.fallback_to_cm' rcParam is deprecated since 3.3 and will be removed two minor releases later; use 'mathtext.fallback : 'cm' instead.\n",
      "In C:\\Users\\user\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The validate_bool_maybe_none function was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\user\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The savefig.jpeg_quality rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\user\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The keymap.all_axes rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\user\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The animation.avconv_path rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\user\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The animation.avconv_args rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Price'>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.distplot((y_test-predictions.reshape(3300,)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plotting the Model Losses**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a dataframe\n",
    "losses = pd.DataFrame(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>mse</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_mse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>238301520.0</td>\n",
       "      <td>238301520.0</td>\n",
       "      <td>244323632.0</td>\n",
       "      <td>244323632.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>236845024.0</td>\n",
       "      <td>236845024.0</td>\n",
       "      <td>241022144.0</td>\n",
       "      <td>241022144.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>230154272.0</td>\n",
       "      <td>230154272.0</td>\n",
       "      <td>229546048.0</td>\n",
       "      <td>229546048.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>212257120.0</td>\n",
       "      <td>212257120.0</td>\n",
       "      <td>203517504.0</td>\n",
       "      <td>203517504.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>178297120.0</td>\n",
       "      <td>178297120.0</td>\n",
       "      <td>160214352.0</td>\n",
       "      <td>160214352.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          loss          mse     val_loss      val_mse\n",
       "0  238301520.0  238301520.0  244323632.0  244323632.0\n",
       "1  236845024.0  236845024.0  241022144.0  241022144.0\n",
       "2  230154272.0  230154272.0  229546048.0  229546048.0\n",
       "3  212257120.0  212257120.0  203517504.0  203517504.0\n",
       "4  178297120.0  178297120.0  160214352.0  160214352.0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x17041321508>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plotting\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "losses['loss'].plot(label='Training Loss')\n",
    "losses['val_loss'].plot(label = 'Validation Loss')\n",
    "plt.legend(loc = 'center')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss decreases as the model learns; increase in number of epochs. if the validation loss increases while the training loss decreases, it would mean the model is overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Overfitting - Classification Problem**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the neccessary dependencies\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Recency (months)</th>\n",
       "      <th>Frequency (times)</th>\n",
       "      <th>Monetary (c.c. blood)</th>\n",
       "      <th>Time (months)</th>\n",
       "      <th>whether he/she donated blood in March 2007</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>12500</td>\n",
       "      <td>98</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>3250</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>4000</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>5000</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>6000</td>\n",
       "      <td>77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Recency (months)  Frequency (times)  Monetary (c.c. blood)  Time (months)  \\\n",
       "0                 2                 50                  12500             98   \n",
       "1                 0                 13                   3250             28   \n",
       "2                 1                 16                   4000             35   \n",
       "3                 2                 20                   5000             45   \n",
       "4                 1                 24                   6000             77   \n",
       "\n",
       "   whether he/she donated blood in March 2007  \n",
       "0                                           1  \n",
       "1                                           1  \n",
       "2                                           1  \n",
       "3                                           1  \n",
       "4                                           0  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing the dataset\n",
    "df = pd.read_csv('transfusion.data')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Recency (months)</th>\n",
       "      <th>Frequency (times)</th>\n",
       "      <th>Monetary (c.c. blood)</th>\n",
       "      <th>Time (months)</th>\n",
       "      <th>whether he/she donated blood in March 2007</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>748.000000</td>\n",
       "      <td>748.000000</td>\n",
       "      <td>748.000000</td>\n",
       "      <td>748.000000</td>\n",
       "      <td>748.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>9.506684</td>\n",
       "      <td>5.514706</td>\n",
       "      <td>1378.676471</td>\n",
       "      <td>34.282086</td>\n",
       "      <td>0.237968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>8.095396</td>\n",
       "      <td>5.839307</td>\n",
       "      <td>1459.826781</td>\n",
       "      <td>24.376714</td>\n",
       "      <td>0.426124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>2.750000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1750.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>12500.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Recency (months)  Frequency (times)  Monetary (c.c. blood)  \\\n",
       "count        748.000000         748.000000             748.000000   \n",
       "mean           9.506684           5.514706            1378.676471   \n",
       "std            8.095396           5.839307            1459.826781   \n",
       "min            0.000000           1.000000             250.000000   \n",
       "25%            2.750000           2.000000             500.000000   \n",
       "50%            7.000000           4.000000            1000.000000   \n",
       "75%           14.000000           7.000000            1750.000000   \n",
       "max           74.000000          50.000000           12500.000000   \n",
       "\n",
       "       Time (months)  whether he/she donated blood in March 2007  \n",
       "count     748.000000                                  748.000000  \n",
       "mean       34.282086                                    0.237968  \n",
       "std        24.376714                                    0.426124  \n",
       "min         2.000000                                    0.000000  \n",
       "25%        16.000000                                    0.000000  \n",
       "50%        28.000000                                    0.000000  \n",
       "75%        50.000000                                    0.000000  \n",
       "max        98.000000                                    1.000000  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data preparation - creating features and target variable\n",
    "X = df.drop(['whether he/she donated blood in March 2007'],axis = 1)\n",
    "y = df['whether he/she donated blood in March 2007']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sc = MinMaxScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building the model\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense,Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    #passing layer- passing 20 neurons,input_dim: equal to number of features\n",
    "    Dense(20,activation = 'relu',input_dim=4),\n",
    "    Dense(15,activation = 'relu'),\n",
    "    #means dropping 20% of the layers at this point to prevent overfitting\n",
    "    Dropout(0.2),\n",
    "    #creating output layer with actibvation using relu\n",
    "    Dense(25,activation = 'relu'),\n",
    "    #using sigmoid becaose we are predicting two classes\n",
    "    Dense(1,activation=keras.activations.sigmoid)\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compiling the model\n",
    "model.compile(optimizer = 'adam',loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1600\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6766 - accuracy: 0.7485 - val_loss: 0.6543 - val_accuracy: 0.7449\n",
      "Epoch 2/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.6383 - accuracy: 0.7705 - val_loss: 0.6144 - val_accuracy: 0.7449\n",
      "Epoch 3/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5963 - accuracy: 0.7705 - val_loss: 0.5758 - val_accuracy: 0.7449\n",
      "Epoch 4/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5583 - accuracy: 0.7705 - val_loss: 0.5509 - val_accuracy: 0.7449\n",
      "Epoch 5/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5356 - accuracy: 0.7705 - val_loss: 0.5411 - val_accuracy: 0.7449\n",
      "Epoch 6/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5137 - accuracy: 0.7705 - val_loss: 0.5357 - val_accuracy: 0.7449\n",
      "Epoch 7/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5235 - accuracy: 0.7705 - val_loss: 0.5308 - val_accuracy: 0.7449\n",
      "Epoch 8/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5058 - accuracy: 0.7705 - val_loss: 0.5247 - val_accuracy: 0.7449\n",
      "Epoch 9/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5041 - accuracy: 0.7705 - val_loss: 0.5208 - val_accuracy: 0.7449\n",
      "Epoch 10/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5011 - accuracy: 0.7705 - val_loss: 0.5163 - val_accuracy: 0.7449\n",
      "Epoch 11/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4921 - accuracy: 0.7725 - val_loss: 0.5117 - val_accuracy: 0.7449\n",
      "Epoch 12/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4918 - accuracy: 0.7764 - val_loss: 0.5089 - val_accuracy: 0.7490\n",
      "Epoch 13/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4834 - accuracy: 0.7824 - val_loss: 0.5078 - val_accuracy: 0.7490\n",
      "Epoch 14/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4922 - accuracy: 0.7824 - val_loss: 0.5036 - val_accuracy: 0.7449\n",
      "Epoch 15/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4882 - accuracy: 0.7784 - val_loss: 0.5022 - val_accuracy: 0.7490\n",
      "Epoch 16/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4818 - accuracy: 0.7884 - val_loss: 0.4997 - val_accuracy: 0.7490\n",
      "Epoch 17/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4737 - accuracy: 0.7804 - val_loss: 0.4986 - val_accuracy: 0.7490\n",
      "Epoch 18/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4801 - accuracy: 0.7725 - val_loss: 0.4975 - val_accuracy: 0.7530\n",
      "Epoch 19/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4809 - accuracy: 0.7844 - val_loss: 0.4956 - val_accuracy: 0.7530\n",
      "Epoch 20/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4690 - accuracy: 0.7745 - val_loss: 0.4940 - val_accuracy: 0.7530\n",
      "Epoch 21/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4786 - accuracy: 0.7725 - val_loss: 0.4946 - val_accuracy: 0.7530\n",
      "Epoch 22/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4664 - accuracy: 0.7764 - val_loss: 0.4928 - val_accuracy: 0.7530\n",
      "Epoch 23/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4785 - accuracy: 0.7784 - val_loss: 0.4929 - val_accuracy: 0.7571\n",
      "Epoch 24/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4758 - accuracy: 0.7745 - val_loss: 0.4903 - val_accuracy: 0.7652\n",
      "Epoch 25/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4709 - accuracy: 0.7784 - val_loss: 0.4918 - val_accuracy: 0.7571\n",
      "Epoch 26/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4741 - accuracy: 0.7705 - val_loss: 0.4890 - val_accuracy: 0.7611\n",
      "Epoch 27/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4740 - accuracy: 0.7864 - val_loss: 0.4896 - val_accuracy: 0.7611\n",
      "Epoch 28/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4687 - accuracy: 0.7824 - val_loss: 0.4901 - val_accuracy: 0.7611\n",
      "Epoch 29/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4637 - accuracy: 0.7844 - val_loss: 0.4903 - val_accuracy: 0.7611\n",
      "Epoch 30/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4663 - accuracy: 0.7824 - val_loss: 0.4889 - val_accuracy: 0.7733\n",
      "Epoch 31/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4752 - accuracy: 0.7844 - val_loss: 0.4888 - val_accuracy: 0.7814\n",
      "Epoch 32/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4648 - accuracy: 0.7824 - val_loss: 0.4885 - val_accuracy: 0.7652\n",
      "Epoch 33/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4788 - accuracy: 0.7824 - val_loss: 0.4881 - val_accuracy: 0.7652\n",
      "Epoch 34/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4651 - accuracy: 0.7884 - val_loss: 0.4880 - val_accuracy: 0.7652\n",
      "Epoch 35/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4707 - accuracy: 0.7784 - val_loss: 0.4875 - val_accuracy: 0.7733\n",
      "Epoch 36/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4673 - accuracy: 0.7984 - val_loss: 0.4877 - val_accuracy: 0.7652\n",
      "Epoch 37/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4701 - accuracy: 0.7844 - val_loss: 0.4879 - val_accuracy: 0.7611\n",
      "Epoch 38/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4642 - accuracy: 0.7884 - val_loss: 0.4873 - val_accuracy: 0.7814\n",
      "Epoch 39/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4679 - accuracy: 0.7804 - val_loss: 0.4866 - val_accuracy: 0.7733\n",
      "Epoch 40/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4755 - accuracy: 0.7844 - val_loss: 0.4869 - val_accuracy: 0.7814\n",
      "Epoch 41/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4639 - accuracy: 0.7924 - val_loss: 0.4876 - val_accuracy: 0.7611\n",
      "Epoch 42/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4726 - accuracy: 0.7764 - val_loss: 0.4878 - val_accuracy: 0.7733\n",
      "Epoch 43/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4703 - accuracy: 0.7844 - val_loss: 0.4875 - val_accuracy: 0.7773\n",
      "Epoch 44/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4735 - accuracy: 0.7844 - val_loss: 0.4877 - val_accuracy: 0.7733\n",
      "Epoch 45/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4686 - accuracy: 0.7984 - val_loss: 0.4859 - val_accuracy: 0.7814\n",
      "Epoch 46/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4607 - accuracy: 0.7924 - val_loss: 0.4866 - val_accuracy: 0.7854\n",
      "Epoch 47/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4593 - accuracy: 0.7924 - val_loss: 0.4867 - val_accuracy: 0.7854\n",
      "Epoch 48/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4701 - accuracy: 0.7884 - val_loss: 0.4862 - val_accuracy: 0.7814\n",
      "Epoch 49/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4621 - accuracy: 0.7904 - val_loss: 0.4863 - val_accuracy: 0.7814\n",
      "Epoch 50/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4730 - accuracy: 0.7884 - val_loss: 0.4853 - val_accuracy: 0.7814\n",
      "Epoch 51/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4635 - accuracy: 0.7864 - val_loss: 0.4850 - val_accuracy: 0.7814\n",
      "Epoch 52/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4747 - accuracy: 0.7824 - val_loss: 0.4837 - val_accuracy: 0.7854\n",
      "Epoch 53/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4583 - accuracy: 0.7824 - val_loss: 0.4842 - val_accuracy: 0.7814\n",
      "Epoch 54/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4677 - accuracy: 0.7764 - val_loss: 0.4840 - val_accuracy: 0.7814\n",
      "Epoch 55/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4697 - accuracy: 0.7884 - val_loss: 0.4831 - val_accuracy: 0.7854\n",
      "Epoch 56/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4649 - accuracy: 0.7944 - val_loss: 0.4840 - val_accuracy: 0.7814\n",
      "Epoch 57/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4676 - accuracy: 0.7804 - val_loss: 0.4845 - val_accuracy: 0.7814\n",
      "Epoch 58/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4695 - accuracy: 0.7884 - val_loss: 0.4828 - val_accuracy: 0.7854\n",
      "Epoch 59/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4654 - accuracy: 0.7804 - val_loss: 0.4830 - val_accuracy: 0.7854\n",
      "Epoch 60/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4665 - accuracy: 0.7764 - val_loss: 0.4833 - val_accuracy: 0.7814\n",
      "Epoch 61/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4667 - accuracy: 0.7864 - val_loss: 0.4832 - val_accuracy: 0.7854\n",
      "Epoch 62/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4603 - accuracy: 0.8004 - val_loss: 0.4845 - val_accuracy: 0.7854\n",
      "Epoch 63/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4597 - accuracy: 0.7844 - val_loss: 0.4836 - val_accuracy: 0.7854\n",
      "Epoch 64/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4626 - accuracy: 0.7864 - val_loss: 0.4826 - val_accuracy: 0.7854\n",
      "Epoch 65/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4704 - accuracy: 0.7725 - val_loss: 0.4827 - val_accuracy: 0.7854\n",
      "Epoch 66/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4680 - accuracy: 0.7844 - val_loss: 0.4826 - val_accuracy: 0.7895\n",
      "Epoch 67/1600\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4690 - accuracy: 0.7824 - val_loss: 0.4823 - val_accuracy: 0.7854\n",
      "Epoch 68/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4689 - accuracy: 0.7844 - val_loss: 0.4825 - val_accuracy: 0.7854\n",
      "Epoch 69/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4670 - accuracy: 0.7804 - val_loss: 0.4817 - val_accuracy: 0.7854\n",
      "Epoch 70/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4657 - accuracy: 0.7824 - val_loss: 0.4832 - val_accuracy: 0.7814\n",
      "Epoch 71/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4695 - accuracy: 0.7824 - val_loss: 0.4825 - val_accuracy: 0.7814\n",
      "Epoch 72/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4647 - accuracy: 0.7884 - val_loss: 0.4817 - val_accuracy: 0.7854\n",
      "Epoch 73/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4684 - accuracy: 0.7844 - val_loss: 0.4822 - val_accuracy: 0.7814\n",
      "Epoch 74/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4551 - accuracy: 0.7884 - val_loss: 0.4814 - val_accuracy: 0.7895\n",
      "Epoch 75/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4623 - accuracy: 0.7784 - val_loss: 0.4815 - val_accuracy: 0.7854\n",
      "Epoch 76/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4667 - accuracy: 0.7844 - val_loss: 0.4817 - val_accuracy: 0.7854\n",
      "Epoch 77/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4657 - accuracy: 0.7784 - val_loss: 0.4805 - val_accuracy: 0.7854\n",
      "Epoch 78/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4524 - accuracy: 0.7844 - val_loss: 0.4799 - val_accuracy: 0.7854\n",
      "Epoch 79/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4672 - accuracy: 0.7864 - val_loss: 0.4800 - val_accuracy: 0.7895\n",
      "Epoch 80/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4689 - accuracy: 0.7864 - val_loss: 0.4805 - val_accuracy: 0.7854\n",
      "Epoch 81/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4771 - accuracy: 0.7884 - val_loss: 0.4793 - val_accuracy: 0.7895\n",
      "Epoch 82/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4718 - accuracy: 0.7924 - val_loss: 0.4819 - val_accuracy: 0.7854\n",
      "Epoch 83/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4592 - accuracy: 0.7944 - val_loss: 0.4805 - val_accuracy: 0.7814\n",
      "Epoch 84/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4606 - accuracy: 0.7844 - val_loss: 0.4809 - val_accuracy: 0.7854\n",
      "Epoch 85/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4721 - accuracy: 0.7745 - val_loss: 0.4806 - val_accuracy: 0.7854\n",
      "Epoch 86/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4647 - accuracy: 0.7864 - val_loss: 0.4803 - val_accuracy: 0.7854\n",
      "Epoch 87/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4639 - accuracy: 0.7844 - val_loss: 0.4807 - val_accuracy: 0.7854\n",
      "Epoch 88/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4664 - accuracy: 0.7824 - val_loss: 0.4824 - val_accuracy: 0.7814\n",
      "Epoch 89/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4550 - accuracy: 0.7904 - val_loss: 0.4812 - val_accuracy: 0.7895\n",
      "Epoch 90/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4585 - accuracy: 0.7944 - val_loss: 0.4831 - val_accuracy: 0.7814\n",
      "Epoch 91/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4564 - accuracy: 0.7745 - val_loss: 0.4813 - val_accuracy: 0.7854\n",
      "Epoch 92/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4604 - accuracy: 0.7844 - val_loss: 0.4805 - val_accuracy: 0.7895\n",
      "Epoch 93/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4645 - accuracy: 0.7844 - val_loss: 0.4816 - val_accuracy: 0.7854\n",
      "Epoch 94/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4646 - accuracy: 0.7804 - val_loss: 0.4797 - val_accuracy: 0.7895\n",
      "Epoch 95/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4614 - accuracy: 0.7764 - val_loss: 0.4802 - val_accuracy: 0.7854\n",
      "Epoch 96/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4682 - accuracy: 0.7844 - val_loss: 0.4794 - val_accuracy: 0.7895\n",
      "Epoch 97/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4530 - accuracy: 0.7904 - val_loss: 0.4789 - val_accuracy: 0.7895\n",
      "Epoch 98/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4619 - accuracy: 0.7924 - val_loss: 0.4790 - val_accuracy: 0.7895\n",
      "Epoch 99/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4601 - accuracy: 0.7984 - val_loss: 0.4792 - val_accuracy: 0.7854\n",
      "Epoch 100/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4661 - accuracy: 0.7745 - val_loss: 0.4780 - val_accuracy: 0.7895\n",
      "Epoch 101/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4658 - accuracy: 0.7844 - val_loss: 0.4779 - val_accuracy: 0.7895\n",
      "Epoch 102/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4610 - accuracy: 0.7824 - val_loss: 0.4786 - val_accuracy: 0.7895\n",
      "Epoch 103/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4596 - accuracy: 0.7924 - val_loss: 0.4783 - val_accuracy: 0.7895\n",
      "Epoch 104/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4614 - accuracy: 0.7824 - val_loss: 0.4774 - val_accuracy: 0.7854\n",
      "Epoch 105/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4635 - accuracy: 0.7764 - val_loss: 0.4780 - val_accuracy: 0.7895\n",
      "Epoch 106/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4593 - accuracy: 0.7944 - val_loss: 0.4784 - val_accuracy: 0.7854\n",
      "Epoch 107/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4681 - accuracy: 0.7784 - val_loss: 0.4775 - val_accuracy: 0.7895\n",
      "Epoch 108/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4629 - accuracy: 0.7884 - val_loss: 0.4769 - val_accuracy: 0.7895\n",
      "Epoch 109/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4631 - accuracy: 0.7804 - val_loss: 0.4775 - val_accuracy: 0.7935\n",
      "Epoch 110/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4584 - accuracy: 0.7824 - val_loss: 0.4790 - val_accuracy: 0.7895\n",
      "Epoch 111/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4635 - accuracy: 0.7884 - val_loss: 0.4788 - val_accuracy: 0.7895\n",
      "Epoch 112/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4670 - accuracy: 0.7904 - val_loss: 0.4780 - val_accuracy: 0.7854\n",
      "Epoch 113/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4607 - accuracy: 0.7745 - val_loss: 0.4783 - val_accuracy: 0.7854\n",
      "Epoch 114/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4665 - accuracy: 0.7884 - val_loss: 0.4780 - val_accuracy: 0.7895\n",
      "Epoch 115/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4642 - accuracy: 0.7824 - val_loss: 0.4781 - val_accuracy: 0.7895\n",
      "Epoch 116/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4648 - accuracy: 0.7804 - val_loss: 0.4783 - val_accuracy: 0.7895\n",
      "Epoch 117/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4624 - accuracy: 0.7904 - val_loss: 0.4791 - val_accuracy: 0.7935\n",
      "Epoch 118/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4598 - accuracy: 0.7864 - val_loss: 0.4807 - val_accuracy: 0.7814\n",
      "Epoch 119/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4574 - accuracy: 0.7824 - val_loss: 0.4801 - val_accuracy: 0.7814\n",
      "Epoch 120/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4648 - accuracy: 0.7864 - val_loss: 0.4789 - val_accuracy: 0.7854\n",
      "Epoch 121/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4635 - accuracy: 0.7884 - val_loss: 0.4787 - val_accuracy: 0.7854\n",
      "Epoch 122/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4605 - accuracy: 0.7884 - val_loss: 0.4777 - val_accuracy: 0.7895\n",
      "Epoch 123/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4648 - accuracy: 0.7804 - val_loss: 0.4777 - val_accuracy: 0.7895\n",
      "Epoch 124/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4598 - accuracy: 0.7804 - val_loss: 0.4774 - val_accuracy: 0.7854\n",
      "Epoch 125/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4602 - accuracy: 0.7904 - val_loss: 0.4783 - val_accuracy: 0.7854\n",
      "Epoch 126/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4637 - accuracy: 0.7784 - val_loss: 0.4763 - val_accuracy: 0.7854\n",
      "Epoch 127/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4580 - accuracy: 0.7784 - val_loss: 0.4788 - val_accuracy: 0.7895\n",
      "Epoch 128/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4614 - accuracy: 0.7804 - val_loss: 0.4783 - val_accuracy: 0.7935\n",
      "Epoch 129/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4646 - accuracy: 0.7844 - val_loss: 0.4767 - val_accuracy: 0.7854\n",
      "Epoch 130/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4653 - accuracy: 0.7784 - val_loss: 0.4780 - val_accuracy: 0.7935\n",
      "Epoch 131/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4543 - accuracy: 0.7904 - val_loss: 0.4773 - val_accuracy: 0.7895\n",
      "Epoch 132/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4695 - accuracy: 0.7924 - val_loss: 0.4771 - val_accuracy: 0.7895\n",
      "Epoch 133/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4621 - accuracy: 0.7864 - val_loss: 0.4766 - val_accuracy: 0.7854\n",
      "Epoch 134/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4621 - accuracy: 0.7864 - val_loss: 0.4771 - val_accuracy: 0.7935\n",
      "Epoch 135/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4606 - accuracy: 0.7864 - val_loss: 0.4770 - val_accuracy: 0.7854\n",
      "Epoch 136/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4625 - accuracy: 0.7844 - val_loss: 0.4766 - val_accuracy: 0.7854\n",
      "Epoch 137/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4617 - accuracy: 0.7864 - val_loss: 0.4783 - val_accuracy: 0.7895\n",
      "Epoch 138/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4608 - accuracy: 0.7824 - val_loss: 0.4776 - val_accuracy: 0.7854\n",
      "Epoch 139/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4537 - accuracy: 0.7944 - val_loss: 0.4796 - val_accuracy: 0.7854\n",
      "Epoch 140/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4636 - accuracy: 0.7904 - val_loss: 0.4791 - val_accuracy: 0.7854\n",
      "Epoch 141/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4609 - accuracy: 0.7824 - val_loss: 0.4781 - val_accuracy: 0.7895\n",
      "Epoch 142/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4619 - accuracy: 0.7844 - val_loss: 0.4773 - val_accuracy: 0.7854\n",
      "Epoch 143/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4569 - accuracy: 0.7884 - val_loss: 0.4783 - val_accuracy: 0.7935\n",
      "Epoch 144/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4706 - accuracy: 0.7764 - val_loss: 0.4779 - val_accuracy: 0.7854\n",
      "Epoch 145/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4653 - accuracy: 0.7864 - val_loss: 0.4784 - val_accuracy: 0.7854\n",
      "Epoch 146/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4717 - accuracy: 0.7745 - val_loss: 0.4779 - val_accuracy: 0.7935\n",
      "Epoch 147/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4616 - accuracy: 0.7725 - val_loss: 0.4772 - val_accuracy: 0.7854\n",
      "Epoch 148/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4674 - accuracy: 0.7824 - val_loss: 0.4779 - val_accuracy: 0.7895\n",
      "Epoch 149/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4613 - accuracy: 0.7844 - val_loss: 0.4788 - val_accuracy: 0.7895\n",
      "Epoch 150/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4629 - accuracy: 0.7884 - val_loss: 0.4786 - val_accuracy: 0.7895\n",
      "Epoch 151/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4617 - accuracy: 0.7964 - val_loss: 0.4775 - val_accuracy: 0.7895\n",
      "Epoch 152/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4711 - accuracy: 0.7844 - val_loss: 0.4791 - val_accuracy: 0.7895\n",
      "Epoch 153/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4617 - accuracy: 0.7824 - val_loss: 0.4778 - val_accuracy: 0.7814\n",
      "Epoch 154/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4580 - accuracy: 0.7904 - val_loss: 0.4780 - val_accuracy: 0.7935\n",
      "Epoch 155/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4581 - accuracy: 0.7804 - val_loss: 0.4794 - val_accuracy: 0.7854\n",
      "Epoch 156/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4622 - accuracy: 0.7844 - val_loss: 0.4782 - val_accuracy: 0.7854\n",
      "Epoch 157/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4631 - accuracy: 0.7864 - val_loss: 0.4783 - val_accuracy: 0.7854\n",
      "Epoch 158/1600\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4606 - accuracy: 0.7884 - val_loss: 0.4807 - val_accuracy: 0.7935\n",
      "Epoch 159/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4515 - accuracy: 0.7844 - val_loss: 0.4784 - val_accuracy: 0.7854\n",
      "Epoch 160/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4596 - accuracy: 0.7804 - val_loss: 0.4792 - val_accuracy: 0.7895\n",
      "Epoch 161/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4605 - accuracy: 0.7884 - val_loss: 0.4787 - val_accuracy: 0.7935\n",
      "Epoch 162/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4618 - accuracy: 0.7924 - val_loss: 0.4777 - val_accuracy: 0.7935\n",
      "Epoch 163/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4609 - accuracy: 0.7964 - val_loss: 0.4772 - val_accuracy: 0.7895\n",
      "Epoch 164/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4584 - accuracy: 0.7844 - val_loss: 0.4783 - val_accuracy: 0.7895\n",
      "Epoch 165/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4611 - accuracy: 0.7824 - val_loss: 0.4765 - val_accuracy: 0.7895\n",
      "Epoch 166/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4612 - accuracy: 0.8004 - val_loss: 0.4780 - val_accuracy: 0.7976\n",
      "Epoch 167/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4659 - accuracy: 0.7784 - val_loss: 0.4797 - val_accuracy: 0.7895\n",
      "Epoch 168/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4570 - accuracy: 0.7884 - val_loss: 0.4782 - val_accuracy: 0.7935\n",
      "Epoch 169/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4576 - accuracy: 0.7824 - val_loss: 0.4792 - val_accuracy: 0.7895\n",
      "Epoch 170/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4598 - accuracy: 0.7804 - val_loss: 0.4798 - val_accuracy: 0.7854\n",
      "Epoch 171/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4637 - accuracy: 0.7705 - val_loss: 0.4800 - val_accuracy: 0.7895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 172/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4591 - accuracy: 0.7824 - val_loss: 0.4780 - val_accuracy: 0.7895\n",
      "Epoch 173/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4557 - accuracy: 0.7844 - val_loss: 0.4765 - val_accuracy: 0.7854\n",
      "Epoch 174/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4591 - accuracy: 0.7745 - val_loss: 0.4781 - val_accuracy: 0.7895\n",
      "Epoch 175/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4553 - accuracy: 0.7764 - val_loss: 0.4779 - val_accuracy: 0.7976\n",
      "Epoch 176/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4603 - accuracy: 0.7904 - val_loss: 0.4767 - val_accuracy: 0.7895\n",
      "Epoch 177/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4651 - accuracy: 0.7784 - val_loss: 0.4778 - val_accuracy: 0.7895\n",
      "Epoch 178/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4580 - accuracy: 0.7884 - val_loss: 0.4783 - val_accuracy: 0.7895\n",
      "Epoch 179/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4602 - accuracy: 0.7824 - val_loss: 0.4776 - val_accuracy: 0.7895\n",
      "Epoch 180/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4586 - accuracy: 0.7764 - val_loss: 0.4768 - val_accuracy: 0.7895\n",
      "Epoch 181/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4597 - accuracy: 0.7824 - val_loss: 0.4775 - val_accuracy: 0.7895\n",
      "Epoch 182/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4633 - accuracy: 0.7864 - val_loss: 0.4780 - val_accuracy: 0.7935\n",
      "Epoch 183/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4543 - accuracy: 0.7964 - val_loss: 0.4766 - val_accuracy: 0.7895\n",
      "Epoch 184/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4563 - accuracy: 0.7904 - val_loss: 0.4771 - val_accuracy: 0.7935\n",
      "Epoch 185/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4591 - accuracy: 0.7824 - val_loss: 0.4771 - val_accuracy: 0.8016\n",
      "Epoch 186/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4526 - accuracy: 0.7844 - val_loss: 0.4775 - val_accuracy: 0.7895\n",
      "Epoch 187/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4598 - accuracy: 0.7864 - val_loss: 0.4769 - val_accuracy: 0.7895\n",
      "Epoch 188/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4574 - accuracy: 0.7884 - val_loss: 0.4778 - val_accuracy: 0.8016\n",
      "Epoch 189/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4629 - accuracy: 0.7824 - val_loss: 0.4756 - val_accuracy: 0.7935\n",
      "Epoch 190/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4580 - accuracy: 0.7804 - val_loss: 0.4762 - val_accuracy: 0.7976\n",
      "Epoch 191/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4609 - accuracy: 0.7745 - val_loss: 0.4772 - val_accuracy: 0.7895\n",
      "Epoch 192/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4532 - accuracy: 0.7924 - val_loss: 0.4774 - val_accuracy: 0.7854\n",
      "Epoch 193/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4556 - accuracy: 0.7884 - val_loss: 0.4774 - val_accuracy: 0.7976\n",
      "Epoch 194/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4558 - accuracy: 0.7864 - val_loss: 0.4779 - val_accuracy: 0.8016\n",
      "Epoch 195/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4558 - accuracy: 0.7824 - val_loss: 0.4779 - val_accuracy: 0.7935\n",
      "Epoch 196/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4616 - accuracy: 0.7824 - val_loss: 0.4776 - val_accuracy: 0.7854\n",
      "Epoch 197/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4566 - accuracy: 0.7964 - val_loss: 0.4780 - val_accuracy: 0.7854\n",
      "Epoch 198/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4616 - accuracy: 0.7824 - val_loss: 0.4781 - val_accuracy: 0.7895\n",
      "Epoch 199/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4588 - accuracy: 0.7844 - val_loss: 0.4785 - val_accuracy: 0.7854\n",
      "Epoch 200/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4629 - accuracy: 0.7844 - val_loss: 0.4799 - val_accuracy: 0.7976\n",
      "Epoch 201/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4666 - accuracy: 0.7725 - val_loss: 0.4781 - val_accuracy: 0.7854\n",
      "Epoch 202/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4577 - accuracy: 0.7864 - val_loss: 0.4786 - val_accuracy: 0.7895\n",
      "Epoch 203/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4581 - accuracy: 0.7924 - val_loss: 0.4784 - val_accuracy: 0.7854\n",
      "Epoch 204/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4533 - accuracy: 0.7924 - val_loss: 0.4782 - val_accuracy: 0.7935\n",
      "Epoch 205/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4616 - accuracy: 0.7824 - val_loss: 0.4831 - val_accuracy: 0.8016\n",
      "Epoch 206/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4557 - accuracy: 0.7844 - val_loss: 0.4779 - val_accuracy: 0.7854\n",
      "Epoch 207/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4503 - accuracy: 0.7844 - val_loss: 0.4773 - val_accuracy: 0.7854\n",
      "Epoch 208/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4570 - accuracy: 0.7864 - val_loss: 0.4771 - val_accuracy: 0.7935\n",
      "Epoch 209/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4572 - accuracy: 0.7964 - val_loss: 0.4784 - val_accuracy: 0.7895\n",
      "Epoch 210/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4529 - accuracy: 0.7844 - val_loss: 0.4792 - val_accuracy: 0.7854\n",
      "Epoch 211/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4580 - accuracy: 0.7745 - val_loss: 0.4804 - val_accuracy: 0.8016\n",
      "Epoch 212/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4593 - accuracy: 0.7844 - val_loss: 0.4773 - val_accuracy: 0.7854\n",
      "Epoch 213/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4668 - accuracy: 0.7745 - val_loss: 0.4776 - val_accuracy: 0.7935\n",
      "Epoch 214/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4574 - accuracy: 0.7824 - val_loss: 0.4765 - val_accuracy: 0.7854\n",
      "Epoch 215/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4543 - accuracy: 0.7844 - val_loss: 0.4765 - val_accuracy: 0.7935\n",
      "Epoch 216/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4609 - accuracy: 0.7964 - val_loss: 0.4786 - val_accuracy: 0.8057\n",
      "Epoch 217/1600\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.4442 - accuracy: 0.78 - 0s 3ms/step - loss: 0.4594 - accuracy: 0.7705 - val_loss: 0.4790 - val_accuracy: 0.7976\n",
      "Epoch 218/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4571 - accuracy: 0.7844 - val_loss: 0.4770 - val_accuracy: 0.7935\n",
      "Epoch 219/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4531 - accuracy: 0.7924 - val_loss: 0.4784 - val_accuracy: 0.7976\n",
      "Epoch 220/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4596 - accuracy: 0.7864 - val_loss: 0.4783 - val_accuracy: 0.7935\n",
      "Epoch 221/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4619 - accuracy: 0.7725 - val_loss: 0.4788 - val_accuracy: 0.7895\n",
      "Epoch 222/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4600 - accuracy: 0.7864 - val_loss: 0.4780 - val_accuracy: 0.7854\n",
      "Epoch 223/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4553 - accuracy: 0.7784 - val_loss: 0.4789 - val_accuracy: 0.7935\n",
      "Epoch 224/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4539 - accuracy: 0.7824 - val_loss: 0.4790 - val_accuracy: 0.7854\n",
      "Epoch 225/1600\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4612 - accuracy: 0.7784 - val_loss: 0.4793 - val_accuracy: 0.7895\n",
      "Epoch 226/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4534 - accuracy: 0.7864 - val_loss: 0.4793 - val_accuracy: 0.7854\n",
      "Epoch 227/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4597 - accuracy: 0.7864 - val_loss: 0.4790 - val_accuracy: 0.7935\n",
      "Epoch 228/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4590 - accuracy: 0.7784 - val_loss: 0.4813 - val_accuracy: 0.8016\n",
      "Epoch 229/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4616 - accuracy: 0.7924 - val_loss: 0.4776 - val_accuracy: 0.7895\n",
      "Epoch 230/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4697 - accuracy: 0.7725 - val_loss: 0.4796 - val_accuracy: 0.7976\n",
      "Epoch 231/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4630 - accuracy: 0.7824 - val_loss: 0.4785 - val_accuracy: 0.7814\n",
      "Epoch 232/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4646 - accuracy: 0.7804 - val_loss: 0.4804 - val_accuracy: 0.7976\n",
      "Epoch 233/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4606 - accuracy: 0.7784 - val_loss: 0.4779 - val_accuracy: 0.7814\n",
      "Epoch 234/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4603 - accuracy: 0.7804 - val_loss: 0.4793 - val_accuracy: 0.7935\n",
      "Epoch 235/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4593 - accuracy: 0.7844 - val_loss: 0.4784 - val_accuracy: 0.7854\n",
      "Epoch 236/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4613 - accuracy: 0.7824 - val_loss: 0.4782 - val_accuracy: 0.7814\n",
      "Epoch 237/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4558 - accuracy: 0.7804 - val_loss: 0.4776 - val_accuracy: 0.7854\n",
      "Epoch 238/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4581 - accuracy: 0.7884 - val_loss: 0.4777 - val_accuracy: 0.7935\n",
      "Epoch 239/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4579 - accuracy: 0.7864 - val_loss: 0.4771 - val_accuracy: 0.7895\n",
      "Epoch 240/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4502 - accuracy: 0.7884 - val_loss: 0.4785 - val_accuracy: 0.7935\n",
      "Epoch 241/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4607 - accuracy: 0.7745 - val_loss: 0.4795 - val_accuracy: 0.7895\n",
      "Epoch 242/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4652 - accuracy: 0.7804 - val_loss: 0.4804 - val_accuracy: 0.7895\n",
      "Epoch 243/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4627 - accuracy: 0.7864 - val_loss: 0.4798 - val_accuracy: 0.7854\n",
      "Epoch 244/1600\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4647 - accuracy: 0.7725 - val_loss: 0.4788 - val_accuracy: 0.7854\n",
      "Epoch 245/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4540 - accuracy: 0.7944 - val_loss: 0.4783 - val_accuracy: 0.7895\n",
      "Epoch 246/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4583 - accuracy: 0.7764 - val_loss: 0.4797 - val_accuracy: 0.7935\n",
      "Epoch 247/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4573 - accuracy: 0.7884 - val_loss: 0.4788 - val_accuracy: 0.7895\n",
      "Epoch 248/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4556 - accuracy: 0.7784 - val_loss: 0.4817 - val_accuracy: 0.8016\n",
      "Epoch 249/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4569 - accuracy: 0.7764 - val_loss: 0.4795 - val_accuracy: 0.7854\n",
      "Epoch 250/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4613 - accuracy: 0.7864 - val_loss: 0.4776 - val_accuracy: 0.7935\n",
      "Epoch 251/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4629 - accuracy: 0.7864 - val_loss: 0.4819 - val_accuracy: 0.7895\n",
      "Epoch 252/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4530 - accuracy: 0.7864 - val_loss: 0.4812 - val_accuracy: 0.7895\n",
      "Epoch 253/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4538 - accuracy: 0.7824 - val_loss: 0.4803 - val_accuracy: 0.7854\n",
      "Epoch 254/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4583 - accuracy: 0.7764 - val_loss: 0.4805 - val_accuracy: 0.7854\n",
      "Epoch 255/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4627 - accuracy: 0.7725 - val_loss: 0.4803 - val_accuracy: 0.7854\n",
      "Epoch 256/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4580 - accuracy: 0.7924 - val_loss: 0.4785 - val_accuracy: 0.7854\n",
      "Epoch 257/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4550 - accuracy: 0.7844 - val_loss: 0.4787 - val_accuracy: 0.7895\n",
      "Epoch 258/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4586 - accuracy: 0.7844 - val_loss: 0.4791 - val_accuracy: 0.7976\n",
      "Epoch 259/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4593 - accuracy: 0.7705 - val_loss: 0.4786 - val_accuracy: 0.7976\n",
      "Epoch 260/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4534 - accuracy: 0.7904 - val_loss: 0.4789 - val_accuracy: 0.7854\n",
      "Epoch 261/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4543 - accuracy: 0.7844 - val_loss: 0.4789 - val_accuracy: 0.7854\n",
      "Epoch 262/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4583 - accuracy: 0.7884 - val_loss: 0.4794 - val_accuracy: 0.7895\n",
      "Epoch 263/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4549 - accuracy: 0.7864 - val_loss: 0.4790 - val_accuracy: 0.7895\n",
      "Epoch 264/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4507 - accuracy: 0.8024 - val_loss: 0.4796 - val_accuracy: 0.7976\n",
      "Epoch 265/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4596 - accuracy: 0.7844 - val_loss: 0.4782 - val_accuracy: 0.7976\n",
      "Epoch 266/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4542 - accuracy: 0.7804 - val_loss: 0.4789 - val_accuracy: 0.7976\n",
      "Epoch 267/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4609 - accuracy: 0.7764 - val_loss: 0.4781 - val_accuracy: 0.8057\n",
      "Epoch 268/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4547 - accuracy: 0.7784 - val_loss: 0.4778 - val_accuracy: 0.7976\n",
      "Epoch 269/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4560 - accuracy: 0.7844 - val_loss: 0.4778 - val_accuracy: 0.8057\n",
      "Epoch 270/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4600 - accuracy: 0.7884 - val_loss: 0.4794 - val_accuracy: 0.7895\n",
      "Epoch 271/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4596 - accuracy: 0.7705 - val_loss: 0.4802 - val_accuracy: 0.7976\n",
      "Epoch 272/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4537 - accuracy: 0.7924 - val_loss: 0.4783 - val_accuracy: 0.7976\n",
      "Epoch 273/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4538 - accuracy: 0.7864 - val_loss: 0.4778 - val_accuracy: 0.7854\n",
      "Epoch 274/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4615 - accuracy: 0.7924 - val_loss: 0.4805 - val_accuracy: 0.8016\n",
      "Epoch 275/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4592 - accuracy: 0.7764 - val_loss: 0.4798 - val_accuracy: 0.7854\n",
      "Epoch 276/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4584 - accuracy: 0.7864 - val_loss: 0.4806 - val_accuracy: 0.7976\n",
      "Epoch 277/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4533 - accuracy: 0.7784 - val_loss: 0.4799 - val_accuracy: 0.7895\n",
      "Epoch 278/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4581 - accuracy: 0.7804 - val_loss: 0.4783 - val_accuracy: 0.7895\n",
      "Epoch 279/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4567 - accuracy: 0.7804 - val_loss: 0.4798 - val_accuracy: 0.7895\n",
      "Epoch 280/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4583 - accuracy: 0.7864 - val_loss: 0.4801 - val_accuracy: 0.7895\n",
      "Epoch 281/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4625 - accuracy: 0.7824 - val_loss: 0.4787 - val_accuracy: 0.7895\n",
      "Epoch 282/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4558 - accuracy: 0.7824 - val_loss: 0.4817 - val_accuracy: 0.8016\n",
      "Epoch 283/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4542 - accuracy: 0.7884 - val_loss: 0.4794 - val_accuracy: 0.7854\n",
      "Epoch 284/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4556 - accuracy: 0.8004 - val_loss: 0.4792 - val_accuracy: 0.7854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 285/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4547 - accuracy: 0.7864 - val_loss: 0.4811 - val_accuracy: 0.8057\n",
      "Epoch 286/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4566 - accuracy: 0.7804 - val_loss: 0.4784 - val_accuracy: 0.7935\n",
      "Epoch 287/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4591 - accuracy: 0.7844 - val_loss: 0.4788 - val_accuracy: 0.7854\n",
      "Epoch 288/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4603 - accuracy: 0.7904 - val_loss: 0.4800 - val_accuracy: 0.7895\n",
      "Epoch 289/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4558 - accuracy: 0.7864 - val_loss: 0.4792 - val_accuracy: 0.7935\n",
      "Epoch 290/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4559 - accuracy: 0.7804 - val_loss: 0.4788 - val_accuracy: 0.7895\n",
      "Epoch 291/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4517 - accuracy: 0.7884 - val_loss: 0.4786 - val_accuracy: 0.7895\n",
      "Epoch 292/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4577 - accuracy: 0.7725 - val_loss: 0.4785 - val_accuracy: 0.7895\n",
      "Epoch 293/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4562 - accuracy: 0.7764 - val_loss: 0.4792 - val_accuracy: 0.7895\n",
      "Epoch 294/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4561 - accuracy: 0.7824 - val_loss: 0.4806 - val_accuracy: 0.7976\n",
      "Epoch 295/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4599 - accuracy: 0.7784 - val_loss: 0.4793 - val_accuracy: 0.7895\n",
      "Epoch 296/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4556 - accuracy: 0.7764 - val_loss: 0.4795 - val_accuracy: 0.8016\n",
      "Epoch 297/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4509 - accuracy: 0.7904 - val_loss: 0.4790 - val_accuracy: 0.8057\n",
      "Epoch 298/1600\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4586 - accuracy: 0.7924 - val_loss: 0.4780 - val_accuracy: 0.7895\n",
      "Epoch 299/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4567 - accuracy: 0.7844 - val_loss: 0.4792 - val_accuracy: 0.8057\n",
      "Epoch 300/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4527 - accuracy: 0.7804 - val_loss: 0.4789 - val_accuracy: 0.7935\n",
      "Epoch 301/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4572 - accuracy: 0.7884 - val_loss: 0.4782 - val_accuracy: 0.7935\n",
      "Epoch 302/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4471 - accuracy: 0.7784 - val_loss: 0.4798 - val_accuracy: 0.8016\n",
      "Epoch 303/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4555 - accuracy: 0.7864 - val_loss: 0.4797 - val_accuracy: 0.8057\n",
      "Epoch 304/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4577 - accuracy: 0.7884 - val_loss: 0.4784 - val_accuracy: 0.7935\n",
      "Epoch 305/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4514 - accuracy: 0.7824 - val_loss: 0.4811 - val_accuracy: 0.8097\n",
      "Epoch 306/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4496 - accuracy: 0.7864 - val_loss: 0.4782 - val_accuracy: 0.7854\n",
      "Epoch 307/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4569 - accuracy: 0.7984 - val_loss: 0.4799 - val_accuracy: 0.7895\n",
      "Epoch 308/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4551 - accuracy: 0.7904 - val_loss: 0.4804 - val_accuracy: 0.7895\n",
      "Epoch 309/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4561 - accuracy: 0.7864 - val_loss: 0.4803 - val_accuracy: 0.8016\n",
      "Epoch 310/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4569 - accuracy: 0.7725 - val_loss: 0.4794 - val_accuracy: 0.7854\n",
      "Epoch 311/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4556 - accuracy: 0.7964 - val_loss: 0.4789 - val_accuracy: 0.7976\n",
      "Epoch 312/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4527 - accuracy: 0.7844 - val_loss: 0.4779 - val_accuracy: 0.7854\n",
      "Epoch 313/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4523 - accuracy: 0.7804 - val_loss: 0.4789 - val_accuracy: 0.8057\n",
      "Epoch 314/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4607 - accuracy: 0.7764 - val_loss: 0.4781 - val_accuracy: 0.7854\n",
      "Epoch 315/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4545 - accuracy: 0.7904 - val_loss: 0.4798 - val_accuracy: 0.7976\n",
      "Epoch 316/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4525 - accuracy: 0.7804 - val_loss: 0.4785 - val_accuracy: 0.7895\n",
      "Epoch 317/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4553 - accuracy: 0.7884 - val_loss: 0.4786 - val_accuracy: 0.7814\n",
      "Epoch 318/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4574 - accuracy: 0.7824 - val_loss: 0.4806 - val_accuracy: 0.8016\n",
      "Epoch 319/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4545 - accuracy: 0.7924 - val_loss: 0.4790 - val_accuracy: 0.7935\n",
      "Epoch 320/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4544 - accuracy: 0.7784 - val_loss: 0.4788 - val_accuracy: 0.8016\n",
      "Epoch 321/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4511 - accuracy: 0.7745 - val_loss: 0.4784 - val_accuracy: 0.7895\n",
      "Epoch 322/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4517 - accuracy: 0.7705 - val_loss: 0.4778 - val_accuracy: 0.7895\n",
      "Epoch 323/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4546 - accuracy: 0.7784 - val_loss: 0.4779 - val_accuracy: 0.8016\n",
      "Epoch 324/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4602 - accuracy: 0.7804 - val_loss: 0.4788 - val_accuracy: 0.8016\n",
      "Epoch 325/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4553 - accuracy: 0.7725 - val_loss: 0.4782 - val_accuracy: 0.7854\n",
      "Epoch 326/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4489 - accuracy: 0.8004 - val_loss: 0.4789 - val_accuracy: 0.8016\n",
      "Epoch 327/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4553 - accuracy: 0.7904 - val_loss: 0.4785 - val_accuracy: 0.7895\n",
      "Epoch 328/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4579 - accuracy: 0.7784 - val_loss: 0.4779 - val_accuracy: 0.7935\n",
      "Epoch 329/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4537 - accuracy: 0.7884 - val_loss: 0.4785 - val_accuracy: 0.7854\n",
      "Epoch 330/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4547 - accuracy: 0.7824 - val_loss: 0.4797 - val_accuracy: 0.7935\n",
      "Epoch 331/1600\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4548 - accuracy: 0.7884 - val_loss: 0.4792 - val_accuracy: 0.7895\n",
      "Epoch 332/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4538 - accuracy: 0.7864 - val_loss: 0.4805 - val_accuracy: 0.8057\n",
      "Epoch 333/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4505 - accuracy: 0.7864 - val_loss: 0.4787 - val_accuracy: 0.8097\n",
      "Epoch 334/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4542 - accuracy: 0.7964 - val_loss: 0.4791 - val_accuracy: 0.7895\n",
      "Epoch 335/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4582 - accuracy: 0.7864 - val_loss: 0.4786 - val_accuracy: 0.8057\n",
      "Epoch 336/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4526 - accuracy: 0.7884 - val_loss: 0.4785 - val_accuracy: 0.7895\n",
      "Epoch 337/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4540 - accuracy: 0.7924 - val_loss: 0.4794 - val_accuracy: 0.7895\n",
      "Epoch 338/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4585 - accuracy: 0.7944 - val_loss: 0.4808 - val_accuracy: 0.8097\n",
      "Epoch 339/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4559 - accuracy: 0.7904 - val_loss: 0.4780 - val_accuracy: 0.7854\n",
      "Epoch 340/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4507 - accuracy: 0.7944 - val_loss: 0.4789 - val_accuracy: 0.7935\n",
      "Epoch 341/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4536 - accuracy: 0.7864 - val_loss: 0.4789 - val_accuracy: 0.7895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 342/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4513 - accuracy: 0.7904 - val_loss: 0.4796 - val_accuracy: 0.7895\n",
      "Epoch 343/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4564 - accuracy: 0.7685 - val_loss: 0.4799 - val_accuracy: 0.8057\n",
      "Epoch 344/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4571 - accuracy: 0.7964 - val_loss: 0.4797 - val_accuracy: 0.7854\n",
      "Epoch 345/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4611 - accuracy: 0.7784 - val_loss: 0.4806 - val_accuracy: 0.7814\n",
      "Epoch 346/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4559 - accuracy: 0.8064 - val_loss: 0.4797 - val_accuracy: 0.7854\n",
      "Epoch 347/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4577 - accuracy: 0.7864 - val_loss: 0.4798 - val_accuracy: 0.7854\n",
      "Epoch 348/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4543 - accuracy: 0.7864 - val_loss: 0.4794 - val_accuracy: 0.7895\n",
      "Epoch 349/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4531 - accuracy: 0.7884 - val_loss: 0.4785 - val_accuracy: 0.7895\n",
      "Epoch 350/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4523 - accuracy: 0.7944 - val_loss: 0.4791 - val_accuracy: 0.7895\n",
      "Epoch 351/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4534 - accuracy: 0.7844 - val_loss: 0.4809 - val_accuracy: 0.8057\n",
      "Epoch 352/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4582 - accuracy: 0.7904 - val_loss: 0.4796 - val_accuracy: 0.8097\n",
      "Epoch 353/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4549 - accuracy: 0.7864 - val_loss: 0.4781 - val_accuracy: 0.8057\n",
      "Epoch 354/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4545 - accuracy: 0.7884 - val_loss: 0.4783 - val_accuracy: 0.8016\n",
      "Epoch 355/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4600 - accuracy: 0.7764 - val_loss: 0.4802 - val_accuracy: 0.7895\n",
      "Epoch 356/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4583 - accuracy: 0.7824 - val_loss: 0.4800 - val_accuracy: 0.8057\n",
      "Epoch 357/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4585 - accuracy: 0.7824 - val_loss: 0.4791 - val_accuracy: 0.7854\n",
      "Epoch 358/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4564 - accuracy: 0.7904 - val_loss: 0.4802 - val_accuracy: 0.7895\n",
      "Epoch 359/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4542 - accuracy: 0.7904 - val_loss: 0.4787 - val_accuracy: 0.7895\n",
      "Epoch 360/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4545 - accuracy: 0.7725 - val_loss: 0.4788 - val_accuracy: 0.8097\n",
      "Epoch 361/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4508 - accuracy: 0.7864 - val_loss: 0.4775 - val_accuracy: 0.7895\n",
      "Epoch 362/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4553 - accuracy: 0.7864 - val_loss: 0.4793 - val_accuracy: 0.8016\n",
      "Epoch 363/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4517 - accuracy: 0.7884 - val_loss: 0.4792 - val_accuracy: 0.7854\n",
      "Epoch 364/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4506 - accuracy: 0.7904 - val_loss: 0.4793 - val_accuracy: 0.8057\n",
      "Epoch 365/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4550 - accuracy: 0.7824 - val_loss: 0.4797 - val_accuracy: 0.8057\n",
      "Epoch 366/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4548 - accuracy: 0.7745 - val_loss: 0.4783 - val_accuracy: 0.8097\n",
      "Epoch 367/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4512 - accuracy: 0.7824 - val_loss: 0.4783 - val_accuracy: 0.8016\n",
      "Epoch 368/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4629 - accuracy: 0.7804 - val_loss: 0.4790 - val_accuracy: 0.7814\n",
      "Epoch 369/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4561 - accuracy: 0.7745 - val_loss: 0.4794 - val_accuracy: 0.8016\n",
      "Epoch 370/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4516 - accuracy: 0.7804 - val_loss: 0.4788 - val_accuracy: 0.8097\n",
      "Epoch 371/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4449 - accuracy: 0.7964 - val_loss: 0.4795 - val_accuracy: 0.7814\n",
      "Epoch 372/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4578 - accuracy: 0.7944 - val_loss: 0.4821 - val_accuracy: 0.7976\n",
      "Epoch 373/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4526 - accuracy: 0.7745 - val_loss: 0.4793 - val_accuracy: 0.7814\n",
      "Epoch 374/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4539 - accuracy: 0.7824 - val_loss: 0.4790 - val_accuracy: 0.7854\n",
      "Epoch 375/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4526 - accuracy: 0.7725 - val_loss: 0.4806 - val_accuracy: 0.8057\n",
      "Epoch 376/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4603 - accuracy: 0.7804 - val_loss: 0.4792 - val_accuracy: 0.7895\n",
      "Epoch 377/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4537 - accuracy: 0.7804 - val_loss: 0.4793 - val_accuracy: 0.7854\n",
      "Epoch 378/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4531 - accuracy: 0.7864 - val_loss: 0.4792 - val_accuracy: 0.7854\n",
      "Epoch 379/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4568 - accuracy: 0.7844 - val_loss: 0.4794 - val_accuracy: 0.7895\n",
      "Epoch 380/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4521 - accuracy: 0.7924 - val_loss: 0.4793 - val_accuracy: 0.7895\n",
      "Epoch 381/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4511 - accuracy: 0.7864 - val_loss: 0.4794 - val_accuracy: 0.7854\n",
      "Epoch 382/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4548 - accuracy: 0.7924 - val_loss: 0.4794 - val_accuracy: 0.7976\n",
      "Epoch 383/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4494 - accuracy: 0.8024 - val_loss: 0.4797 - val_accuracy: 0.8057\n",
      "Epoch 384/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4469 - accuracy: 0.7924 - val_loss: 0.4793 - val_accuracy: 0.8057\n",
      "Epoch 385/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4578 - accuracy: 0.7844 - val_loss: 0.4784 - val_accuracy: 0.8057\n",
      "Epoch 386/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4535 - accuracy: 0.7924 - val_loss: 0.4794 - val_accuracy: 0.7854\n",
      "Epoch 387/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4565 - accuracy: 0.7844 - val_loss: 0.4799 - val_accuracy: 0.8016\n",
      "Epoch 388/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4521 - accuracy: 0.8004 - val_loss: 0.4787 - val_accuracy: 0.8057\n",
      "Epoch 389/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4531 - accuracy: 0.7904 - val_loss: 0.4790 - val_accuracy: 0.8016\n",
      "Epoch 390/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4545 - accuracy: 0.7824 - val_loss: 0.4779 - val_accuracy: 0.7935\n",
      "Epoch 391/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4555 - accuracy: 0.7924 - val_loss: 0.4799 - val_accuracy: 0.8016\n",
      "Epoch 392/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4463 - accuracy: 0.7924 - val_loss: 0.4788 - val_accuracy: 0.8097\n",
      "Epoch 393/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4571 - accuracy: 0.7764 - val_loss: 0.4782 - val_accuracy: 0.7854\n",
      "Epoch 394/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4553 - accuracy: 0.7924 - val_loss: 0.4793 - val_accuracy: 0.8057\n",
      "Epoch 395/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4500 - accuracy: 0.7804 - val_loss: 0.4785 - val_accuracy: 0.8016\n",
      "Epoch 396/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4520 - accuracy: 0.7924 - val_loss: 0.4790 - val_accuracy: 0.8057\n",
      "Epoch 397/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4553 - accuracy: 0.7864 - val_loss: 0.4772 - val_accuracy: 0.7976\n",
      "Epoch 398/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4558 - accuracy: 0.7884 - val_loss: 0.4807 - val_accuracy: 0.8016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 399/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4484 - accuracy: 0.7904 - val_loss: 0.4794 - val_accuracy: 0.7976\n",
      "Epoch 400/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4486 - accuracy: 0.7884 - val_loss: 0.4793 - val_accuracy: 0.7976\n",
      "Epoch 401/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4490 - accuracy: 0.7784 - val_loss: 0.4794 - val_accuracy: 0.7854\n",
      "Epoch 402/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4588 - accuracy: 0.7804 - val_loss: 0.4810 - val_accuracy: 0.8097\n",
      "Epoch 403/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4616 - accuracy: 0.7804 - val_loss: 0.4789 - val_accuracy: 0.7895\n",
      "Epoch 404/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4520 - accuracy: 0.7804 - val_loss: 0.4796 - val_accuracy: 0.7854\n",
      "Epoch 405/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4493 - accuracy: 0.8004 - val_loss: 0.4812 - val_accuracy: 0.8097\n",
      "Epoch 406/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4529 - accuracy: 0.7984 - val_loss: 0.4807 - val_accuracy: 0.8057\n",
      "Epoch 407/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4534 - accuracy: 0.7904 - val_loss: 0.4792 - val_accuracy: 0.8138\n",
      "Epoch 408/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4555 - accuracy: 0.7924 - val_loss: 0.4798 - val_accuracy: 0.8057\n",
      "Epoch 409/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4533 - accuracy: 0.7924 - val_loss: 0.4794 - val_accuracy: 0.8057\n",
      "Epoch 410/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4486 - accuracy: 0.7864 - val_loss: 0.4779 - val_accuracy: 0.7976\n",
      "Epoch 411/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4503 - accuracy: 0.7864 - val_loss: 0.4786 - val_accuracy: 0.8057\n",
      "Epoch 412/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4479 - accuracy: 0.7824 - val_loss: 0.4805 - val_accuracy: 0.8057\n",
      "Epoch 413/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4602 - accuracy: 0.7864 - val_loss: 0.4784 - val_accuracy: 0.8057\n",
      "Epoch 414/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4488 - accuracy: 0.7904 - val_loss: 0.4798 - val_accuracy: 0.8057\n",
      "Epoch 415/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4491 - accuracy: 0.7924 - val_loss: 0.4808 - val_accuracy: 0.8057\n",
      "Epoch 416/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4528 - accuracy: 0.7884 - val_loss: 0.4796 - val_accuracy: 0.8057\n",
      "Epoch 417/1600\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4500 - accuracy: 0.8004 - val_loss: 0.4784 - val_accuracy: 0.8097\n",
      "Epoch 418/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4578 - accuracy: 0.7884 - val_loss: 0.4797 - val_accuracy: 0.8057\n",
      "Epoch 419/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4536 - accuracy: 0.7824 - val_loss: 0.4801 - val_accuracy: 0.7976\n",
      "Epoch 420/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4463 - accuracy: 0.7924 - val_loss: 0.4801 - val_accuracy: 0.8057\n",
      "Epoch 421/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4539 - accuracy: 0.7884 - val_loss: 0.4798 - val_accuracy: 0.8057\n",
      "Epoch 422/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4508 - accuracy: 0.7804 - val_loss: 0.4791 - val_accuracy: 0.7976\n",
      "Epoch 423/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4521 - accuracy: 0.7904 - val_loss: 0.4787 - val_accuracy: 0.7935\n",
      "Epoch 424/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4537 - accuracy: 0.7864 - val_loss: 0.4803 - val_accuracy: 0.8057\n",
      "Epoch 425/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4509 - accuracy: 0.7844 - val_loss: 0.4795 - val_accuracy: 0.7976\n",
      "Epoch 426/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4502 - accuracy: 0.8004 - val_loss: 0.4795 - val_accuracy: 0.8097\n",
      "Epoch 427/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4583 - accuracy: 0.7884 - val_loss: 0.4807 - val_accuracy: 0.8057\n",
      "Epoch 428/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4516 - accuracy: 0.7944 - val_loss: 0.4804 - val_accuracy: 0.8097\n",
      "Epoch 429/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4496 - accuracy: 0.7864 - val_loss: 0.4794 - val_accuracy: 0.7854\n",
      "Epoch 430/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4513 - accuracy: 0.7884 - val_loss: 0.4814 - val_accuracy: 0.8097\n",
      "Epoch 431/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4560 - accuracy: 0.7804 - val_loss: 0.4790 - val_accuracy: 0.8016\n",
      "Epoch 432/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4584 - accuracy: 0.7844 - val_loss: 0.4799 - val_accuracy: 0.8097\n",
      "Epoch 433/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4531 - accuracy: 0.7745 - val_loss: 0.4798 - val_accuracy: 0.7976\n",
      "Epoch 434/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4512 - accuracy: 0.7924 - val_loss: 0.4802 - val_accuracy: 0.7976\n",
      "Epoch 435/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4460 - accuracy: 0.7984 - val_loss: 0.4799 - val_accuracy: 0.7895\n",
      "Epoch 436/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4504 - accuracy: 0.7864 - val_loss: 0.4818 - val_accuracy: 0.8057\n",
      "Epoch 437/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4461 - accuracy: 0.7944 - val_loss: 0.4800 - val_accuracy: 0.7854\n",
      "Epoch 438/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4528 - accuracy: 0.7745 - val_loss: 0.4809 - val_accuracy: 0.8097\n",
      "Epoch 439/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4488 - accuracy: 0.7884 - val_loss: 0.4795 - val_accuracy: 0.8097\n",
      "Epoch 440/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4528 - accuracy: 0.7984 - val_loss: 0.4794 - val_accuracy: 0.8057\n",
      "Epoch 441/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4541 - accuracy: 0.7844 - val_loss: 0.4802 - val_accuracy: 0.7854\n",
      "Epoch 442/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4480 - accuracy: 0.7984 - val_loss: 0.4796 - val_accuracy: 0.8097\n",
      "Epoch 443/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4539 - accuracy: 0.7884 - val_loss: 0.4796 - val_accuracy: 0.8057\n",
      "Epoch 444/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4523 - accuracy: 0.7904 - val_loss: 0.4803 - val_accuracy: 0.7935\n",
      "Epoch 445/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4521 - accuracy: 0.7804 - val_loss: 0.4822 - val_accuracy: 0.8057\n",
      "Epoch 446/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4511 - accuracy: 0.7824 - val_loss: 0.4806 - val_accuracy: 0.7854\n",
      "Epoch 447/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4524 - accuracy: 0.7844 - val_loss: 0.4813 - val_accuracy: 0.7814\n",
      "Epoch 448/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4499 - accuracy: 0.7904 - val_loss: 0.4804 - val_accuracy: 0.7976\n",
      "Epoch 449/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4552 - accuracy: 0.7864 - val_loss: 0.4810 - val_accuracy: 0.7976\n",
      "Epoch 450/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4447 - accuracy: 0.8024 - val_loss: 0.4793 - val_accuracy: 0.8097\n",
      "Epoch 451/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4503 - accuracy: 0.7864 - val_loss: 0.4814 - val_accuracy: 0.8057\n",
      "Epoch 452/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4577 - accuracy: 0.7904 - val_loss: 0.4795 - val_accuracy: 0.7895\n",
      "Epoch 453/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4528 - accuracy: 0.7804 - val_loss: 0.4788 - val_accuracy: 0.7976\n",
      "Epoch 454/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4502 - accuracy: 0.7844 - val_loss: 0.4817 - val_accuracy: 0.8057\n",
      "Epoch 455/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4456 - accuracy: 0.8004 - val_loss: 0.4801 - val_accuracy: 0.8097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 456/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4487 - accuracy: 0.7904 - val_loss: 0.4795 - val_accuracy: 0.8057\n",
      "Epoch 457/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4554 - accuracy: 0.7824 - val_loss: 0.4798 - val_accuracy: 0.8057\n",
      "Epoch 458/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4516 - accuracy: 0.7824 - val_loss: 0.4798 - val_accuracy: 0.7935\n",
      "Epoch 459/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4510 - accuracy: 0.7904 - val_loss: 0.4805 - val_accuracy: 0.7976\n",
      "Epoch 460/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4503 - accuracy: 0.7904 - val_loss: 0.4816 - val_accuracy: 0.8097\n",
      "Epoch 461/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4502 - accuracy: 0.7864 - val_loss: 0.4810 - val_accuracy: 0.8016\n",
      "Epoch 462/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4497 - accuracy: 0.7804 - val_loss: 0.4799 - val_accuracy: 0.8057\n",
      "Epoch 463/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4516 - accuracy: 0.7824 - val_loss: 0.4798 - val_accuracy: 0.8057\n",
      "Epoch 464/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4576 - accuracy: 0.7804 - val_loss: 0.4794 - val_accuracy: 0.7935\n",
      "Epoch 465/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4483 - accuracy: 0.7964 - val_loss: 0.4810 - val_accuracy: 0.8057\n",
      "Epoch 466/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4517 - accuracy: 0.7964 - val_loss: 0.4796 - val_accuracy: 0.8097\n",
      "Epoch 467/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4578 - accuracy: 0.7705 - val_loss: 0.4793 - val_accuracy: 0.8057\n",
      "Epoch 468/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4550 - accuracy: 0.7784 - val_loss: 0.4796 - val_accuracy: 0.8097\n",
      "Epoch 469/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4485 - accuracy: 0.8004 - val_loss: 0.4800 - val_accuracy: 0.8097\n",
      "Epoch 470/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4502 - accuracy: 0.7964 - val_loss: 0.4796 - val_accuracy: 0.7935\n",
      "Epoch 471/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4547 - accuracy: 0.7844 - val_loss: 0.4813 - val_accuracy: 0.8057\n",
      "Epoch 472/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4515 - accuracy: 0.7784 - val_loss: 0.4805 - val_accuracy: 0.8097\n",
      "Epoch 473/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4528 - accuracy: 0.7804 - val_loss: 0.4794 - val_accuracy: 0.8097\n",
      "Epoch 474/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4485 - accuracy: 0.7864 - val_loss: 0.4803 - val_accuracy: 0.7935\n",
      "Epoch 475/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4507 - accuracy: 0.7804 - val_loss: 0.4796 - val_accuracy: 0.8097\n",
      "Epoch 476/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4495 - accuracy: 0.7824 - val_loss: 0.4795 - val_accuracy: 0.8057\n",
      "Epoch 477/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4482 - accuracy: 0.7884 - val_loss: 0.4790 - val_accuracy: 0.8097\n",
      "Epoch 478/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4473 - accuracy: 0.7884 - val_loss: 0.4802 - val_accuracy: 0.8016\n",
      "Epoch 479/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4462 - accuracy: 0.7924 - val_loss: 0.4801 - val_accuracy: 0.8097\n",
      "Epoch 480/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4482 - accuracy: 0.7964 - val_loss: 0.4808 - val_accuracy: 0.8097\n",
      "Epoch 481/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4474 - accuracy: 0.7864 - val_loss: 0.4793 - val_accuracy: 0.8016\n",
      "Epoch 482/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4500 - accuracy: 0.7884 - val_loss: 0.4803 - val_accuracy: 0.8057\n",
      "Epoch 483/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4461 - accuracy: 0.7904 - val_loss: 0.4804 - val_accuracy: 0.8016\n",
      "Epoch 484/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4481 - accuracy: 0.7864 - val_loss: 0.4814 - val_accuracy: 0.8057\n",
      "Epoch 485/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4541 - accuracy: 0.7944 - val_loss: 0.4820 - val_accuracy: 0.8016\n",
      "Epoch 486/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4456 - accuracy: 0.7884 - val_loss: 0.4815 - val_accuracy: 0.7854\n",
      "Epoch 487/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4509 - accuracy: 0.7924 - val_loss: 0.4820 - val_accuracy: 0.8016\n",
      "Epoch 488/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4510 - accuracy: 0.7904 - val_loss: 0.4812 - val_accuracy: 0.7854\n",
      "Epoch 489/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4499 - accuracy: 0.7884 - val_loss: 0.4837 - val_accuracy: 0.8057\n",
      "Epoch 490/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4512 - accuracy: 0.7964 - val_loss: 0.4798 - val_accuracy: 0.8057\n",
      "Epoch 491/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4499 - accuracy: 0.7904 - val_loss: 0.4794 - val_accuracy: 0.8057\n",
      "Epoch 492/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4511 - accuracy: 0.7864 - val_loss: 0.4816 - val_accuracy: 0.8057\n",
      "Epoch 493/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4573 - accuracy: 0.7864 - val_loss: 0.4798 - val_accuracy: 0.7895\n",
      "Epoch 494/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4440 - accuracy: 0.8044 - val_loss: 0.4797 - val_accuracy: 0.8057\n",
      "Epoch 495/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4480 - accuracy: 0.7884 - val_loss: 0.4811 - val_accuracy: 0.8016\n",
      "Epoch 496/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4518 - accuracy: 0.7924 - val_loss: 0.4801 - val_accuracy: 0.8097\n",
      "Epoch 497/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4494 - accuracy: 0.7924 - val_loss: 0.4796 - val_accuracy: 0.7935\n",
      "Epoch 498/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4520 - accuracy: 0.7904 - val_loss: 0.4812 - val_accuracy: 0.8097\n",
      "Epoch 499/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4542 - accuracy: 0.7804 - val_loss: 0.4792 - val_accuracy: 0.7935\n",
      "Epoch 500/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4514 - accuracy: 0.8004 - val_loss: 0.4797 - val_accuracy: 0.8097\n",
      "Epoch 501/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4414 - accuracy: 0.8184 - val_loss: 0.4804 - val_accuracy: 0.8057\n",
      "Epoch 502/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4450 - accuracy: 0.7964 - val_loss: 0.4802 - val_accuracy: 0.8016\n",
      "Epoch 503/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4528 - accuracy: 0.7844 - val_loss: 0.4806 - val_accuracy: 0.8097\n",
      "Epoch 504/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4497 - accuracy: 0.7844 - val_loss: 0.4796 - val_accuracy: 0.7854\n",
      "Epoch 505/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4545 - accuracy: 0.7924 - val_loss: 0.4804 - val_accuracy: 0.8097\n",
      "Epoch 506/1600\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4475 - accuracy: 0.7904 - val_loss: 0.4800 - val_accuracy: 0.8016\n",
      "Epoch 507/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4461 - accuracy: 0.7944 - val_loss: 0.4813 - val_accuracy: 0.8097\n",
      "Epoch 508/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4499 - accuracy: 0.7944 - val_loss: 0.4801 - val_accuracy: 0.7935\n",
      "Epoch 509/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4486 - accuracy: 0.7944 - val_loss: 0.4825 - val_accuracy: 0.8016\n",
      "Epoch 510/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4500 - accuracy: 0.7964 - val_loss: 0.4818 - val_accuracy: 0.8016\n",
      "Epoch 511/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4484 - accuracy: 0.7884 - val_loss: 0.4812 - val_accuracy: 0.8016\n",
      "Epoch 512/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4521 - accuracy: 0.7864 - val_loss: 0.4809 - val_accuracy: 0.8057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 513/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4436 - accuracy: 0.8044 - val_loss: 0.4809 - val_accuracy: 0.8097\n",
      "Epoch 514/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4484 - accuracy: 0.7984 - val_loss: 0.4808 - val_accuracy: 0.8057\n",
      "Epoch 515/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4463 - accuracy: 0.8004 - val_loss: 0.4818 - val_accuracy: 0.8057\n",
      "Epoch 516/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4458 - accuracy: 0.8024 - val_loss: 0.4791 - val_accuracy: 0.8097\n",
      "Epoch 517/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4502 - accuracy: 0.7884 - val_loss: 0.4799 - val_accuracy: 0.8057\n",
      "Epoch 518/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4486 - accuracy: 0.8004 - val_loss: 0.4808 - val_accuracy: 0.8057\n",
      "Epoch 519/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4506 - accuracy: 0.7764 - val_loss: 0.4801 - val_accuracy: 0.8057\n",
      "Epoch 520/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4503 - accuracy: 0.7924 - val_loss: 0.4801 - val_accuracy: 0.8097\n",
      "Epoch 521/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4421 - accuracy: 0.7964 - val_loss: 0.4797 - val_accuracy: 0.8097\n",
      "Epoch 522/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4432 - accuracy: 0.7984 - val_loss: 0.4812 - val_accuracy: 0.8097\n",
      "Epoch 523/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4514 - accuracy: 0.7864 - val_loss: 0.4797 - val_accuracy: 0.8097\n",
      "Epoch 524/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4490 - accuracy: 0.7924 - val_loss: 0.4804 - val_accuracy: 0.7976\n",
      "Epoch 525/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4457 - accuracy: 0.8084 - val_loss: 0.4804 - val_accuracy: 0.8097\n",
      "Epoch 526/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4435 - accuracy: 0.8024 - val_loss: 0.4793 - val_accuracy: 0.8138\n",
      "Epoch 527/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4423 - accuracy: 0.7984 - val_loss: 0.4828 - val_accuracy: 0.8057\n",
      "Epoch 528/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4543 - accuracy: 0.7944 - val_loss: 0.4815 - val_accuracy: 0.7976\n",
      "Epoch 529/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4539 - accuracy: 0.7824 - val_loss: 0.4822 - val_accuracy: 0.8057\n",
      "Epoch 530/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4552 - accuracy: 0.7764 - val_loss: 0.4828 - val_accuracy: 0.7976\n",
      "Epoch 531/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4545 - accuracy: 0.7884 - val_loss: 0.4820 - val_accuracy: 0.7814\n",
      "Epoch 532/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4512 - accuracy: 0.7924 - val_loss: 0.4833 - val_accuracy: 0.8097\n",
      "Epoch 533/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4478 - accuracy: 0.7864 - val_loss: 0.4818 - val_accuracy: 0.8016\n",
      "Epoch 534/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4513 - accuracy: 0.7824 - val_loss: 0.4822 - val_accuracy: 0.7895\n",
      "Epoch 535/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4404 - accuracy: 0.8084 - val_loss: 0.4832 - val_accuracy: 0.8097\n",
      "Epoch 536/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4512 - accuracy: 0.7784 - val_loss: 0.4815 - val_accuracy: 0.8097\n",
      "Epoch 537/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4491 - accuracy: 0.8004 - val_loss: 0.4813 - val_accuracy: 0.7895\n",
      "Epoch 538/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4515 - accuracy: 0.7824 - val_loss: 0.4825 - val_accuracy: 0.8097\n",
      "Epoch 539/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4533 - accuracy: 0.7824 - val_loss: 0.4799 - val_accuracy: 0.8097\n",
      "Epoch 540/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4520 - accuracy: 0.7804 - val_loss: 0.4795 - val_accuracy: 0.7854\n",
      "Epoch 541/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4480 - accuracy: 0.7924 - val_loss: 0.4811 - val_accuracy: 0.8016\n",
      "Epoch 542/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4516 - accuracy: 0.7824 - val_loss: 0.4789 - val_accuracy: 0.8097\n",
      "Epoch 543/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4437 - accuracy: 0.7864 - val_loss: 0.4783 - val_accuracy: 0.8097\n",
      "Epoch 544/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4497 - accuracy: 0.7944 - val_loss: 0.4819 - val_accuracy: 0.8057\n",
      "Epoch 545/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4522 - accuracy: 0.7864 - val_loss: 0.4808 - val_accuracy: 0.8097\n",
      "Epoch 546/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4461 - accuracy: 0.7884 - val_loss: 0.4809 - val_accuracy: 0.8057\n",
      "Epoch 547/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4441 - accuracy: 0.7904 - val_loss: 0.4804 - val_accuracy: 0.8097\n",
      "Epoch 548/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4582 - accuracy: 0.7884 - val_loss: 0.4799 - val_accuracy: 0.8057\n",
      "Epoch 549/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4573 - accuracy: 0.7924 - val_loss: 0.4824 - val_accuracy: 0.8057\n",
      "Epoch 550/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4482 - accuracy: 0.8004 - val_loss: 0.4802 - val_accuracy: 0.8097\n",
      "Epoch 551/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4543 - accuracy: 0.7725 - val_loss: 0.4823 - val_accuracy: 0.8097\n",
      "Epoch 552/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4440 - accuracy: 0.7964 - val_loss: 0.4812 - val_accuracy: 0.8097\n",
      "Epoch 553/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4500 - accuracy: 0.7804 - val_loss: 0.4819 - val_accuracy: 0.8057\n",
      "Epoch 554/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4505 - accuracy: 0.7804 - val_loss: 0.4811 - val_accuracy: 0.8097\n",
      "Epoch 555/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4493 - accuracy: 0.7904 - val_loss: 0.4829 - val_accuracy: 0.8057\n",
      "Epoch 556/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4473 - accuracy: 0.7864 - val_loss: 0.4821 - val_accuracy: 0.8057\n",
      "Epoch 557/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4457 - accuracy: 0.7804 - val_loss: 0.4833 - val_accuracy: 0.8097\n",
      "Epoch 558/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4480 - accuracy: 0.7944 - val_loss: 0.4810 - val_accuracy: 0.8097\n",
      "Epoch 559/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4483 - accuracy: 0.7964 - val_loss: 0.4820 - val_accuracy: 0.8097\n",
      "Epoch 560/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4457 - accuracy: 0.7824 - val_loss: 0.4811 - val_accuracy: 0.8016\n",
      "Epoch 561/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4507 - accuracy: 0.7745 - val_loss: 0.4821 - val_accuracy: 0.8097\n",
      "Epoch 562/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4439 - accuracy: 0.7984 - val_loss: 0.4821 - val_accuracy: 0.8097\n",
      "Epoch 563/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4485 - accuracy: 0.7904 - val_loss: 0.4824 - val_accuracy: 0.7976\n",
      "Epoch 564/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4483 - accuracy: 0.7924 - val_loss: 0.4798 - val_accuracy: 0.8097\n",
      "Epoch 565/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4466 - accuracy: 0.8044 - val_loss: 0.4806 - val_accuracy: 0.8057\n",
      "Epoch 566/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4412 - accuracy: 0.8024 - val_loss: 0.4817 - val_accuracy: 0.8138\n",
      "Epoch 567/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4508 - accuracy: 0.7824 - val_loss: 0.4804 - val_accuracy: 0.8097\n",
      "Epoch 568/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4483 - accuracy: 0.7804 - val_loss: 0.4802 - val_accuracy: 0.8097\n",
      "Epoch 569/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4547 - accuracy: 0.7784 - val_loss: 0.4798 - val_accuracy: 0.8097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 570/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4553 - accuracy: 0.7884 - val_loss: 0.4797 - val_accuracy: 0.8016\n",
      "Epoch 571/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4499 - accuracy: 0.8004 - val_loss: 0.4817 - val_accuracy: 0.8097\n",
      "Epoch 572/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4502 - accuracy: 0.7944 - val_loss: 0.4789 - val_accuracy: 0.8057\n",
      "Epoch 573/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4514 - accuracy: 0.7804 - val_loss: 0.4802 - val_accuracy: 0.8097\n",
      "Epoch 574/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4418 - accuracy: 0.7964 - val_loss: 0.4804 - val_accuracy: 0.8097\n",
      "Epoch 575/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4536 - accuracy: 0.7984 - val_loss: 0.4838 - val_accuracy: 0.8097\n",
      "Epoch 576/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4425 - accuracy: 0.8004 - val_loss: 0.4800 - val_accuracy: 0.8138\n",
      "Epoch 577/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4414 - accuracy: 0.8004 - val_loss: 0.4818 - val_accuracy: 0.8097\n",
      "Epoch 578/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4457 - accuracy: 0.7964 - val_loss: 0.4828 - val_accuracy: 0.8097\n",
      "Epoch 579/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4556 - accuracy: 0.7784 - val_loss: 0.4801 - val_accuracy: 0.8097\n",
      "Epoch 580/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4472 - accuracy: 0.8024 - val_loss: 0.4798 - val_accuracy: 0.8016\n",
      "Epoch 581/1600\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4480 - accuracy: 0.7964 - val_loss: 0.4820 - val_accuracy: 0.8016\n",
      "Epoch 582/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4428 - accuracy: 0.8024 - val_loss: 0.4800 - val_accuracy: 0.8097\n",
      "Epoch 583/1600\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4501 - accuracy: 0.7824 - val_loss: 0.4793 - val_accuracy: 0.8097\n",
      "Epoch 584/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4448 - accuracy: 0.7884 - val_loss: 0.4794 - val_accuracy: 0.8097\n",
      "Epoch 585/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4430 - accuracy: 0.7984 - val_loss: 0.4793 - val_accuracy: 0.8178\n",
      "Epoch 586/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4480 - accuracy: 0.7884 - val_loss: 0.4825 - val_accuracy: 0.7976\n",
      "Epoch 587/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4508 - accuracy: 0.7904 - val_loss: 0.4806 - val_accuracy: 0.8097\n",
      "Epoch 588/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4440 - accuracy: 0.7944 - val_loss: 0.4827 - val_accuracy: 0.8057\n",
      "Epoch 589/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4466 - accuracy: 0.7924 - val_loss: 0.4797 - val_accuracy: 0.8016\n",
      "Epoch 590/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4460 - accuracy: 0.8004 - val_loss: 0.4809 - val_accuracy: 0.8097\n",
      "Epoch 591/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4471 - accuracy: 0.7904 - val_loss: 0.4802 - val_accuracy: 0.8138\n",
      "Epoch 592/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4518 - accuracy: 0.7844 - val_loss: 0.4797 - val_accuracy: 0.8138\n",
      "Epoch 593/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4525 - accuracy: 0.7884 - val_loss: 0.4816 - val_accuracy: 0.8097\n",
      "Epoch 594/1600\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4460 - accuracy: 0.7884 - val_loss: 0.4801 - val_accuracy: 0.8138\n",
      "Epoch 595/1600\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4481 - accuracy: 0.7844 - val_loss: 0.4822 - val_accuracy: 0.8097\n",
      "Epoch 596/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4499 - accuracy: 0.7904 - val_loss: 0.4827 - val_accuracy: 0.8057\n",
      "Epoch 597/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4452 - accuracy: 0.7964 - val_loss: 0.4821 - val_accuracy: 0.8057\n",
      "Epoch 598/1600\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.4506 - accuracy: 0.7784 - val_loss: 0.4828 - val_accuracy: 0.8016\n",
      "Epoch 599/1600\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4475 - accuracy: 0.7784 - val_loss: 0.4815 - val_accuracy: 0.8057\n",
      "Epoch 600/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4442 - accuracy: 0.7904 - val_loss: 0.4811 - val_accuracy: 0.8057\n",
      "Epoch 601/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4519 - accuracy: 0.7864 - val_loss: 0.4800 - val_accuracy: 0.8057\n",
      "Epoch 602/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4456 - accuracy: 0.8004 - val_loss: 0.4783 - val_accuracy: 0.8097\n",
      "Epoch 603/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4534 - accuracy: 0.7884 - val_loss: 0.4805 - val_accuracy: 0.8097\n",
      "Epoch 604/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4449 - accuracy: 0.7924 - val_loss: 0.4810 - val_accuracy: 0.8138\n",
      "Epoch 605/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4466 - accuracy: 0.7984 - val_loss: 0.4829 - val_accuracy: 0.8057\n",
      "Epoch 606/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4499 - accuracy: 0.7924 - val_loss: 0.4813 - val_accuracy: 0.8097\n",
      "Epoch 607/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4558 - accuracy: 0.7764 - val_loss: 0.4821 - val_accuracy: 0.8057\n",
      "Epoch 608/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4449 - accuracy: 0.7864 - val_loss: 0.4836 - val_accuracy: 0.8057\n",
      "Epoch 609/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4499 - accuracy: 0.7884 - val_loss: 0.4823 - val_accuracy: 0.8097\n",
      "Epoch 610/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4446 - accuracy: 0.7884 - val_loss: 0.4818 - val_accuracy: 0.8097\n",
      "Epoch 611/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4488 - accuracy: 0.7844 - val_loss: 0.4813 - val_accuracy: 0.8097\n",
      "Epoch 612/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4481 - accuracy: 0.7764 - val_loss: 0.4821 - val_accuracy: 0.8097\n",
      "Epoch 613/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4489 - accuracy: 0.7844 - val_loss: 0.4833 - val_accuracy: 0.8097\n",
      "Epoch 614/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4466 - accuracy: 0.7844 - val_loss: 0.4834 - val_accuracy: 0.7976\n",
      "Epoch 615/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4423 - accuracy: 0.8044 - val_loss: 0.4830 - val_accuracy: 0.8097\n",
      "Epoch 616/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4485 - accuracy: 0.7665 - val_loss: 0.4818 - val_accuracy: 0.8097\n",
      "Epoch 617/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4504 - accuracy: 0.7864 - val_loss: 0.4827 - val_accuracy: 0.8097\n",
      "Epoch 618/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4436 - accuracy: 0.7984 - val_loss: 0.4816 - val_accuracy: 0.7935\n",
      "Epoch 619/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4457 - accuracy: 0.8004 - val_loss: 0.4826 - val_accuracy: 0.8097\n",
      "Epoch 620/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4503 - accuracy: 0.7864 - val_loss: 0.4821 - val_accuracy: 0.8138\n",
      "Epoch 621/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4473 - accuracy: 0.7924 - val_loss: 0.4826 - val_accuracy: 0.8138\n",
      "Epoch 622/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4505 - accuracy: 0.7984 - val_loss: 0.4809 - val_accuracy: 0.8057\n",
      "Epoch 623/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4434 - accuracy: 0.7944 - val_loss: 0.4801 - val_accuracy: 0.8097\n",
      "Epoch 624/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4428 - accuracy: 0.7904 - val_loss: 0.4797 - val_accuracy: 0.8138\n",
      "Epoch 625/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4518 - accuracy: 0.7764 - val_loss: 0.4809 - val_accuracy: 0.8057\n",
      "Epoch 626/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4399 - accuracy: 0.7924 - val_loss: 0.4844 - val_accuracy: 0.8057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 627/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4466 - accuracy: 0.7824 - val_loss: 0.4816 - val_accuracy: 0.8138\n",
      "Epoch 628/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4431 - accuracy: 0.8004 - val_loss: 0.4828 - val_accuracy: 0.8057\n",
      "Epoch 629/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4440 - accuracy: 0.7884 - val_loss: 0.4823 - val_accuracy: 0.8097\n",
      "Epoch 630/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4503 - accuracy: 0.7884 - val_loss: 0.4814 - val_accuracy: 0.8097\n",
      "Epoch 631/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4410 - accuracy: 0.8064 - val_loss: 0.4835 - val_accuracy: 0.8057\n",
      "Epoch 632/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4414 - accuracy: 0.7984 - val_loss: 0.4811 - val_accuracy: 0.8097\n",
      "Epoch 633/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4515 - accuracy: 0.7804 - val_loss: 0.4811 - val_accuracy: 0.7976\n",
      "Epoch 634/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4440 - accuracy: 0.7984 - val_loss: 0.4816 - val_accuracy: 0.8057\n",
      "Epoch 635/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4453 - accuracy: 0.7844 - val_loss: 0.4811 - val_accuracy: 0.8138\n",
      "Epoch 636/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4482 - accuracy: 0.7804 - val_loss: 0.4816 - val_accuracy: 0.8057\n",
      "Epoch 637/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4429 - accuracy: 0.7964 - val_loss: 0.4817 - val_accuracy: 0.8138\n",
      "Epoch 638/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4439 - accuracy: 0.7984 - val_loss: 0.4828 - val_accuracy: 0.8016\n",
      "Epoch 639/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4429 - accuracy: 0.8124 - val_loss: 0.4839 - val_accuracy: 0.8057\n",
      "Epoch 640/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4477 - accuracy: 0.7764 - val_loss: 0.4807 - val_accuracy: 0.8178\n",
      "Epoch 641/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4492 - accuracy: 0.7924 - val_loss: 0.4809 - val_accuracy: 0.7854\n",
      "Epoch 642/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4484 - accuracy: 0.7804 - val_loss: 0.4825 - val_accuracy: 0.8138\n",
      "Epoch 643/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4531 - accuracy: 0.7864 - val_loss: 0.4846 - val_accuracy: 0.8138\n",
      "Epoch 644/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4482 - accuracy: 0.7924 - val_loss: 0.4813 - val_accuracy: 0.7854\n",
      "Epoch 645/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4476 - accuracy: 0.7924 - val_loss: 0.4820 - val_accuracy: 0.8057\n",
      "Epoch 646/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4396 - accuracy: 0.8004 - val_loss: 0.4823 - val_accuracy: 0.8016\n",
      "Epoch 647/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4483 - accuracy: 0.7864 - val_loss: 0.4811 - val_accuracy: 0.8138\n",
      "Epoch 648/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4442 - accuracy: 0.7884 - val_loss: 0.4834 - val_accuracy: 0.8057\n",
      "Epoch 649/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4432 - accuracy: 0.8004 - val_loss: 0.4807 - val_accuracy: 0.8138\n",
      "Epoch 650/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4412 - accuracy: 0.7884 - val_loss: 0.4810 - val_accuracy: 0.8097\n",
      "Epoch 651/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4451 - accuracy: 0.7904 - val_loss: 0.4795 - val_accuracy: 0.8097\n",
      "Epoch 652/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4429 - accuracy: 0.7924 - val_loss: 0.4797 - val_accuracy: 0.8057\n",
      "Epoch 653/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4486 - accuracy: 0.7884 - val_loss: 0.4800 - val_accuracy: 0.8016\n",
      "Epoch 654/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4439 - accuracy: 0.7944 - val_loss: 0.4812 - val_accuracy: 0.8016\n",
      "Epoch 655/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4462 - accuracy: 0.7964 - val_loss: 0.4819 - val_accuracy: 0.8057\n",
      "Epoch 656/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4454 - accuracy: 0.7924 - val_loss: 0.4824 - val_accuracy: 0.8016\n",
      "Epoch 657/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4411 - accuracy: 0.8024 - val_loss: 0.4841 - val_accuracy: 0.8057\n",
      "Epoch 658/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4495 - accuracy: 0.7904 - val_loss: 0.4805 - val_accuracy: 0.8138\n",
      "Epoch 659/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4441 - accuracy: 0.7804 - val_loss: 0.4814 - val_accuracy: 0.8016\n",
      "Epoch 660/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4389 - accuracy: 0.7884 - val_loss: 0.4809 - val_accuracy: 0.8097\n",
      "Epoch 661/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4433 - accuracy: 0.7964 - val_loss: 0.4816 - val_accuracy: 0.8016\n",
      "Epoch 662/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4438 - accuracy: 0.8004 - val_loss: 0.4825 - val_accuracy: 0.8057\n",
      "Epoch 663/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4494 - accuracy: 0.7924 - val_loss: 0.4815 - val_accuracy: 0.8016\n",
      "Epoch 664/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4392 - accuracy: 0.7984 - val_loss: 0.4819 - val_accuracy: 0.8097\n",
      "Epoch 665/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4410 - accuracy: 0.8004 - val_loss: 0.4804 - val_accuracy: 0.8097\n",
      "Epoch 666/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4505 - accuracy: 0.7944 - val_loss: 0.4819 - val_accuracy: 0.8057\n",
      "Epoch 667/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4471 - accuracy: 0.7904 - val_loss: 0.4809 - val_accuracy: 0.8178\n",
      "Epoch 668/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4496 - accuracy: 0.7864 - val_loss: 0.4822 - val_accuracy: 0.8057\n",
      "Epoch 669/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4488 - accuracy: 0.7924 - val_loss: 0.4793 - val_accuracy: 0.8138\n",
      "Epoch 670/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4465 - accuracy: 0.7924 - val_loss: 0.4793 - val_accuracy: 0.8097\n",
      "Epoch 671/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4483 - accuracy: 0.7964 - val_loss: 0.4807 - val_accuracy: 0.8138\n",
      "Epoch 672/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4409 - accuracy: 0.7944 - val_loss: 0.4797 - val_accuracy: 0.8138\n",
      "Epoch 673/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4432 - accuracy: 0.7964 - val_loss: 0.4816 - val_accuracy: 0.8057\n",
      "Epoch 674/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4440 - accuracy: 0.7904 - val_loss: 0.4807 - val_accuracy: 0.8178\n",
      "Epoch 675/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4379 - accuracy: 0.8144 - val_loss: 0.4838 - val_accuracy: 0.8057\n",
      "Epoch 676/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4428 - accuracy: 0.7984 - val_loss: 0.4823 - val_accuracy: 0.8097\n",
      "Epoch 677/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4403 - accuracy: 0.7944 - val_loss: 0.4833 - val_accuracy: 0.8097\n",
      "Epoch 678/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4474 - accuracy: 0.8024 - val_loss: 0.4828 - val_accuracy: 0.8178\n",
      "Epoch 679/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4472 - accuracy: 0.7964 - val_loss: 0.4812 - val_accuracy: 0.8016\n",
      "Epoch 680/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4405 - accuracy: 0.7884 - val_loss: 0.4811 - val_accuracy: 0.8178\n",
      "Epoch 681/1600\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4435 - accuracy: 0.7904 - val_loss: 0.4828 - val_accuracy: 0.8057\n",
      "Epoch 682/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4437 - accuracy: 0.7804 - val_loss: 0.4806 - val_accuracy: 0.8057\n",
      "Epoch 683/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4430 - accuracy: 0.7964 - val_loss: 0.4803 - val_accuracy: 0.8097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 684/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4406 - accuracy: 0.7924 - val_loss: 0.4817 - val_accuracy: 0.8138\n",
      "Epoch 685/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4422 - accuracy: 0.7984 - val_loss: 0.4821 - val_accuracy: 0.8016\n",
      "Epoch 686/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4408 - accuracy: 0.7745 - val_loss: 0.4830 - val_accuracy: 0.8057\n",
      "Epoch 687/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4426 - accuracy: 0.8064 - val_loss: 0.4815 - val_accuracy: 0.8016\n",
      "Epoch 688/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4460 - accuracy: 0.7924 - val_loss: 0.4801 - val_accuracy: 0.8097\n",
      "Epoch 689/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4422 - accuracy: 0.7944 - val_loss: 0.4796 - val_accuracy: 0.8138\n",
      "Epoch 690/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4438 - accuracy: 0.7884 - val_loss: 0.4817 - val_accuracy: 0.8016\n",
      "Epoch 691/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4412 - accuracy: 0.7964 - val_loss: 0.4807 - val_accuracy: 0.8016\n",
      "Epoch 692/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4443 - accuracy: 0.7924 - val_loss: 0.4793 - val_accuracy: 0.8219\n",
      "Epoch 693/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4432 - accuracy: 0.7924 - val_loss: 0.4826 - val_accuracy: 0.8057\n",
      "Epoch 694/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4498 - accuracy: 0.7784 - val_loss: 0.4815 - val_accuracy: 0.8016\n",
      "Epoch 695/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4451 - accuracy: 0.7884 - val_loss: 0.4813 - val_accuracy: 0.8097\n",
      "Epoch 696/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4417 - accuracy: 0.8044 - val_loss: 0.4826 - val_accuracy: 0.8178\n",
      "Epoch 697/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4425 - accuracy: 0.7884 - val_loss: 0.4830 - val_accuracy: 0.8138\n",
      "Epoch 698/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4461 - accuracy: 0.8004 - val_loss: 0.4821 - val_accuracy: 0.8097\n",
      "Epoch 699/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4354 - accuracy: 0.8004 - val_loss: 0.4818 - val_accuracy: 0.8097\n",
      "Epoch 700/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4415 - accuracy: 0.7884 - val_loss: 0.4821 - val_accuracy: 0.8057\n",
      "Epoch 701/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4389 - accuracy: 0.7984 - val_loss: 0.4827 - val_accuracy: 0.8057\n",
      "Epoch 702/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4446 - accuracy: 0.8004 - val_loss: 0.4809 - val_accuracy: 0.8057\n",
      "Epoch 703/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4434 - accuracy: 0.7924 - val_loss: 0.4832 - val_accuracy: 0.8057\n",
      "Epoch 704/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4465 - accuracy: 0.7784 - val_loss: 0.4814 - val_accuracy: 0.8057\n",
      "Epoch 705/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4428 - accuracy: 0.7824 - val_loss: 0.4829 - val_accuracy: 0.8097\n",
      "Epoch 706/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4453 - accuracy: 0.7844 - val_loss: 0.4834 - val_accuracy: 0.8219\n",
      "Epoch 707/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4458 - accuracy: 0.8024 - val_loss: 0.4833 - val_accuracy: 0.8219\n",
      "Epoch 708/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4444 - accuracy: 0.7944 - val_loss: 0.4869 - val_accuracy: 0.8057\n",
      "Epoch 709/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4422 - accuracy: 0.7764 - val_loss: 0.4828 - val_accuracy: 0.7814\n",
      "Epoch 710/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4417 - accuracy: 0.8004 - val_loss: 0.4835 - val_accuracy: 0.8138\n",
      "Epoch 711/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4431 - accuracy: 0.7964 - val_loss: 0.4831 - val_accuracy: 0.8057\n",
      "Epoch 712/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4447 - accuracy: 0.7924 - val_loss: 0.4822 - val_accuracy: 0.8219\n",
      "Epoch 713/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4448 - accuracy: 0.7904 - val_loss: 0.4840 - val_accuracy: 0.8138\n",
      "Epoch 714/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4384 - accuracy: 0.7924 - val_loss: 0.4828 - val_accuracy: 0.8016\n",
      "Epoch 715/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4472 - accuracy: 0.8084 - val_loss: 0.4849 - val_accuracy: 0.8097\n",
      "Epoch 716/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4349 - accuracy: 0.7964 - val_loss: 0.4831 - val_accuracy: 0.8016\n",
      "Epoch 717/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4487 - accuracy: 0.7964 - val_loss: 0.4821 - val_accuracy: 0.8178\n",
      "Epoch 718/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4385 - accuracy: 0.7984 - val_loss: 0.4830 - val_accuracy: 0.8097\n",
      "Epoch 719/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4451 - accuracy: 0.7904 - val_loss: 0.4854 - val_accuracy: 0.7814\n",
      "Epoch 720/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4383 - accuracy: 0.8084 - val_loss: 0.4828 - val_accuracy: 0.8097\n",
      "Epoch 721/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4440 - accuracy: 0.7984 - val_loss: 0.4834 - val_accuracy: 0.8057\n",
      "Epoch 722/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4416 - accuracy: 0.7984 - val_loss: 0.4807 - val_accuracy: 0.8097\n",
      "Epoch 723/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4492 - accuracy: 0.7904 - val_loss: 0.4818 - val_accuracy: 0.8057\n",
      "Epoch 724/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4428 - accuracy: 0.7964 - val_loss: 0.4800 - val_accuracy: 0.8178\n",
      "Epoch 725/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4392 - accuracy: 0.7924 - val_loss: 0.4835 - val_accuracy: 0.8016\n",
      "Epoch 726/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4398 - accuracy: 0.7944 - val_loss: 0.4823 - val_accuracy: 0.8097\n",
      "Epoch 727/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4490 - accuracy: 0.7924 - val_loss: 0.4815 - val_accuracy: 0.8138\n",
      "Epoch 728/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4480 - accuracy: 0.7884 - val_loss: 0.4840 - val_accuracy: 0.8138\n",
      "Epoch 729/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4463 - accuracy: 0.7924 - val_loss: 0.4815 - val_accuracy: 0.8057\n",
      "Epoch 730/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4476 - accuracy: 0.7944 - val_loss: 0.4839 - val_accuracy: 0.8016\n",
      "Epoch 731/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4368 - accuracy: 0.8004 - val_loss: 0.4797 - val_accuracy: 0.8016\n",
      "Epoch 732/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4413 - accuracy: 0.7904 - val_loss: 0.4826 - val_accuracy: 0.8057\n",
      "Epoch 733/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4464 - accuracy: 0.7904 - val_loss: 0.4830 - val_accuracy: 0.8097\n",
      "Epoch 734/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4401 - accuracy: 0.7884 - val_loss: 0.4829 - val_accuracy: 0.8097\n",
      "Epoch 735/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4449 - accuracy: 0.7864 - val_loss: 0.4815 - val_accuracy: 0.8097\n",
      "Epoch 736/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4446 - accuracy: 0.7944 - val_loss: 0.4843 - val_accuracy: 0.8016\n",
      "Epoch 737/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4372 - accuracy: 0.7864 - val_loss: 0.4807 - val_accuracy: 0.8219\n",
      "Epoch 738/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4427 - accuracy: 0.8044 - val_loss: 0.4824 - val_accuracy: 0.8057\n",
      "Epoch 739/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4415 - accuracy: 0.7864 - val_loss: 0.4818 - val_accuracy: 0.8138\n",
      "Epoch 740/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4403 - accuracy: 0.7884 - val_loss: 0.4818 - val_accuracy: 0.8178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 741/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4415 - accuracy: 0.7864 - val_loss: 0.4811 - val_accuracy: 0.8016\n",
      "Epoch 742/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4435 - accuracy: 0.7964 - val_loss: 0.4804 - val_accuracy: 0.8057\n",
      "Epoch 743/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4398 - accuracy: 0.7944 - val_loss: 0.4802 - val_accuracy: 0.8057\n",
      "Epoch 744/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4462 - accuracy: 0.7984 - val_loss: 0.4803 - val_accuracy: 0.8016\n",
      "Epoch 745/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4416 - accuracy: 0.8064 - val_loss: 0.4806 - val_accuracy: 0.8016\n",
      "Epoch 746/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4453 - accuracy: 0.7924 - val_loss: 0.4809 - val_accuracy: 0.8016\n",
      "Epoch 747/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4399 - accuracy: 0.7904 - val_loss: 0.4819 - val_accuracy: 0.8016\n",
      "Epoch 748/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4359 - accuracy: 0.7924 - val_loss: 0.4813 - val_accuracy: 0.8097\n",
      "Epoch 749/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4419 - accuracy: 0.7964 - val_loss: 0.4803 - val_accuracy: 0.8016\n",
      "Epoch 750/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4413 - accuracy: 0.7884 - val_loss: 0.4828 - val_accuracy: 0.8057\n",
      "Epoch 751/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4387 - accuracy: 0.7924 - val_loss: 0.4819 - val_accuracy: 0.8097\n",
      "Epoch 752/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4446 - accuracy: 0.8024 - val_loss: 0.4803 - val_accuracy: 0.8016\n",
      "Epoch 753/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4461 - accuracy: 0.7924 - val_loss: 0.4810 - val_accuracy: 0.8097\n",
      "Epoch 754/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4420 - accuracy: 0.7984 - val_loss: 0.4817 - val_accuracy: 0.8057\n",
      "Epoch 755/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4416 - accuracy: 0.7924 - val_loss: 0.4796 - val_accuracy: 0.8178\n",
      "Epoch 756/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4437 - accuracy: 0.7944 - val_loss: 0.4805 - val_accuracy: 0.8097\n",
      "Epoch 757/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4420 - accuracy: 0.7884 - val_loss: 0.4792 - val_accuracy: 0.8057\n",
      "Epoch 758/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4399 - accuracy: 0.7984 - val_loss: 0.4790 - val_accuracy: 0.8057\n",
      "Epoch 759/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4433 - accuracy: 0.7844 - val_loss: 0.4812 - val_accuracy: 0.8016\n",
      "Epoch 760/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4472 - accuracy: 0.8044 - val_loss: 0.4807 - val_accuracy: 0.8178\n",
      "Epoch 761/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4430 - accuracy: 0.7884 - val_loss: 0.4844 - val_accuracy: 0.8057\n",
      "Epoch 762/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4384 - accuracy: 0.7944 - val_loss: 0.4813 - val_accuracy: 0.8057\n",
      "Epoch 763/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4360 - accuracy: 0.8044 - val_loss: 0.4799 - val_accuracy: 0.8057\n",
      "Epoch 764/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4473 - accuracy: 0.7864 - val_loss: 0.4804 - val_accuracy: 0.8057\n",
      "Epoch 765/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4429 - accuracy: 0.7984 - val_loss: 0.4823 - val_accuracy: 0.8097\n",
      "Epoch 766/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4466 - accuracy: 0.7864 - val_loss: 0.4833 - val_accuracy: 0.8057\n",
      "Epoch 767/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4391 - accuracy: 0.8004 - val_loss: 0.4813 - val_accuracy: 0.8016\n",
      "Epoch 768/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4453 - accuracy: 0.8004 - val_loss: 0.4827 - val_accuracy: 0.8057\n",
      "Epoch 769/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4439 - accuracy: 0.7884 - val_loss: 0.4812 - val_accuracy: 0.8016\n",
      "Epoch 770/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4450 - accuracy: 0.7904 - val_loss: 0.4805 - val_accuracy: 0.8178\n",
      "Epoch 771/1600\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4389 - accuracy: 0.7884 - val_loss: 0.4815 - val_accuracy: 0.8057\n",
      "Epoch 772/1600\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4407 - accuracy: 0.7964 - val_loss: 0.4819 - val_accuracy: 0.8016\n",
      "Epoch 773/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4393 - accuracy: 0.8044 - val_loss: 0.4790 - val_accuracy: 0.8016\n",
      "Epoch 774/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4458 - accuracy: 0.7784 - val_loss: 0.4812 - val_accuracy: 0.8016\n",
      "Epoch 775/1600\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4512 - accuracy: 0.7904 - val_loss: 0.4812 - val_accuracy: 0.8138\n",
      "Epoch 776/1600\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4446 - accuracy: 0.7844 - val_loss: 0.4816 - val_accuracy: 0.8016\n",
      "Epoch 777/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4423 - accuracy: 0.7884 - val_loss: 0.4789 - val_accuracy: 0.8138\n",
      "Epoch 778/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4399 - accuracy: 0.8104 - val_loss: 0.4814 - val_accuracy: 0.8138\n",
      "Epoch 779/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4410 - accuracy: 0.7984 - val_loss: 0.4820 - val_accuracy: 0.8057\n",
      "Epoch 780/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4428 - accuracy: 0.7924 - val_loss: 0.4813 - val_accuracy: 0.8097\n",
      "Epoch 781/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4408 - accuracy: 0.7904 - val_loss: 0.4827 - val_accuracy: 0.8016\n",
      "Epoch 782/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4433 - accuracy: 0.8124 - val_loss: 0.4794 - val_accuracy: 0.8016\n",
      "Epoch 783/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4425 - accuracy: 0.7904 - val_loss: 0.4796 - val_accuracy: 0.8178\n",
      "Epoch 784/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4441 - accuracy: 0.7964 - val_loss: 0.4797 - val_accuracy: 0.8138\n",
      "Epoch 785/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4413 - accuracy: 0.7884 - val_loss: 0.4816 - val_accuracy: 0.8097\n",
      "Epoch 786/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4418 - accuracy: 0.8024 - val_loss: 0.4797 - val_accuracy: 0.8057\n",
      "Epoch 787/1600\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.4644 - accuracy: 0.78 - 0s 4ms/step - loss: 0.4430 - accuracy: 0.8004 - val_loss: 0.4789 - val_accuracy: 0.8097\n",
      "Epoch 788/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4427 - accuracy: 0.7884 - val_loss: 0.4813 - val_accuracy: 0.8097\n",
      "Epoch 789/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4372 - accuracy: 0.7944 - val_loss: 0.4808 - val_accuracy: 0.8016\n",
      "Epoch 790/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4404 - accuracy: 0.7784 - val_loss: 0.4834 - val_accuracy: 0.8057\n",
      "Epoch 791/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4429 - accuracy: 0.7944 - val_loss: 0.4832 - val_accuracy: 0.8016\n",
      "Epoch 792/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4407 - accuracy: 0.8024 - val_loss: 0.4808 - val_accuracy: 0.8057\n",
      "Epoch 793/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4416 - accuracy: 0.7944 - val_loss: 0.4825 - val_accuracy: 0.8016\n",
      "Epoch 794/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4407 - accuracy: 0.8024 - val_loss: 0.4839 - val_accuracy: 0.8016\n",
      "Epoch 795/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4426 - accuracy: 0.7924 - val_loss: 0.4818 - val_accuracy: 0.8016\n",
      "Epoch 796/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4452 - accuracy: 0.7964 - val_loss: 0.4805 - val_accuracy: 0.8138\n",
      "Epoch 797/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4388 - accuracy: 0.7964 - val_loss: 0.4830 - val_accuracy: 0.8057\n",
      "Epoch 798/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4378 - accuracy: 0.7864 - val_loss: 0.4808 - val_accuracy: 0.8057\n",
      "Epoch 799/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4400 - accuracy: 0.7924 - val_loss: 0.4820 - val_accuracy: 0.8016\n",
      "Epoch 800/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4397 - accuracy: 0.8024 - val_loss: 0.4839 - val_accuracy: 0.8057\n",
      "Epoch 801/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4446 - accuracy: 0.7904 - val_loss: 0.4802 - val_accuracy: 0.8178\n",
      "Epoch 802/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4412 - accuracy: 0.7904 - val_loss: 0.4824 - val_accuracy: 0.8057\n",
      "Epoch 803/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4381 - accuracy: 0.7844 - val_loss: 0.4795 - val_accuracy: 0.8057\n",
      "Epoch 804/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4435 - accuracy: 0.7824 - val_loss: 0.4820 - val_accuracy: 0.8057\n",
      "Epoch 805/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4412 - accuracy: 0.7904 - val_loss: 0.4827 - val_accuracy: 0.8016\n",
      "Epoch 806/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4350 - accuracy: 0.7944 - val_loss: 0.4810 - val_accuracy: 0.8057\n",
      "Epoch 807/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4370 - accuracy: 0.8044 - val_loss: 0.4826 - val_accuracy: 0.8057\n",
      "Epoch 808/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4429 - accuracy: 0.7884 - val_loss: 0.4817 - val_accuracy: 0.8016\n",
      "Epoch 809/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4420 - accuracy: 0.7844 - val_loss: 0.4860 - val_accuracy: 0.8097\n",
      "Epoch 810/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4356 - accuracy: 0.8084 - val_loss: 0.4805 - val_accuracy: 0.8016\n",
      "Epoch 811/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4446 - accuracy: 0.7824 - val_loss: 0.4841 - val_accuracy: 0.8138\n",
      "Epoch 812/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4458 - accuracy: 0.8124 - val_loss: 0.4821 - val_accuracy: 0.8057\n",
      "Epoch 813/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4364 - accuracy: 0.7984 - val_loss: 0.4832 - val_accuracy: 0.8057\n",
      "Epoch 814/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4403 - accuracy: 0.7884 - val_loss: 0.4803 - val_accuracy: 0.8178\n",
      "Epoch 815/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4442 - accuracy: 0.7824 - val_loss: 0.4825 - val_accuracy: 0.8097\n",
      "Epoch 816/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4366 - accuracy: 0.7924 - val_loss: 0.4814 - val_accuracy: 0.8016\n",
      "Epoch 817/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4300 - accuracy: 0.8124 - val_loss: 0.4842 - val_accuracy: 0.8138\n",
      "Epoch 818/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4370 - accuracy: 0.7944 - val_loss: 0.4812 - val_accuracy: 0.8057\n",
      "Epoch 819/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4500 - accuracy: 0.7844 - val_loss: 0.4815 - val_accuracy: 0.8016\n",
      "Epoch 820/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4385 - accuracy: 0.7964 - val_loss: 0.4836 - val_accuracy: 0.8057\n",
      "Epoch 821/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4358 - accuracy: 0.8044 - val_loss: 0.4837 - val_accuracy: 0.8138\n",
      "Epoch 822/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4374 - accuracy: 0.7864 - val_loss: 0.4824 - val_accuracy: 0.8057\n",
      "Epoch 823/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4308 - accuracy: 0.7924 - val_loss: 0.4823 - val_accuracy: 0.8016\n",
      "Epoch 824/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4380 - accuracy: 0.7964 - val_loss: 0.4812 - val_accuracy: 0.8057\n",
      "Epoch 825/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4376 - accuracy: 0.8064 - val_loss: 0.4810 - val_accuracy: 0.8057\n",
      "Epoch 826/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4369 - accuracy: 0.8004 - val_loss: 0.4824 - val_accuracy: 0.8097\n",
      "Epoch 827/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4508 - accuracy: 0.7725 - val_loss: 0.4822 - val_accuracy: 0.8016\n",
      "Epoch 828/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4389 - accuracy: 0.7904 - val_loss: 0.4829 - val_accuracy: 0.8016\n",
      "Epoch 829/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4417 - accuracy: 0.7944 - val_loss: 0.4847 - val_accuracy: 0.8057\n",
      "Epoch 830/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4382 - accuracy: 0.7924 - val_loss: 0.4825 - val_accuracy: 0.8057\n",
      "Epoch 831/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4338 - accuracy: 0.8024 - val_loss: 0.4813 - val_accuracy: 0.8057\n",
      "Epoch 832/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4442 - accuracy: 0.7944 - val_loss: 0.4828 - val_accuracy: 0.8057\n",
      "Epoch 833/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4319 - accuracy: 0.7984 - val_loss: 0.4838 - val_accuracy: 0.8016\n",
      "Epoch 834/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4331 - accuracy: 0.7984 - val_loss: 0.4834 - val_accuracy: 0.8016\n",
      "Epoch 835/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4435 - accuracy: 0.7844 - val_loss: 0.4878 - val_accuracy: 0.8016\n",
      "Epoch 836/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4381 - accuracy: 0.7964 - val_loss: 0.4841 - val_accuracy: 0.8057\n",
      "Epoch 837/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4414 - accuracy: 0.7924 - val_loss: 0.4857 - val_accuracy: 0.8097\n",
      "Epoch 838/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4420 - accuracy: 0.7924 - val_loss: 0.4835 - val_accuracy: 0.8178\n",
      "Epoch 839/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4379 - accuracy: 0.7944 - val_loss: 0.4827 - val_accuracy: 0.8057\n",
      "Epoch 840/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4410 - accuracy: 0.7844 - val_loss: 0.4836 - val_accuracy: 0.8097\n",
      "Epoch 841/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4358 - accuracy: 0.7984 - val_loss: 0.4839 - val_accuracy: 0.8016\n",
      "Epoch 842/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4396 - accuracy: 0.7984 - val_loss: 0.4828 - val_accuracy: 0.8057\n",
      "Epoch 843/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4367 - accuracy: 0.7924 - val_loss: 0.4847 - val_accuracy: 0.8057\n",
      "Epoch 844/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4387 - accuracy: 0.7864 - val_loss: 0.4838 - val_accuracy: 0.8016\n",
      "Epoch 845/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4411 - accuracy: 0.7884 - val_loss: 0.4838 - val_accuracy: 0.8057\n",
      "Epoch 846/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4445 - accuracy: 0.7824 - val_loss: 0.4830 - val_accuracy: 0.8016\n",
      "Epoch 847/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4435 - accuracy: 0.7824 - val_loss: 0.4837 - val_accuracy: 0.8057\n",
      "Epoch 848/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4329 - accuracy: 0.8024 - val_loss: 0.4830 - val_accuracy: 0.8057\n",
      "Epoch 849/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4371 - accuracy: 0.7964 - val_loss: 0.4842 - val_accuracy: 0.8016\n",
      "Epoch 850/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4381 - accuracy: 0.7844 - val_loss: 0.4823 - val_accuracy: 0.8057\n",
      "Epoch 851/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4313 - accuracy: 0.7884 - val_loss: 0.4840 - val_accuracy: 0.8057\n",
      "Epoch 852/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4415 - accuracy: 0.7964 - val_loss: 0.4838 - val_accuracy: 0.8097\n",
      "Epoch 853/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4281 - accuracy: 0.8004 - val_loss: 0.4848 - val_accuracy: 0.8057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 854/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4328 - accuracy: 0.8024 - val_loss: 0.4840 - val_accuracy: 0.8057\n",
      "Epoch 855/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4373 - accuracy: 0.8064 - val_loss: 0.4841 - val_accuracy: 0.8057\n",
      "Epoch 856/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4442 - accuracy: 0.7944 - val_loss: 0.4845 - val_accuracy: 0.8057\n",
      "Epoch 857/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4385 - accuracy: 0.8004 - val_loss: 0.4807 - val_accuracy: 0.7935\n",
      "Epoch 858/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4393 - accuracy: 0.7924 - val_loss: 0.4856 - val_accuracy: 0.7935\n",
      "Epoch 859/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4409 - accuracy: 0.7884 - val_loss: 0.4854 - val_accuracy: 0.8057\n",
      "Epoch 860/1600\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4385 - accuracy: 0.8044 - val_loss: 0.4874 - val_accuracy: 0.8097\n",
      "Epoch 861/1600\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4407 - accuracy: 0.8004 - val_loss: 0.4833 - val_accuracy: 0.8057\n",
      "Epoch 862/1600\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4438 - accuracy: 0.7964 - val_loss: 0.4836 - val_accuracy: 0.8057\n",
      "Epoch 863/1600\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4407 - accuracy: 0.8004 - val_loss: 0.4851 - val_accuracy: 0.8057\n",
      "Epoch 864/1600\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4409 - accuracy: 0.8044 - val_loss: 0.4856 - val_accuracy: 0.8057\n",
      "Epoch 865/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4313 - accuracy: 0.8084 - val_loss: 0.4820 - val_accuracy: 0.8057\n",
      "Epoch 866/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4483 - accuracy: 0.7864 - val_loss: 0.4863 - val_accuracy: 0.8016\n",
      "Epoch 867/1600\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4387 - accuracy: 0.8004 - val_loss: 0.4867 - val_accuracy: 0.8057\n",
      "Epoch 868/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4357 - accuracy: 0.7984 - val_loss: 0.4823 - val_accuracy: 0.8016\n",
      "Epoch 869/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4327 - accuracy: 0.8024 - val_loss: 0.4834 - val_accuracy: 0.8057\n",
      "Epoch 870/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4381 - accuracy: 0.7864 - val_loss: 0.4833 - val_accuracy: 0.8057\n",
      "Epoch 871/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4356 - accuracy: 0.7964 - val_loss: 0.4836 - val_accuracy: 0.7935\n",
      "Epoch 872/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4375 - accuracy: 0.7924 - val_loss: 0.4833 - val_accuracy: 0.8057\n",
      "Epoch 873/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4444 - accuracy: 0.7904 - val_loss: 0.4824 - val_accuracy: 0.8057\n",
      "Epoch 874/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4386 - accuracy: 0.8004 - val_loss: 0.4872 - val_accuracy: 0.8057\n",
      "Epoch 875/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4405 - accuracy: 0.7984 - val_loss: 0.4821 - val_accuracy: 0.8057\n",
      "Epoch 876/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4344 - accuracy: 0.8144 - val_loss: 0.4852 - val_accuracy: 0.8057\n",
      "Epoch 877/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4380 - accuracy: 0.7984 - val_loss: 0.4836 - val_accuracy: 0.8057\n",
      "Epoch 878/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4331 - accuracy: 0.8044 - val_loss: 0.4825 - val_accuracy: 0.8057\n",
      "Epoch 879/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4349 - accuracy: 0.8044 - val_loss: 0.4843 - val_accuracy: 0.8057\n",
      "Epoch 880/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4322 - accuracy: 0.8024 - val_loss: 0.4869 - val_accuracy: 0.8057\n",
      "Epoch 881/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4461 - accuracy: 0.7944 - val_loss: 0.4849 - val_accuracy: 0.8016\n",
      "Epoch 882/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4441 - accuracy: 0.7924 - val_loss: 0.4885 - val_accuracy: 0.8097\n",
      "Epoch 883/1600\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4468 - accuracy: 0.7884 - val_loss: 0.4827 - val_accuracy: 0.8057\n",
      "Epoch 884/1600\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4407 - accuracy: 0.7924 - val_loss: 0.4835 - val_accuracy: 0.8057\n",
      "Epoch 885/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4344 - accuracy: 0.8064 - val_loss: 0.4831 - val_accuracy: 0.8057\n",
      "Epoch 886/1600\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4345 - accuracy: 0.8004 - val_loss: 0.4824 - val_accuracy: 0.8057\n",
      "Epoch 887/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4371 - accuracy: 0.8044 - val_loss: 0.4871 - val_accuracy: 0.8057\n",
      "Epoch 888/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4423 - accuracy: 0.8044 - val_loss: 0.4835 - val_accuracy: 0.8057\n",
      "Epoch 889/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4389 - accuracy: 0.7924 - val_loss: 0.4830 - val_accuracy: 0.7976\n",
      "Epoch 890/1600\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4343 - accuracy: 0.8064 - val_loss: 0.4855 - val_accuracy: 0.8057\n",
      "Epoch 891/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4462 - accuracy: 0.7904 - val_loss: 0.4862 - val_accuracy: 0.8057\n",
      "Epoch 892/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4333 - accuracy: 0.8044 - val_loss: 0.4851 - val_accuracy: 0.8057\n",
      "Epoch 893/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4335 - accuracy: 0.7984 - val_loss: 0.4882 - val_accuracy: 0.8057\n",
      "Epoch 894/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4353 - accuracy: 0.7924 - val_loss: 0.4866 - val_accuracy: 0.8057\n",
      "Epoch 895/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4474 - accuracy: 0.8044 - val_loss: 0.4852 - val_accuracy: 0.8057\n",
      "Epoch 896/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4440 - accuracy: 0.7884 - val_loss: 0.4851 - val_accuracy: 0.8057\n",
      "Epoch 897/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4375 - accuracy: 0.7924 - val_loss: 0.4858 - val_accuracy: 0.7976\n",
      "Epoch 898/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4405 - accuracy: 0.7964 - val_loss: 0.4839 - val_accuracy: 0.8057\n",
      "Epoch 899/1600\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4405 - accuracy: 0.7864 - val_loss: 0.4839 - val_accuracy: 0.8057\n",
      "Epoch 900/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4439 - accuracy: 0.7924 - val_loss: 0.4848 - val_accuracy: 0.8057\n",
      "Epoch 901/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4388 - accuracy: 0.8064 - val_loss: 0.4847 - val_accuracy: 0.8057\n",
      "Epoch 902/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4349 - accuracy: 0.8044 - val_loss: 0.4850 - val_accuracy: 0.8057\n",
      "Epoch 903/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4339 - accuracy: 0.7964 - val_loss: 0.4838 - val_accuracy: 0.8057\n",
      "Epoch 904/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4378 - accuracy: 0.8024 - val_loss: 0.4853 - val_accuracy: 0.8016\n",
      "Epoch 905/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4362 - accuracy: 0.7864 - val_loss: 0.4840 - val_accuracy: 0.8097\n",
      "Epoch 906/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4405 - accuracy: 0.8064 - val_loss: 0.4895 - val_accuracy: 0.8016\n",
      "Epoch 907/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4424 - accuracy: 0.7984 - val_loss: 0.4859 - val_accuracy: 0.8057\n",
      "Epoch 908/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4410 - accuracy: 0.8024 - val_loss: 0.4848 - val_accuracy: 0.8057\n",
      "Epoch 909/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4352 - accuracy: 0.7984 - val_loss: 0.4855 - val_accuracy: 0.8016\n",
      "Epoch 910/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4356 - accuracy: 0.8104 - val_loss: 0.4830 - val_accuracy: 0.8057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 911/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4393 - accuracy: 0.7944 - val_loss: 0.4865 - val_accuracy: 0.8057\n",
      "Epoch 912/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4406 - accuracy: 0.8104 - val_loss: 0.4827 - val_accuracy: 0.8057\n",
      "Epoch 913/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4320 - accuracy: 0.8064 - val_loss: 0.4838 - val_accuracy: 0.8057\n",
      "Epoch 914/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4372 - accuracy: 0.7964 - val_loss: 0.4850 - val_accuracy: 0.8057\n",
      "Epoch 915/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4379 - accuracy: 0.8044 - val_loss: 0.4859 - val_accuracy: 0.8057\n",
      "Epoch 916/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4346 - accuracy: 0.8024 - val_loss: 0.4813 - val_accuracy: 0.8016\n",
      "Epoch 917/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4484 - accuracy: 0.7844 - val_loss: 0.4844 - val_accuracy: 0.7976\n",
      "Epoch 918/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4372 - accuracy: 0.7964 - val_loss: 0.4854 - val_accuracy: 0.8057\n",
      "Epoch 919/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4291 - accuracy: 0.7944 - val_loss: 0.4814 - val_accuracy: 0.7976\n",
      "Epoch 920/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4346 - accuracy: 0.7904 - val_loss: 0.4823 - val_accuracy: 0.7976\n",
      "Epoch 921/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4355 - accuracy: 0.8004 - val_loss: 0.4835 - val_accuracy: 0.8016\n",
      "Epoch 922/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4315 - accuracy: 0.8224 - val_loss: 0.4846 - val_accuracy: 0.8057\n",
      "Epoch 923/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4346 - accuracy: 0.7984 - val_loss: 0.4836 - val_accuracy: 0.8057\n",
      "Epoch 924/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4421 - accuracy: 0.7924 - val_loss: 0.4855 - val_accuracy: 0.8057\n",
      "Epoch 925/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4387 - accuracy: 0.7964 - val_loss: 0.4836 - val_accuracy: 0.8057\n",
      "Epoch 926/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4371 - accuracy: 0.8144 - val_loss: 0.4845 - val_accuracy: 0.8057\n",
      "Epoch 927/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4400 - accuracy: 0.7964 - val_loss: 0.4855 - val_accuracy: 0.7976\n",
      "Epoch 928/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4369 - accuracy: 0.8064 - val_loss: 0.4869 - val_accuracy: 0.8016\n",
      "Epoch 929/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4294 - accuracy: 0.8084 - val_loss: 0.4815 - val_accuracy: 0.8016\n",
      "Epoch 930/1600\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4334 - accuracy: 0.8004 - val_loss: 0.4837 - val_accuracy: 0.8016\n",
      "Epoch 931/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4399 - accuracy: 0.7984 - val_loss: 0.4845 - val_accuracy: 0.7976\n",
      "Epoch 932/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4301 - accuracy: 0.7984 - val_loss: 0.4814 - val_accuracy: 0.7976\n",
      "Epoch 933/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4397 - accuracy: 0.8064 - val_loss: 0.4858 - val_accuracy: 0.8057\n",
      "Epoch 934/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4351 - accuracy: 0.8144 - val_loss: 0.4889 - val_accuracy: 0.8057\n",
      "Epoch 935/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4366 - accuracy: 0.7984 - val_loss: 0.4810 - val_accuracy: 0.7976\n",
      "Epoch 936/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4353 - accuracy: 0.8024 - val_loss: 0.4850 - val_accuracy: 0.8057\n",
      "Epoch 937/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4361 - accuracy: 0.8004 - val_loss: 0.4843 - val_accuracy: 0.7976\n",
      "Epoch 938/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4428 - accuracy: 0.7884 - val_loss: 0.4863 - val_accuracy: 0.8097\n",
      "Epoch 939/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4434 - accuracy: 0.7964 - val_loss: 0.4859 - val_accuracy: 0.8016\n",
      "Epoch 940/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4367 - accuracy: 0.7984 - val_loss: 0.4855 - val_accuracy: 0.8016\n",
      "Epoch 941/1600\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4380 - accuracy: 0.7984 - val_loss: 0.4848 - val_accuracy: 0.8057\n",
      "Epoch 942/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4331 - accuracy: 0.8084 - val_loss: 0.4868 - val_accuracy: 0.8016\n",
      "Epoch 943/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4408 - accuracy: 0.8084 - val_loss: 0.4910 - val_accuracy: 0.8097\n",
      "Epoch 944/1600\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4390 - accuracy: 0.8024 - val_loss: 0.4829 - val_accuracy: 0.8016\n",
      "Epoch 945/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4386 - accuracy: 0.8024 - val_loss: 0.4840 - val_accuracy: 0.8057\n",
      "Epoch 946/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4396 - accuracy: 0.7904 - val_loss: 0.4863 - val_accuracy: 0.8057\n",
      "Epoch 947/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4347 - accuracy: 0.8004 - val_loss: 0.4827 - val_accuracy: 0.8016\n",
      "Epoch 948/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4386 - accuracy: 0.7884 - val_loss: 0.4861 - val_accuracy: 0.7976\n",
      "Epoch 949/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4369 - accuracy: 0.8024 - val_loss: 0.4896 - val_accuracy: 0.8057\n",
      "Epoch 950/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4434 - accuracy: 0.7944 - val_loss: 0.4821 - val_accuracy: 0.8057\n",
      "Epoch 951/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4341 - accuracy: 0.8064 - val_loss: 0.4855 - val_accuracy: 0.8016\n",
      "Epoch 952/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4329 - accuracy: 0.8024 - val_loss: 0.4865 - val_accuracy: 0.8016\n",
      "Epoch 953/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4329 - accuracy: 0.8064 - val_loss: 0.4839 - val_accuracy: 0.8016\n",
      "Epoch 954/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4313 - accuracy: 0.8104 - val_loss: 0.4867 - val_accuracy: 0.8016\n",
      "Epoch 955/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4288 - accuracy: 0.7984 - val_loss: 0.4863 - val_accuracy: 0.8057\n",
      "Epoch 956/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4411 - accuracy: 0.7964 - val_loss: 0.4872 - val_accuracy: 0.8016\n",
      "Epoch 957/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4408 - accuracy: 0.8064 - val_loss: 0.4853 - val_accuracy: 0.8016\n",
      "Epoch 958/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4356 - accuracy: 0.7984 - val_loss: 0.4853 - val_accuracy: 0.8016\n",
      "Epoch 959/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4401 - accuracy: 0.8024 - val_loss: 0.4884 - val_accuracy: 0.8016\n",
      "Epoch 960/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4363 - accuracy: 0.8024 - val_loss: 0.4817 - val_accuracy: 0.7976\n",
      "Epoch 961/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4397 - accuracy: 0.7924 - val_loss: 0.4874 - val_accuracy: 0.8057\n",
      "Epoch 962/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4356 - accuracy: 0.8124 - val_loss: 0.4844 - val_accuracy: 0.8057\n",
      "Epoch 963/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4408 - accuracy: 0.8044 - val_loss: 0.4863 - val_accuracy: 0.8016\n",
      "Epoch 964/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4409 - accuracy: 0.7904 - val_loss: 0.4859 - val_accuracy: 0.8016\n",
      "Epoch 965/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4351 - accuracy: 0.8024 - val_loss: 0.4876 - val_accuracy: 0.8097\n",
      "Epoch 966/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4389 - accuracy: 0.7904 - val_loss: 0.4864 - val_accuracy: 0.7976\n",
      "Epoch 967/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4348 - accuracy: 0.7904 - val_loss: 0.4859 - val_accuracy: 0.7976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 968/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4379 - accuracy: 0.8044 - val_loss: 0.4871 - val_accuracy: 0.8016\n",
      "Epoch 969/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4317 - accuracy: 0.8104 - val_loss: 0.4877 - val_accuracy: 0.8016\n",
      "Epoch 970/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4324 - accuracy: 0.7964 - val_loss: 0.4854 - val_accuracy: 0.7976\n",
      "Epoch 971/1600\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4402 - accuracy: 0.7984 - val_loss: 0.4860 - val_accuracy: 0.7976\n",
      "Epoch 972/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4372 - accuracy: 0.8004 - val_loss: 0.4882 - val_accuracy: 0.8097\n",
      "Epoch 973/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4386 - accuracy: 0.7964 - val_loss: 0.4862 - val_accuracy: 0.8057\n",
      "Epoch 974/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4374 - accuracy: 0.8044 - val_loss: 0.4868 - val_accuracy: 0.7976\n",
      "Epoch 975/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4397 - accuracy: 0.7944 - val_loss: 0.4885 - val_accuracy: 0.8016\n",
      "Epoch 976/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4345 - accuracy: 0.8044 - val_loss: 0.4922 - val_accuracy: 0.8016\n",
      "Epoch 977/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4318 - accuracy: 0.8144 - val_loss: 0.4857 - val_accuracy: 0.7976\n",
      "Epoch 978/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4342 - accuracy: 0.8064 - val_loss: 0.4908 - val_accuracy: 0.8016\n",
      "Epoch 979/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4352 - accuracy: 0.7944 - val_loss: 0.4859 - val_accuracy: 0.7976\n",
      "Epoch 980/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4333 - accuracy: 0.8004 - val_loss: 0.4877 - val_accuracy: 0.8016\n",
      "Epoch 981/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4283 - accuracy: 0.7864 - val_loss: 0.4876 - val_accuracy: 0.7935\n",
      "Epoch 982/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4324 - accuracy: 0.8084 - val_loss: 0.4855 - val_accuracy: 0.7976\n",
      "Epoch 983/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4321 - accuracy: 0.8044 - val_loss: 0.4899 - val_accuracy: 0.7976\n",
      "Epoch 984/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4376 - accuracy: 0.8024 - val_loss: 0.4881 - val_accuracy: 0.8057\n",
      "Epoch 985/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4345 - accuracy: 0.8044 - val_loss: 0.4906 - val_accuracy: 0.8016\n",
      "Epoch 986/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4344 - accuracy: 0.7984 - val_loss: 0.4867 - val_accuracy: 0.8016\n",
      "Epoch 987/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4347 - accuracy: 0.8004 - val_loss: 0.4886 - val_accuracy: 0.7935\n",
      "Epoch 988/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4315 - accuracy: 0.8144 - val_loss: 0.4893 - val_accuracy: 0.7976\n",
      "Epoch 989/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4371 - accuracy: 0.7964 - val_loss: 0.4868 - val_accuracy: 0.7935\n",
      "Epoch 990/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4368 - accuracy: 0.7904 - val_loss: 0.4893 - val_accuracy: 0.8057\n",
      "Epoch 991/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4331 - accuracy: 0.8084 - val_loss: 0.4908 - val_accuracy: 0.7976\n",
      "Epoch 992/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4408 - accuracy: 0.7884 - val_loss: 0.4870 - val_accuracy: 0.7935\n",
      "Epoch 993/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4388 - accuracy: 0.8044 - val_loss: 0.4855 - val_accuracy: 0.8057\n",
      "Epoch 994/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4358 - accuracy: 0.8024 - val_loss: 0.4901 - val_accuracy: 0.7976\n",
      "Epoch 995/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4340 - accuracy: 0.7904 - val_loss: 0.4879 - val_accuracy: 0.7935\n",
      "Epoch 996/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4356 - accuracy: 0.7964 - val_loss: 0.4886 - val_accuracy: 0.8016\n",
      "Epoch 997/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4371 - accuracy: 0.8044 - val_loss: 0.4906 - val_accuracy: 0.8016\n",
      "Epoch 998/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4322 - accuracy: 0.7844 - val_loss: 0.4902 - val_accuracy: 0.7976\n",
      "Epoch 999/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4336 - accuracy: 0.7964 - val_loss: 0.4922 - val_accuracy: 0.8016\n",
      "Epoch 1000/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4343 - accuracy: 0.8044 - val_loss: 0.4865 - val_accuracy: 0.8016\n",
      "Epoch 1001/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4378 - accuracy: 0.7964 - val_loss: 0.4866 - val_accuracy: 0.7935\n",
      "Epoch 1002/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4339 - accuracy: 0.8064 - val_loss: 0.4889 - val_accuracy: 0.7976\n",
      "Epoch 1003/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4360 - accuracy: 0.7844 - val_loss: 0.4894 - val_accuracy: 0.8016\n",
      "Epoch 1004/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4287 - accuracy: 0.8024 - val_loss: 0.4875 - val_accuracy: 0.7976\n",
      "Epoch 1005/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4336 - accuracy: 0.8064 - val_loss: 0.4877 - val_accuracy: 0.8016\n",
      "Epoch 1006/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4338 - accuracy: 0.7944 - val_loss: 0.4870 - val_accuracy: 0.8016\n",
      "Epoch 1007/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4351 - accuracy: 0.8064 - val_loss: 0.4895 - val_accuracy: 0.8057\n",
      "Epoch 1008/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4270 - accuracy: 0.7984 - val_loss: 0.4879 - val_accuracy: 0.7976\n",
      "Epoch 1009/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4318 - accuracy: 0.8084 - val_loss: 0.4906 - val_accuracy: 0.7976\n",
      "Epoch 1010/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4305 - accuracy: 0.8064 - val_loss: 0.4877 - val_accuracy: 0.7976\n",
      "Epoch 1011/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4351 - accuracy: 0.8024 - val_loss: 0.4885 - val_accuracy: 0.7976\n",
      "Epoch 1012/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4374 - accuracy: 0.8064 - val_loss: 0.4907 - val_accuracy: 0.7976\n",
      "Epoch 1013/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4335 - accuracy: 0.8064 - val_loss: 0.4881 - val_accuracy: 0.7976\n",
      "Epoch 1014/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4283 - accuracy: 0.8044 - val_loss: 0.4928 - val_accuracy: 0.8057\n",
      "Epoch 1015/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4324 - accuracy: 0.8064 - val_loss: 0.4878 - val_accuracy: 0.8016\n",
      "Epoch 1016/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4391 - accuracy: 0.7924 - val_loss: 0.4929 - val_accuracy: 0.7976\n",
      "Epoch 1017/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4290 - accuracy: 0.7944 - val_loss: 0.4849 - val_accuracy: 0.7976\n",
      "Epoch 1018/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4362 - accuracy: 0.7904 - val_loss: 0.4905 - val_accuracy: 0.7935\n",
      "Epoch 1019/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4355 - accuracy: 0.7964 - val_loss: 0.4924 - val_accuracy: 0.7895\n",
      "Epoch 1020/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4330 - accuracy: 0.8024 - val_loss: 0.4871 - val_accuracy: 0.7976\n",
      "Epoch 1021/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4373 - accuracy: 0.7944 - val_loss: 0.4885 - val_accuracy: 0.7976\n",
      "Epoch 1022/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4362 - accuracy: 0.8064 - val_loss: 0.4895 - val_accuracy: 0.7976\n",
      "Epoch 1023/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4379 - accuracy: 0.7964 - val_loss: 0.4876 - val_accuracy: 0.7935\n",
      "Epoch 1024/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4356 - accuracy: 0.8044 - val_loss: 0.4939 - val_accuracy: 0.8016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1025/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4337 - accuracy: 0.7944 - val_loss: 0.4868 - val_accuracy: 0.7976\n",
      "Epoch 1026/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4360 - accuracy: 0.7944 - val_loss: 0.4905 - val_accuracy: 0.7976\n",
      "Epoch 1027/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4351 - accuracy: 0.8024 - val_loss: 0.4915 - val_accuracy: 0.7976\n",
      "Epoch 1028/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4318 - accuracy: 0.7904 - val_loss: 0.4893 - val_accuracy: 0.7935\n",
      "Epoch 1029/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4331 - accuracy: 0.8044 - val_loss: 0.4932 - val_accuracy: 0.7935\n",
      "Epoch 1030/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4316 - accuracy: 0.8044 - val_loss: 0.4913 - val_accuracy: 0.7895\n",
      "Epoch 1031/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4352 - accuracy: 0.7944 - val_loss: 0.4881 - val_accuracy: 0.7976\n",
      "Epoch 1032/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4385 - accuracy: 0.8044 - val_loss: 0.4917 - val_accuracy: 0.7895\n",
      "Epoch 1033/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4359 - accuracy: 0.8044 - val_loss: 0.4876 - val_accuracy: 0.7935\n",
      "Epoch 1034/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4355 - accuracy: 0.8004 - val_loss: 0.4902 - val_accuracy: 0.7895\n",
      "Epoch 1035/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4367 - accuracy: 0.7964 - val_loss: 0.4894 - val_accuracy: 0.7935\n",
      "Epoch 1036/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4332 - accuracy: 0.8044 - val_loss: 0.4924 - val_accuracy: 0.7895\n",
      "Epoch 1037/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4373 - accuracy: 0.7964 - val_loss: 0.4879 - val_accuracy: 0.7935\n",
      "Epoch 1038/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4317 - accuracy: 0.8104 - val_loss: 0.4881 - val_accuracy: 0.7976\n",
      "Epoch 1039/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4364 - accuracy: 0.8024 - val_loss: 0.4878 - val_accuracy: 0.7976\n",
      "Epoch 1040/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4397 - accuracy: 0.7884 - val_loss: 0.4895 - val_accuracy: 0.7935\n",
      "Epoch 1041/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4324 - accuracy: 0.7924 - val_loss: 0.4859 - val_accuracy: 0.7935\n",
      "Epoch 1042/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4340 - accuracy: 0.8024 - val_loss: 0.4914 - val_accuracy: 0.7935\n",
      "Epoch 1043/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4316 - accuracy: 0.8044 - val_loss: 0.4916 - val_accuracy: 0.7976\n",
      "Epoch 1044/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4372 - accuracy: 0.7924 - val_loss: 0.4864 - val_accuracy: 0.7935\n",
      "Epoch 1045/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4310 - accuracy: 0.8004 - val_loss: 0.4896 - val_accuracy: 0.8057\n",
      "Epoch 1046/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4357 - accuracy: 0.7984 - val_loss: 0.4892 - val_accuracy: 0.7976\n",
      "Epoch 1047/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4357 - accuracy: 0.7964 - val_loss: 0.4869 - val_accuracy: 0.8016\n",
      "Epoch 1048/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4292 - accuracy: 0.8004 - val_loss: 0.4900 - val_accuracy: 0.7895\n",
      "Epoch 1049/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4301 - accuracy: 0.8024 - val_loss: 0.4853 - val_accuracy: 0.8016\n",
      "Epoch 1050/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4375 - accuracy: 0.7964 - val_loss: 0.4940 - val_accuracy: 0.8016\n",
      "Epoch 1051/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4384 - accuracy: 0.7984 - val_loss: 0.4881 - val_accuracy: 0.7976\n",
      "Epoch 1052/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4364 - accuracy: 0.7964 - val_loss: 0.4944 - val_accuracy: 0.7976\n",
      "Epoch 1053/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4357 - accuracy: 0.7944 - val_loss: 0.4878 - val_accuracy: 0.7976\n",
      "Epoch 1054/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4293 - accuracy: 0.8004 - val_loss: 0.4902 - val_accuracy: 0.7935\n",
      "Epoch 1055/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4379 - accuracy: 0.8044 - val_loss: 0.4934 - val_accuracy: 0.8057\n",
      "Epoch 1056/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4322 - accuracy: 0.7984 - val_loss: 0.4874 - val_accuracy: 0.7935\n",
      "Epoch 1057/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4401 - accuracy: 0.8004 - val_loss: 0.4937 - val_accuracy: 0.7935\n",
      "Epoch 1058/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4363 - accuracy: 0.8044 - val_loss: 0.4897 - val_accuracy: 0.7935\n",
      "Epoch 1059/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4263 - accuracy: 0.7984 - val_loss: 0.4917 - val_accuracy: 0.7895\n",
      "Epoch 1060/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4233 - accuracy: 0.8084 - val_loss: 0.4881 - val_accuracy: 0.8057\n",
      "Epoch 1061/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4347 - accuracy: 0.8024 - val_loss: 0.4937 - val_accuracy: 0.7895\n",
      "Epoch 1062/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4378 - accuracy: 0.8104 - val_loss: 0.4904 - val_accuracy: 0.7976\n",
      "Epoch 1063/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4360 - accuracy: 0.7964 - val_loss: 0.4881 - val_accuracy: 0.8016\n",
      "Epoch 1064/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4325 - accuracy: 0.8044 - val_loss: 0.4895 - val_accuracy: 0.7935\n",
      "Epoch 1065/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4374 - accuracy: 0.7984 - val_loss: 0.4937 - val_accuracy: 0.8016\n",
      "Epoch 1066/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4334 - accuracy: 0.8044 - val_loss: 0.4886 - val_accuracy: 0.8016\n",
      "Epoch 1067/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4361 - accuracy: 0.7924 - val_loss: 0.4939 - val_accuracy: 0.7895\n",
      "Epoch 1068/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4274 - accuracy: 0.8164 - val_loss: 0.4902 - val_accuracy: 0.7976\n",
      "Epoch 1069/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4321 - accuracy: 0.8124 - val_loss: 0.4901 - val_accuracy: 0.7935\n",
      "Epoch 1070/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4353 - accuracy: 0.7924 - val_loss: 0.4916 - val_accuracy: 0.7935\n",
      "Epoch 1071/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4378 - accuracy: 0.8024 - val_loss: 0.4914 - val_accuracy: 0.7895\n",
      "Epoch 1072/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4352 - accuracy: 0.8064 - val_loss: 0.4914 - val_accuracy: 0.7976\n",
      "Epoch 1073/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4261 - accuracy: 0.8084 - val_loss: 0.4917 - val_accuracy: 0.7854\n",
      "Epoch 1074/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4336 - accuracy: 0.8044 - val_loss: 0.4922 - val_accuracy: 0.7854\n",
      "Epoch 1075/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4263 - accuracy: 0.8184 - val_loss: 0.4875 - val_accuracy: 0.7935\n",
      "Epoch 1076/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4329 - accuracy: 0.8044 - val_loss: 0.4912 - val_accuracy: 0.8016\n",
      "Epoch 1077/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4333 - accuracy: 0.7864 - val_loss: 0.4855 - val_accuracy: 0.7935\n",
      "Epoch 1078/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4355 - accuracy: 0.8044 - val_loss: 0.4906 - val_accuracy: 0.7976\n",
      "Epoch 1079/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4362 - accuracy: 0.7964 - val_loss: 0.4865 - val_accuracy: 0.7976\n",
      "Epoch 1080/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4333 - accuracy: 0.8004 - val_loss: 0.4916 - val_accuracy: 0.7976\n",
      "Epoch 1081/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4341 - accuracy: 0.7904 - val_loss: 0.4903 - val_accuracy: 0.7935\n",
      "Epoch 1082/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4368 - accuracy: 0.8064 - val_loss: 0.4945 - val_accuracy: 0.7854\n",
      "Epoch 1083/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4377 - accuracy: 0.7944 - val_loss: 0.4922 - val_accuracy: 0.7935\n",
      "Epoch 1084/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4353 - accuracy: 0.7964 - val_loss: 0.4889 - val_accuracy: 0.7935\n",
      "Epoch 1085/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4318 - accuracy: 0.8104 - val_loss: 0.4901 - val_accuracy: 0.7854\n",
      "Epoch 1086/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4289 - accuracy: 0.8104 - val_loss: 0.4879 - val_accuracy: 0.7854\n",
      "Epoch 1087/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4299 - accuracy: 0.8004 - val_loss: 0.4899 - val_accuracy: 0.7976\n",
      "Epoch 1088/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4378 - accuracy: 0.7964 - val_loss: 0.4911 - val_accuracy: 0.7895\n",
      "Epoch 1089/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4413 - accuracy: 0.7864 - val_loss: 0.4893 - val_accuracy: 0.7895\n",
      "Epoch 1090/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4271 - accuracy: 0.8024 - val_loss: 0.4890 - val_accuracy: 0.7935\n",
      "Epoch 1091/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4309 - accuracy: 0.7964 - val_loss: 0.4936 - val_accuracy: 0.7935\n",
      "Epoch 1092/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4286 - accuracy: 0.8004 - val_loss: 0.4877 - val_accuracy: 0.7935\n",
      "Epoch 1093/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4297 - accuracy: 0.8024 - val_loss: 0.4905 - val_accuracy: 0.7935\n",
      "Epoch 1094/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4238 - accuracy: 0.8024 - val_loss: 0.4877 - val_accuracy: 0.7935\n",
      "Epoch 1095/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4339 - accuracy: 0.8044 - val_loss: 0.4928 - val_accuracy: 0.7935\n",
      "Epoch 1096/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4320 - accuracy: 0.7904 - val_loss: 0.4914 - val_accuracy: 0.7976\n",
      "Epoch 1097/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4379 - accuracy: 0.8024 - val_loss: 0.4878 - val_accuracy: 0.8016\n",
      "Epoch 1098/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4324 - accuracy: 0.7924 - val_loss: 0.4888 - val_accuracy: 0.7935\n",
      "Epoch 1099/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4398 - accuracy: 0.8004 - val_loss: 0.4954 - val_accuracy: 0.7854\n",
      "Epoch 1100/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4375 - accuracy: 0.7944 - val_loss: 0.4896 - val_accuracy: 0.8016\n",
      "Epoch 1101/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4316 - accuracy: 0.8004 - val_loss: 0.4925 - val_accuracy: 0.8016\n",
      "Epoch 1102/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4280 - accuracy: 0.8044 - val_loss: 0.4939 - val_accuracy: 0.7935\n",
      "Epoch 1103/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4294 - accuracy: 0.7984 - val_loss: 0.4887 - val_accuracy: 0.7935\n",
      "Epoch 1104/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4248 - accuracy: 0.8084 - val_loss: 0.4939 - val_accuracy: 0.7854\n",
      "Epoch 1105/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4286 - accuracy: 0.7964 - val_loss: 0.4897 - val_accuracy: 0.7935\n",
      "Epoch 1106/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4338 - accuracy: 0.8204 - val_loss: 0.4927 - val_accuracy: 0.7895\n",
      "Epoch 1107/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4349 - accuracy: 0.7844 - val_loss: 0.4930 - val_accuracy: 0.7895\n",
      "Epoch 1108/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4349 - accuracy: 0.8104 - val_loss: 0.4880 - val_accuracy: 0.7935\n",
      "Epoch 1109/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4372 - accuracy: 0.8024 - val_loss: 0.4958 - val_accuracy: 0.7895\n",
      "Epoch 1110/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4297 - accuracy: 0.8024 - val_loss: 0.4905 - val_accuracy: 0.7976\n",
      "Epoch 1111/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4281 - accuracy: 0.8024 - val_loss: 0.4920 - val_accuracy: 0.7935\n",
      "Epoch 1112/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4329 - accuracy: 0.7944 - val_loss: 0.4922 - val_accuracy: 0.7935\n",
      "Epoch 1113/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4322 - accuracy: 0.7944 - val_loss: 0.4921 - val_accuracy: 0.7935\n",
      "Epoch 1114/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4342 - accuracy: 0.7904 - val_loss: 0.4922 - val_accuracy: 0.8016\n",
      "Epoch 1115/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4321 - accuracy: 0.8144 - val_loss: 0.4937 - val_accuracy: 0.7976\n",
      "Epoch 1116/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4331 - accuracy: 0.8084 - val_loss: 0.4924 - val_accuracy: 0.7935\n",
      "Epoch 1117/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4309 - accuracy: 0.8064 - val_loss: 0.4931 - val_accuracy: 0.8016\n",
      "Epoch 1118/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4224 - accuracy: 0.8144 - val_loss: 0.4956 - val_accuracy: 0.8057\n",
      "Epoch 1119/1600\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4250 - accuracy: 0.8044 - val_loss: 0.4921 - val_accuracy: 0.8016\n",
      "Epoch 1120/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4382 - accuracy: 0.7924 - val_loss: 0.4932 - val_accuracy: 0.7935\n",
      "Epoch 1121/1600\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4324 - accuracy: 0.7964 - val_loss: 0.4891 - val_accuracy: 0.7976\n",
      "Epoch 1122/1600\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4283 - accuracy: 0.8004 - val_loss: 0.4959 - val_accuracy: 0.8016\n",
      "Epoch 1123/1600\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4382 - accuracy: 0.7944 - val_loss: 0.4908 - val_accuracy: 0.8016\n",
      "Epoch 1124/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4379 - accuracy: 0.8024 - val_loss: 0.4946 - val_accuracy: 0.7854\n",
      "Epoch 1125/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4379 - accuracy: 0.8044 - val_loss: 0.4985 - val_accuracy: 0.8016\n",
      "Epoch 1126/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4301 - accuracy: 0.8024 - val_loss: 0.4961 - val_accuracy: 0.7935\n",
      "Epoch 1127/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4322 - accuracy: 0.7984 - val_loss: 0.4920 - val_accuracy: 0.7935\n",
      "Epoch 1128/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4294 - accuracy: 0.7964 - val_loss: 0.4905 - val_accuracy: 0.7895\n",
      "Epoch 1129/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4326 - accuracy: 0.7984 - val_loss: 0.4909 - val_accuracy: 0.7976\n",
      "Epoch 1130/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4349 - accuracy: 0.8004 - val_loss: 0.4936 - val_accuracy: 0.7935\n",
      "Epoch 1131/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4362 - accuracy: 0.7924 - val_loss: 0.4928 - val_accuracy: 0.7976\n",
      "Epoch 1132/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4387 - accuracy: 0.8024 - val_loss: 0.4966 - val_accuracy: 0.7895\n",
      "Epoch 1133/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4269 - accuracy: 0.8144 - val_loss: 0.4949 - val_accuracy: 0.7854\n",
      "Epoch 1134/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4328 - accuracy: 0.8024 - val_loss: 0.4929 - val_accuracy: 0.7854\n",
      "Epoch 1135/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4285 - accuracy: 0.8184 - val_loss: 0.4931 - val_accuracy: 0.7854\n",
      "Epoch 1136/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4327 - accuracy: 0.7844 - val_loss: 0.4959 - val_accuracy: 0.7854\n",
      "Epoch 1137/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4341 - accuracy: 0.7944 - val_loss: 0.4951 - val_accuracy: 0.7854\n",
      "Epoch 1138/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4322 - accuracy: 0.7984 - val_loss: 0.4958 - val_accuracy: 0.7976\n",
      "Epoch 1139/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4304 - accuracy: 0.7964 - val_loss: 0.4918 - val_accuracy: 0.8057\n",
      "Epoch 1140/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4294 - accuracy: 0.7964 - val_loss: 0.4938 - val_accuracy: 0.8057\n",
      "Epoch 1141/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4368 - accuracy: 0.7864 - val_loss: 0.4901 - val_accuracy: 0.7854\n",
      "Epoch 1142/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4332 - accuracy: 0.7964 - val_loss: 0.4977 - val_accuracy: 0.7854\n",
      "Epoch 1143/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4316 - accuracy: 0.7864 - val_loss: 0.4924 - val_accuracy: 0.7976\n",
      "Epoch 1144/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4280 - accuracy: 0.8044 - val_loss: 0.4966 - val_accuracy: 0.7854\n",
      "Epoch 1145/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4392 - accuracy: 0.7884 - val_loss: 0.4946 - val_accuracy: 0.7895\n",
      "Epoch 1146/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4343 - accuracy: 0.7944 - val_loss: 0.4934 - val_accuracy: 0.7895\n",
      "Epoch 1147/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4318 - accuracy: 0.8164 - val_loss: 0.4922 - val_accuracy: 0.7895\n",
      "Epoch 1148/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4342 - accuracy: 0.7904 - val_loss: 0.4958 - val_accuracy: 0.8016\n",
      "Epoch 1149/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4341 - accuracy: 0.7924 - val_loss: 0.4928 - val_accuracy: 0.7935\n",
      "Epoch 1150/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4330 - accuracy: 0.7984 - val_loss: 0.4939 - val_accuracy: 0.7895\n",
      "Epoch 1151/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4318 - accuracy: 0.8064 - val_loss: 0.4915 - val_accuracy: 0.7935\n",
      "Epoch 1152/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4293 - accuracy: 0.8024 - val_loss: 0.4906 - val_accuracy: 0.7854\n",
      "Epoch 1153/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4320 - accuracy: 0.8044 - val_loss: 0.4988 - val_accuracy: 0.7895\n",
      "Epoch 1154/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4302 - accuracy: 0.7884 - val_loss: 0.4929 - val_accuracy: 0.7854\n",
      "Epoch 1155/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4295 - accuracy: 0.8004 - val_loss: 0.4946 - val_accuracy: 0.7976\n",
      "Epoch 1156/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4324 - accuracy: 0.7924 - val_loss: 0.4937 - val_accuracy: 0.8016\n",
      "Epoch 1157/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4345 - accuracy: 0.7944 - val_loss: 0.4937 - val_accuracy: 0.7895\n",
      "Epoch 1158/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4319 - accuracy: 0.7984 - val_loss: 0.4940 - val_accuracy: 0.8016\n",
      "Epoch 1159/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4355 - accuracy: 0.8004 - val_loss: 0.4964 - val_accuracy: 0.7854\n",
      "Epoch 1160/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4319 - accuracy: 0.8024 - val_loss: 0.4929 - val_accuracy: 0.7895\n",
      "Epoch 1161/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4420 - accuracy: 0.7884 - val_loss: 0.4958 - val_accuracy: 0.7935\n",
      "Epoch 1162/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4327 - accuracy: 0.8004 - val_loss: 0.4970 - val_accuracy: 0.8016\n",
      "Epoch 1163/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4297 - accuracy: 0.7964 - val_loss: 0.4927 - val_accuracy: 0.7854\n",
      "Epoch 1164/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4339 - accuracy: 0.7984 - val_loss: 0.4998 - val_accuracy: 0.7935\n",
      "Epoch 1165/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4263 - accuracy: 0.8044 - val_loss: 0.4991 - val_accuracy: 0.8016\n",
      "Epoch 1166/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4240 - accuracy: 0.8084 - val_loss: 0.4911 - val_accuracy: 0.8016\n",
      "Epoch 1167/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4251 - accuracy: 0.8104 - val_loss: 0.4975 - val_accuracy: 0.7976\n",
      "Epoch 1168/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4280 - accuracy: 0.8104 - val_loss: 0.4960 - val_accuracy: 0.7935\n",
      "Epoch 1169/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4231 - accuracy: 0.8044 - val_loss: 0.4926 - val_accuracy: 0.7935\n",
      "Epoch 1170/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4305 - accuracy: 0.8064 - val_loss: 0.4935 - val_accuracy: 0.8016\n",
      "Epoch 1171/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4259 - accuracy: 0.8004 - val_loss: 0.4894 - val_accuracy: 0.7935\n",
      "Epoch 1172/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4278 - accuracy: 0.8164 - val_loss: 0.4970 - val_accuracy: 0.7895\n",
      "Epoch 1173/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4410 - accuracy: 0.7924 - val_loss: 0.4954 - val_accuracy: 0.8016\n",
      "Epoch 1174/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4345 - accuracy: 0.7984 - val_loss: 0.4972 - val_accuracy: 0.7854\n",
      "Epoch 1175/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4315 - accuracy: 0.7964 - val_loss: 0.4969 - val_accuracy: 0.7895\n",
      "Epoch 1176/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4270 - accuracy: 0.8024 - val_loss: 0.4907 - val_accuracy: 0.7895\n",
      "Epoch 1177/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4278 - accuracy: 0.7944 - val_loss: 0.4930 - val_accuracy: 0.7976\n",
      "Epoch 1178/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4316 - accuracy: 0.8004 - val_loss: 0.4929 - val_accuracy: 0.7976\n",
      "Epoch 1179/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4354 - accuracy: 0.8024 - val_loss: 0.4967 - val_accuracy: 0.7935\n",
      "Epoch 1180/1600\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.4356 - accuracy: 0.81 - 0s 3ms/step - loss: 0.4322 - accuracy: 0.8044 - val_loss: 0.4905 - val_accuracy: 0.7935\n",
      "Epoch 1181/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4342 - accuracy: 0.8004 - val_loss: 0.4951 - val_accuracy: 0.8016\n",
      "Epoch 1182/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4313 - accuracy: 0.8084 - val_loss: 0.4913 - val_accuracy: 0.7976\n",
      "Epoch 1183/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4383 - accuracy: 0.8024 - val_loss: 0.4939 - val_accuracy: 0.8057\n",
      "Epoch 1184/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4271 - accuracy: 0.8084 - val_loss: 0.4921 - val_accuracy: 0.7935\n",
      "Epoch 1185/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4293 - accuracy: 0.8044 - val_loss: 0.4953 - val_accuracy: 0.7976\n",
      "Epoch 1186/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4280 - accuracy: 0.8044 - val_loss: 0.4954 - val_accuracy: 0.7935\n",
      "Epoch 1187/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4349 - accuracy: 0.8044 - val_loss: 0.4918 - val_accuracy: 0.7854\n",
      "Epoch 1188/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4349 - accuracy: 0.8084 - val_loss: 0.4972 - val_accuracy: 0.7935\n",
      "Epoch 1189/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4251 - accuracy: 0.8024 - val_loss: 0.4928 - val_accuracy: 0.7935\n",
      "Epoch 1190/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4337 - accuracy: 0.8004 - val_loss: 0.4980 - val_accuracy: 0.7935\n",
      "Epoch 1191/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4331 - accuracy: 0.7944 - val_loss: 0.4909 - val_accuracy: 0.7895\n",
      "Epoch 1192/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4280 - accuracy: 0.7964 - val_loss: 0.4948 - val_accuracy: 0.8016\n",
      "Epoch 1193/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4299 - accuracy: 0.8024 - val_loss: 0.4975 - val_accuracy: 0.8016\n",
      "Epoch 1194/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4307 - accuracy: 0.7964 - val_loss: 0.4954 - val_accuracy: 0.7854\n",
      "Epoch 1195/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4338 - accuracy: 0.8004 - val_loss: 0.4966 - val_accuracy: 0.7854\n",
      "Epoch 1196/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4308 - accuracy: 0.7984 - val_loss: 0.4958 - val_accuracy: 0.8057\n",
      "Epoch 1197/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4323 - accuracy: 0.7924 - val_loss: 0.4944 - val_accuracy: 0.7935\n",
      "Epoch 1198/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4343 - accuracy: 0.8084 - val_loss: 0.4960 - val_accuracy: 0.7935\n",
      "Epoch 1199/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4260 - accuracy: 0.7964 - val_loss: 0.4986 - val_accuracy: 0.7935\n",
      "Epoch 1200/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4291 - accuracy: 0.8124 - val_loss: 0.4950 - val_accuracy: 0.7895\n",
      "Epoch 1201/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4383 - accuracy: 0.7924 - val_loss: 0.4959 - val_accuracy: 0.8016\n",
      "Epoch 1202/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4224 - accuracy: 0.8144 - val_loss: 0.4908 - val_accuracy: 0.7895\n",
      "Epoch 1203/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4301 - accuracy: 0.7964 - val_loss: 0.4984 - val_accuracy: 0.7854\n",
      "Epoch 1204/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4261 - accuracy: 0.7984 - val_loss: 0.4944 - val_accuracy: 0.7895\n",
      "Epoch 1205/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4296 - accuracy: 0.7984 - val_loss: 0.4922 - val_accuracy: 0.8016\n",
      "Epoch 1206/1600\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4306 - accuracy: 0.8004 - val_loss: 0.4919 - val_accuracy: 0.7935\n",
      "Epoch 1207/1600\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4304 - accuracy: 0.7984 - val_loss: 0.4936 - val_accuracy: 0.8016\n",
      "Epoch 1208/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4312 - accuracy: 0.8004 - val_loss: 0.4991 - val_accuracy: 0.7976\n",
      "Epoch 1209/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4250 - accuracy: 0.8064 - val_loss: 0.4918 - val_accuracy: 0.8016\n",
      "Epoch 1210/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4285 - accuracy: 0.8024 - val_loss: 0.4975 - val_accuracy: 0.8016\n",
      "Epoch 1211/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4321 - accuracy: 0.7844 - val_loss: 0.4989 - val_accuracy: 0.8016\n",
      "Epoch 1212/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4268 - accuracy: 0.8064 - val_loss: 0.4932 - val_accuracy: 0.7976\n",
      "Epoch 1213/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4339 - accuracy: 0.7904 - val_loss: 0.4928 - val_accuracy: 0.7976\n",
      "Epoch 1214/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4269 - accuracy: 0.8064 - val_loss: 0.4945 - val_accuracy: 0.7935\n",
      "Epoch 1215/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4254 - accuracy: 0.8124 - val_loss: 0.4989 - val_accuracy: 0.7935\n",
      "Epoch 1216/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4332 - accuracy: 0.7924 - val_loss: 0.4959 - val_accuracy: 0.7935\n",
      "Epoch 1217/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4231 - accuracy: 0.8064 - val_loss: 0.4939 - val_accuracy: 0.8057\n",
      "Epoch 1218/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4308 - accuracy: 0.8064 - val_loss: 0.5006 - val_accuracy: 0.7976\n",
      "Epoch 1219/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4328 - accuracy: 0.8104 - val_loss: 0.4965 - val_accuracy: 0.7976\n",
      "Epoch 1220/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4310 - accuracy: 0.8024 - val_loss: 0.4953 - val_accuracy: 0.7935\n",
      "Epoch 1221/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4298 - accuracy: 0.7984 - val_loss: 0.4954 - val_accuracy: 0.7935\n",
      "Epoch 1222/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4258 - accuracy: 0.8044 - val_loss: 0.4973 - val_accuracy: 0.8016\n",
      "Epoch 1223/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4329 - accuracy: 0.8024 - val_loss: 0.4962 - val_accuracy: 0.7895\n",
      "Epoch 1224/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4331 - accuracy: 0.7924 - val_loss: 0.4969 - val_accuracy: 0.7976\n",
      "Epoch 1225/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4311 - accuracy: 0.7984 - val_loss: 0.4926 - val_accuracy: 0.7854\n",
      "Epoch 1226/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4372 - accuracy: 0.7944 - val_loss: 0.4979 - val_accuracy: 0.7935\n",
      "Epoch 1227/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4300 - accuracy: 0.8064 - val_loss: 0.4939 - val_accuracy: 0.7895\n",
      "Epoch 1228/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4323 - accuracy: 0.7944 - val_loss: 0.5021 - val_accuracy: 0.7935\n",
      "Epoch 1229/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4267 - accuracy: 0.8104 - val_loss: 0.4926 - val_accuracy: 0.7895\n",
      "Epoch 1230/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4279 - accuracy: 0.8024 - val_loss: 0.4928 - val_accuracy: 0.7854\n",
      "Epoch 1231/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4315 - accuracy: 0.8024 - val_loss: 0.4965 - val_accuracy: 0.7935\n",
      "Epoch 1232/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4340 - accuracy: 0.7964 - val_loss: 0.4942 - val_accuracy: 0.8016\n",
      "Epoch 1233/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4260 - accuracy: 0.8004 - val_loss: 0.4924 - val_accuracy: 0.7935\n",
      "Epoch 1234/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4308 - accuracy: 0.8044 - val_loss: 0.4938 - val_accuracy: 0.7976\n",
      "Epoch 1235/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4308 - accuracy: 0.7984 - val_loss: 0.4947 - val_accuracy: 0.7976\n",
      "Epoch 1236/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4307 - accuracy: 0.8044 - val_loss: 0.5021 - val_accuracy: 0.7854\n",
      "Epoch 1237/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4269 - accuracy: 0.7984 - val_loss: 0.4960 - val_accuracy: 0.7854\n",
      "Epoch 1238/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4224 - accuracy: 0.8024 - val_loss: 0.4929 - val_accuracy: 0.7976\n",
      "Epoch 1239/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4307 - accuracy: 0.8044 - val_loss: 0.4964 - val_accuracy: 0.8016\n",
      "Epoch 1240/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4280 - accuracy: 0.7984 - val_loss: 0.4986 - val_accuracy: 0.8016\n",
      "Epoch 1241/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4276 - accuracy: 0.7904 - val_loss: 0.4941 - val_accuracy: 0.8016\n",
      "Epoch 1242/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4310 - accuracy: 0.8024 - val_loss: 0.5021 - val_accuracy: 0.7935\n",
      "Epoch 1243/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4242 - accuracy: 0.8004 - val_loss: 0.4942 - val_accuracy: 0.8016\n",
      "Epoch 1244/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4348 - accuracy: 0.8004 - val_loss: 0.4987 - val_accuracy: 0.8016\n",
      "Epoch 1245/1600\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.5342 - accuracy: 0.71 - 0s 3ms/step - loss: 0.4345 - accuracy: 0.7964 - val_loss: 0.4975 - val_accuracy: 0.7976\n",
      "Epoch 1246/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4357 - accuracy: 0.7964 - val_loss: 0.4999 - val_accuracy: 0.7895\n",
      "Epoch 1247/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4295 - accuracy: 0.7984 - val_loss: 0.4928 - val_accuracy: 0.7854\n",
      "Epoch 1248/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4243 - accuracy: 0.8104 - val_loss: 0.4992 - val_accuracy: 0.7854\n",
      "Epoch 1249/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4248 - accuracy: 0.7984 - val_loss: 0.4996 - val_accuracy: 0.7895\n",
      "Epoch 1250/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4290 - accuracy: 0.7944 - val_loss: 0.4933 - val_accuracy: 0.7935\n",
      "Epoch 1251/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4299 - accuracy: 0.7984 - val_loss: 0.5013 - val_accuracy: 0.7976\n",
      "Epoch 1252/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4277 - accuracy: 0.8084 - val_loss: 0.4949 - val_accuracy: 0.7976\n",
      "Epoch 1253/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4341 - accuracy: 0.8024 - val_loss: 0.4973 - val_accuracy: 0.7895\n",
      "Epoch 1254/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4263 - accuracy: 0.8044 - val_loss: 0.5043 - val_accuracy: 0.7976\n",
      "Epoch 1255/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4287 - accuracy: 0.7844 - val_loss: 0.4938 - val_accuracy: 0.7935\n",
      "Epoch 1256/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4364 - accuracy: 0.7944 - val_loss: 0.5027 - val_accuracy: 0.7935\n",
      "Epoch 1257/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4369 - accuracy: 0.7964 - val_loss: 0.4995 - val_accuracy: 0.8016\n",
      "Epoch 1258/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4290 - accuracy: 0.7964 - val_loss: 0.4938 - val_accuracy: 0.8016\n",
      "Epoch 1259/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4283 - accuracy: 0.8004 - val_loss: 0.4979 - val_accuracy: 0.7935\n",
      "Epoch 1260/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4415 - accuracy: 0.8024 - val_loss: 0.4995 - val_accuracy: 0.7935\n",
      "Epoch 1261/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4218 - accuracy: 0.7964 - val_loss: 0.4906 - val_accuracy: 0.7935\n",
      "Epoch 1262/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4302 - accuracy: 0.7964 - val_loss: 0.5010 - val_accuracy: 0.7895\n",
      "Epoch 1263/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4311 - accuracy: 0.7944 - val_loss: 0.4981 - val_accuracy: 0.7935\n",
      "Epoch 1264/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4269 - accuracy: 0.8044 - val_loss: 0.4934 - val_accuracy: 0.8016\n",
      "Epoch 1265/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4271 - accuracy: 0.8024 - val_loss: 0.5050 - val_accuracy: 0.8016\n",
      "Epoch 1266/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4327 - accuracy: 0.7964 - val_loss: 0.4922 - val_accuracy: 0.7935\n",
      "Epoch 1267/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4279 - accuracy: 0.8064 - val_loss: 0.5029 - val_accuracy: 0.7935\n",
      "Epoch 1268/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4324 - accuracy: 0.8064 - val_loss: 0.4963 - val_accuracy: 0.7895\n",
      "Epoch 1269/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4305 - accuracy: 0.7884 - val_loss: 0.4994 - val_accuracy: 0.7976\n",
      "Epoch 1270/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4276 - accuracy: 0.8064 - val_loss: 0.4950 - val_accuracy: 0.7895\n",
      "Epoch 1271/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4299 - accuracy: 0.8084 - val_loss: 0.5006 - val_accuracy: 0.7895\n",
      "Epoch 1272/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4304 - accuracy: 0.8104 - val_loss: 0.4994 - val_accuracy: 0.7976\n",
      "Epoch 1273/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4291 - accuracy: 0.8044 - val_loss: 0.4992 - val_accuracy: 0.7976\n",
      "Epoch 1274/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4327 - accuracy: 0.7964 - val_loss: 0.4975 - val_accuracy: 0.7976\n",
      "Epoch 1275/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4273 - accuracy: 0.8024 - val_loss: 0.4996 - val_accuracy: 0.7976\n",
      "Epoch 1276/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4258 - accuracy: 0.7964 - val_loss: 0.4999 - val_accuracy: 0.7895\n",
      "Epoch 1277/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4264 - accuracy: 0.8104 - val_loss: 0.4979 - val_accuracy: 0.7976\n",
      "Epoch 1278/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4267 - accuracy: 0.8004 - val_loss: 0.4926 - val_accuracy: 0.7895\n",
      "Epoch 1279/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4280 - accuracy: 0.8124 - val_loss: 0.4953 - val_accuracy: 0.7895\n",
      "Epoch 1280/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4342 - accuracy: 0.7984 - val_loss: 0.5074 - val_accuracy: 0.8016\n",
      "Epoch 1281/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4313 - accuracy: 0.8044 - val_loss: 0.4944 - val_accuracy: 0.8016\n",
      "Epoch 1282/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4301 - accuracy: 0.8044 - val_loss: 0.5007 - val_accuracy: 0.8016\n",
      "Epoch 1283/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4268 - accuracy: 0.8024 - val_loss: 0.5031 - val_accuracy: 0.7854\n",
      "Epoch 1284/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4305 - accuracy: 0.7964 - val_loss: 0.4930 - val_accuracy: 0.7854\n",
      "Epoch 1285/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4363 - accuracy: 0.8004 - val_loss: 0.5012 - val_accuracy: 0.7935\n",
      "Epoch 1286/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4296 - accuracy: 0.8144 - val_loss: 0.4985 - val_accuracy: 0.7976\n",
      "Epoch 1287/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4272 - accuracy: 0.8064 - val_loss: 0.4956 - val_accuracy: 0.7854\n",
      "Epoch 1288/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4300 - accuracy: 0.8004 - val_loss: 0.5033 - val_accuracy: 0.7976\n",
      "Epoch 1289/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4454 - accuracy: 0.7964 - val_loss: 0.4977 - val_accuracy: 0.7935\n",
      "Epoch 1290/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4337 - accuracy: 0.8004 - val_loss: 0.4984 - val_accuracy: 0.8016\n",
      "Epoch 1291/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4332 - accuracy: 0.7964 - val_loss: 0.4927 - val_accuracy: 0.8016\n",
      "Epoch 1292/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4300 - accuracy: 0.7904 - val_loss: 0.4972 - val_accuracy: 0.7854\n",
      "Epoch 1293/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4367 - accuracy: 0.8024 - val_loss: 0.5006 - val_accuracy: 0.7976\n",
      "Epoch 1294/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4309 - accuracy: 0.7924 - val_loss: 0.4949 - val_accuracy: 0.7976\n",
      "Epoch 1295/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4283 - accuracy: 0.8024 - val_loss: 0.4924 - val_accuracy: 0.7976\n",
      "Epoch 1296/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4373 - accuracy: 0.7904 - val_loss: 0.4946 - val_accuracy: 0.7895\n",
      "Epoch 1297/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4347 - accuracy: 0.8044 - val_loss: 0.4941 - val_accuracy: 0.7935\n",
      "Epoch 1298/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4337 - accuracy: 0.7964 - val_loss: 0.5011 - val_accuracy: 0.8016\n",
      "Epoch 1299/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4384 - accuracy: 0.7964 - val_loss: 0.4998 - val_accuracy: 0.7854\n",
      "Epoch 1300/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4280 - accuracy: 0.7984 - val_loss: 0.4963 - val_accuracy: 0.7854\n",
      "Epoch 1301/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4306 - accuracy: 0.8004 - val_loss: 0.4982 - val_accuracy: 0.7935\n",
      "Epoch 1302/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4316 - accuracy: 0.8004 - val_loss: 0.5005 - val_accuracy: 0.7935\n",
      "Epoch 1303/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4281 - accuracy: 0.8024 - val_loss: 0.4975 - val_accuracy: 0.7935\n",
      "Epoch 1304/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4222 - accuracy: 0.8064 - val_loss: 0.4968 - val_accuracy: 0.7935\n",
      "Epoch 1305/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4312 - accuracy: 0.8044 - val_loss: 0.4952 - val_accuracy: 0.7976\n",
      "Epoch 1306/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4310 - accuracy: 0.7924 - val_loss: 0.5009 - val_accuracy: 0.8016\n",
      "Epoch 1307/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4288 - accuracy: 0.8044 - val_loss: 0.4978 - val_accuracy: 0.8016\n",
      "Epoch 1308/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4349 - accuracy: 0.8064 - val_loss: 0.4996 - val_accuracy: 0.7935\n",
      "Epoch 1309/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4271 - accuracy: 0.8044 - val_loss: 0.4971 - val_accuracy: 0.7854\n",
      "Epoch 1310/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4261 - accuracy: 0.8044 - val_loss: 0.4951 - val_accuracy: 0.7935\n",
      "Epoch 1311/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4282 - accuracy: 0.8104 - val_loss: 0.4988 - val_accuracy: 0.7976\n",
      "Epoch 1312/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4277 - accuracy: 0.8024 - val_loss: 0.4998 - val_accuracy: 0.7976\n",
      "Epoch 1313/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4257 - accuracy: 0.8084 - val_loss: 0.4934 - val_accuracy: 0.8016\n",
      "Epoch 1314/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4293 - accuracy: 0.7924 - val_loss: 0.4929 - val_accuracy: 0.7895\n",
      "Epoch 1315/1600\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4355 - accuracy: 0.7904 - val_loss: 0.4956 - val_accuracy: 0.8016\n",
      "Epoch 1316/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4264 - accuracy: 0.7984 - val_loss: 0.4981 - val_accuracy: 0.7976\n",
      "Epoch 1317/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4275 - accuracy: 0.8044 - val_loss: 0.4995 - val_accuracy: 0.7976\n",
      "Epoch 1318/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4273 - accuracy: 0.8004 - val_loss: 0.4935 - val_accuracy: 0.7895\n",
      "Epoch 1319/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4298 - accuracy: 0.8044 - val_loss: 0.4984 - val_accuracy: 0.7935\n",
      "Epoch 1320/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4294 - accuracy: 0.8004 - val_loss: 0.5038 - val_accuracy: 0.8016\n",
      "Epoch 1321/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4316 - accuracy: 0.8004 - val_loss: 0.5001 - val_accuracy: 0.8016\n",
      "Epoch 1322/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4272 - accuracy: 0.8064 - val_loss: 0.4977 - val_accuracy: 0.7895\n",
      "Epoch 1323/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4361 - accuracy: 0.7904 - val_loss: 0.5012 - val_accuracy: 0.7935\n",
      "Epoch 1324/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4186 - accuracy: 0.8024 - val_loss: 0.4984 - val_accuracy: 0.8016\n",
      "Epoch 1325/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4255 - accuracy: 0.8064 - val_loss: 0.5008 - val_accuracy: 0.8016\n",
      "Epoch 1326/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4299 - accuracy: 0.7964 - val_loss: 0.4947 - val_accuracy: 0.7935\n",
      "Epoch 1327/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4308 - accuracy: 0.8084 - val_loss: 0.5008 - val_accuracy: 0.7976\n",
      "Epoch 1328/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4246 - accuracy: 0.7984 - val_loss: 0.4980 - val_accuracy: 0.8016\n",
      "Epoch 1329/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4331 - accuracy: 0.7864 - val_loss: 0.5008 - val_accuracy: 0.7854\n",
      "Epoch 1330/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4259 - accuracy: 0.8064 - val_loss: 0.4983 - val_accuracy: 0.7854\n",
      "Epoch 1331/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4310 - accuracy: 0.7904 - val_loss: 0.4988 - val_accuracy: 0.7976\n",
      "Epoch 1332/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4264 - accuracy: 0.8004 - val_loss: 0.4939 - val_accuracy: 0.7976\n",
      "Epoch 1333/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4313 - accuracy: 0.7964 - val_loss: 0.5002 - val_accuracy: 0.8057\n",
      "Epoch 1334/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4214 - accuracy: 0.8144 - val_loss: 0.5011 - val_accuracy: 0.7935\n",
      "Epoch 1335/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4290 - accuracy: 0.7964 - val_loss: 0.4959 - val_accuracy: 0.7854\n",
      "Epoch 1336/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4307 - accuracy: 0.7984 - val_loss: 0.5002 - val_accuracy: 0.7976\n",
      "Epoch 1337/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4273 - accuracy: 0.8044 - val_loss: 0.5005 - val_accuracy: 0.7976\n",
      "Epoch 1338/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4351 - accuracy: 0.7964 - val_loss: 0.4937 - val_accuracy: 0.7976\n",
      "Epoch 1339/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4280 - accuracy: 0.8084 - val_loss: 0.5000 - val_accuracy: 0.7976\n",
      "Epoch 1340/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4194 - accuracy: 0.8064 - val_loss: 0.4942 - val_accuracy: 0.7976\n",
      "Epoch 1341/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4322 - accuracy: 0.7964 - val_loss: 0.4985 - val_accuracy: 0.7976\n",
      "Epoch 1342/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4336 - accuracy: 0.7944 - val_loss: 0.5026 - val_accuracy: 0.7854\n",
      "Epoch 1343/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4279 - accuracy: 0.8084 - val_loss: 0.5026 - val_accuracy: 0.7976\n",
      "Epoch 1344/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4314 - accuracy: 0.7984 - val_loss: 0.4939 - val_accuracy: 0.8016\n",
      "Epoch 1345/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4284 - accuracy: 0.7944 - val_loss: 0.4981 - val_accuracy: 0.8016\n",
      "Epoch 1346/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4290 - accuracy: 0.7964 - val_loss: 0.4991 - val_accuracy: 0.8016\n",
      "Epoch 1347/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4270 - accuracy: 0.7984 - val_loss: 0.5012 - val_accuracy: 0.7935\n",
      "Epoch 1348/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4268 - accuracy: 0.8044 - val_loss: 0.4985 - val_accuracy: 0.7976\n",
      "Epoch 1349/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4272 - accuracy: 0.8084 - val_loss: 0.5006 - val_accuracy: 0.7935\n",
      "Epoch 1350/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4305 - accuracy: 0.8044 - val_loss: 0.4985 - val_accuracy: 0.7976\n",
      "Epoch 1351/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4400 - accuracy: 0.7964 - val_loss: 0.4997 - val_accuracy: 0.7935\n",
      "Epoch 1352/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4290 - accuracy: 0.7964 - val_loss: 0.4992 - val_accuracy: 0.7935\n",
      "Epoch 1353/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4224 - accuracy: 0.7984 - val_loss: 0.4984 - val_accuracy: 0.7895\n",
      "Epoch 1354/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4332 - accuracy: 0.8024 - val_loss: 0.5056 - val_accuracy: 0.7976\n",
      "Epoch 1355/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4318 - accuracy: 0.8024 - val_loss: 0.4959 - val_accuracy: 0.7895\n",
      "Epoch 1356/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4244 - accuracy: 0.8004 - val_loss: 0.4965 - val_accuracy: 0.7935\n",
      "Epoch 1357/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4330 - accuracy: 0.7964 - val_loss: 0.5009 - val_accuracy: 0.7976\n",
      "Epoch 1358/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4338 - accuracy: 0.7944 - val_loss: 0.4988 - val_accuracy: 0.8016\n",
      "Epoch 1359/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4258 - accuracy: 0.8004 - val_loss: 0.5024 - val_accuracy: 0.7854\n",
      "Epoch 1360/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4247 - accuracy: 0.8104 - val_loss: 0.5031 - val_accuracy: 0.7935\n",
      "Epoch 1361/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4276 - accuracy: 0.8004 - val_loss: 0.4933 - val_accuracy: 0.7976\n",
      "Epoch 1362/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4294 - accuracy: 0.8084 - val_loss: 0.4991 - val_accuracy: 0.7935\n",
      "Epoch 1363/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4288 - accuracy: 0.8084 - val_loss: 0.5041 - val_accuracy: 0.7935\n",
      "Epoch 1364/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4271 - accuracy: 0.8044 - val_loss: 0.4995 - val_accuracy: 0.8016\n",
      "Epoch 1365/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4288 - accuracy: 0.7964 - val_loss: 0.5028 - val_accuracy: 0.7935\n",
      "Epoch 1366/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4335 - accuracy: 0.8064 - val_loss: 0.5012 - val_accuracy: 0.7976\n",
      "Epoch 1367/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4279 - accuracy: 0.8004 - val_loss: 0.4975 - val_accuracy: 0.7976\n",
      "Epoch 1368/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4223 - accuracy: 0.8024 - val_loss: 0.4998 - val_accuracy: 0.7935\n",
      "Epoch 1369/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4329 - accuracy: 0.7884 - val_loss: 0.5011 - val_accuracy: 0.7895\n",
      "Epoch 1370/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4226 - accuracy: 0.8004 - val_loss: 0.5042 - val_accuracy: 0.7976\n",
      "Epoch 1371/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4315 - accuracy: 0.8064 - val_loss: 0.5010 - val_accuracy: 0.7935\n",
      "Epoch 1372/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4307 - accuracy: 0.7964 - val_loss: 0.5052 - val_accuracy: 0.7935\n",
      "Epoch 1373/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4365 - accuracy: 0.7964 - val_loss: 0.4974 - val_accuracy: 0.7854\n",
      "Epoch 1374/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4284 - accuracy: 0.8104 - val_loss: 0.5011 - val_accuracy: 0.8016\n",
      "Epoch 1375/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4344 - accuracy: 0.7964 - val_loss: 0.4981 - val_accuracy: 0.7935\n",
      "Epoch 1376/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4186 - accuracy: 0.8064 - val_loss: 0.5032 - val_accuracy: 0.7935\n",
      "Epoch 1377/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4267 - accuracy: 0.8004 - val_loss: 0.5015 - val_accuracy: 0.7935\n",
      "Epoch 1378/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4252 - accuracy: 0.8044 - val_loss: 0.4988 - val_accuracy: 0.7895\n",
      "Epoch 1379/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4315 - accuracy: 0.7884 - val_loss: 0.5020 - val_accuracy: 0.7976\n",
      "Epoch 1380/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4229 - accuracy: 0.8044 - val_loss: 0.4958 - val_accuracy: 0.7935\n",
      "Epoch 1381/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4320 - accuracy: 0.8044 - val_loss: 0.5008 - val_accuracy: 0.7976\n",
      "Epoch 1382/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4270 - accuracy: 0.8104 - val_loss: 0.4995 - val_accuracy: 0.7976\n",
      "Epoch 1383/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4290 - accuracy: 0.7924 - val_loss: 0.4990 - val_accuracy: 0.7976\n",
      "Epoch 1384/1600\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4350 - accuracy: 0.7984 - val_loss: 0.4979 - val_accuracy: 0.7935\n",
      "Epoch 1385/1600\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4325 - accuracy: 0.7924 - val_loss: 0.4973 - val_accuracy: 0.7935\n",
      "Epoch 1386/1600\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4245 - accuracy: 0.8124 - val_loss: 0.4999 - val_accuracy: 0.7976\n",
      "Epoch 1387/1600\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4325 - accuracy: 0.7984 - val_loss: 0.5045 - val_accuracy: 0.7935\n",
      "Epoch 1388/1600\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4308 - accuracy: 0.7964 - val_loss: 0.5029 - val_accuracy: 0.7976\n",
      "Epoch 1389/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4266 - accuracy: 0.7944 - val_loss: 0.5021 - val_accuracy: 0.7895\n",
      "Epoch 1390/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4282 - accuracy: 0.8104 - val_loss: 0.5017 - val_accuracy: 0.7935\n",
      "Epoch 1391/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4289 - accuracy: 0.7924 - val_loss: 0.4941 - val_accuracy: 0.7976\n",
      "Epoch 1392/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4314 - accuracy: 0.7924 - val_loss: 0.4990 - val_accuracy: 0.7895\n",
      "Epoch 1393/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4347 - accuracy: 0.7964 - val_loss: 0.5046 - val_accuracy: 0.7854\n",
      "Epoch 1394/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4286 - accuracy: 0.7944 - val_loss: 0.5016 - val_accuracy: 0.7935\n",
      "Epoch 1395/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4265 - accuracy: 0.7944 - val_loss: 0.4974 - val_accuracy: 0.8057\n",
      "Epoch 1396/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4314 - accuracy: 0.7884 - val_loss: 0.5024 - val_accuracy: 0.7935\n",
      "Epoch 1397/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4248 - accuracy: 0.8004 - val_loss: 0.4963 - val_accuracy: 0.7935\n",
      "Epoch 1398/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4323 - accuracy: 0.7984 - val_loss: 0.5037 - val_accuracy: 0.7935\n",
      "Epoch 1399/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4331 - accuracy: 0.8124 - val_loss: 0.5047 - val_accuracy: 0.7895\n",
      "Epoch 1400/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4349 - accuracy: 0.8044 - val_loss: 0.4962 - val_accuracy: 0.7976\n",
      "Epoch 1401/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4389 - accuracy: 0.7864 - val_loss: 0.5069 - val_accuracy: 0.7854\n",
      "Epoch 1402/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4226 - accuracy: 0.8044 - val_loss: 0.4982 - val_accuracy: 0.7854\n",
      "Epoch 1403/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4345 - accuracy: 0.7904 - val_loss: 0.5043 - val_accuracy: 0.7895\n",
      "Epoch 1404/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4219 - accuracy: 0.8024 - val_loss: 0.4959 - val_accuracy: 0.8016\n",
      "Epoch 1405/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4311 - accuracy: 0.8004 - val_loss: 0.5020 - val_accuracy: 0.7895\n",
      "Epoch 1406/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4302 - accuracy: 0.8084 - val_loss: 0.4967 - val_accuracy: 0.7935\n",
      "Epoch 1407/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4368 - accuracy: 0.7964 - val_loss: 0.5048 - val_accuracy: 0.7976\n",
      "Epoch 1408/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4251 - accuracy: 0.8084 - val_loss: 0.4958 - val_accuracy: 0.7935\n",
      "Epoch 1409/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4285 - accuracy: 0.8004 - val_loss: 0.5042 - val_accuracy: 0.7976\n",
      "Epoch 1410/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4275 - accuracy: 0.8004 - val_loss: 0.4993 - val_accuracy: 0.7976\n",
      "Epoch 1411/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4272 - accuracy: 0.8064 - val_loss: 0.5017 - val_accuracy: 0.7935\n",
      "Epoch 1412/1600\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4272 - accuracy: 0.7984 - val_loss: 0.5009 - val_accuracy: 0.7935\n",
      "Epoch 1413/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4357 - accuracy: 0.7964 - val_loss: 0.5001 - val_accuracy: 0.7935\n",
      "Epoch 1414/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4215 - accuracy: 0.8044 - val_loss: 0.5049 - val_accuracy: 0.7976\n",
      "Epoch 1415/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4263 - accuracy: 0.8044 - val_loss: 0.5031 - val_accuracy: 0.7895\n",
      "Epoch 1416/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4324 - accuracy: 0.8004 - val_loss: 0.5058 - val_accuracy: 0.7976\n",
      "Epoch 1417/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4210 - accuracy: 0.8044 - val_loss: 0.5025 - val_accuracy: 0.7895\n",
      "Epoch 1418/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4152 - accuracy: 0.8303 - val_loss: 0.4968 - val_accuracy: 0.8016\n",
      "Epoch 1419/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4366 - accuracy: 0.8004 - val_loss: 0.5033 - val_accuracy: 0.7935\n",
      "Epoch 1420/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4328 - accuracy: 0.7884 - val_loss: 0.4996 - val_accuracy: 0.7935\n",
      "Epoch 1421/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4185 - accuracy: 0.8084 - val_loss: 0.5049 - val_accuracy: 0.7935\n",
      "Epoch 1422/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4252 - accuracy: 0.8004 - val_loss: 0.4986 - val_accuracy: 0.7976\n",
      "Epoch 1423/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4368 - accuracy: 0.7964 - val_loss: 0.5066 - val_accuracy: 0.7976\n",
      "Epoch 1424/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4299 - accuracy: 0.8044 - val_loss: 0.5055 - val_accuracy: 0.7895\n",
      "Epoch 1425/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4279 - accuracy: 0.8004 - val_loss: 0.5005 - val_accuracy: 0.7935\n",
      "Epoch 1426/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4261 - accuracy: 0.8004 - val_loss: 0.5018 - val_accuracy: 0.8016\n",
      "Epoch 1427/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4257 - accuracy: 0.7904 - val_loss: 0.4952 - val_accuracy: 0.8016\n",
      "Epoch 1428/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4254 - accuracy: 0.8024 - val_loss: 0.5051 - val_accuracy: 0.7935\n",
      "Epoch 1429/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4312 - accuracy: 0.7824 - val_loss: 0.4994 - val_accuracy: 0.8016\n",
      "Epoch 1430/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4248 - accuracy: 0.8124 - val_loss: 0.5018 - val_accuracy: 0.7854\n",
      "Epoch 1431/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4354 - accuracy: 0.8084 - val_loss: 0.5023 - val_accuracy: 0.7935\n",
      "Epoch 1432/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4257 - accuracy: 0.7984 - val_loss: 0.4990 - val_accuracy: 0.7976\n",
      "Epoch 1433/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4159 - accuracy: 0.8044 - val_loss: 0.5078 - val_accuracy: 0.7854\n",
      "Epoch 1434/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4256 - accuracy: 0.7964 - val_loss: 0.4969 - val_accuracy: 0.8016\n",
      "Epoch 1435/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4348 - accuracy: 0.8044 - val_loss: 0.4975 - val_accuracy: 0.7935\n",
      "Epoch 1436/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4307 - accuracy: 0.8044 - val_loss: 0.5027 - val_accuracy: 0.7935\n",
      "Epoch 1437/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4311 - accuracy: 0.7924 - val_loss: 0.5025 - val_accuracy: 0.7935\n",
      "Epoch 1438/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4321 - accuracy: 0.7924 - val_loss: 0.5040 - val_accuracy: 0.7935\n",
      "Epoch 1439/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4225 - accuracy: 0.8004 - val_loss: 0.5013 - val_accuracy: 0.7895\n",
      "Epoch 1440/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4333 - accuracy: 0.7984 - val_loss: 0.5012 - val_accuracy: 0.7976\n",
      "Epoch 1441/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4253 - accuracy: 0.7984 - val_loss: 0.5011 - val_accuracy: 0.7935\n",
      "Epoch 1442/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4239 - accuracy: 0.7984 - val_loss: 0.5001 - val_accuracy: 0.7854\n",
      "Epoch 1443/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4388 - accuracy: 0.7944 - val_loss: 0.5038 - val_accuracy: 0.7935\n",
      "Epoch 1444/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4327 - accuracy: 0.7964 - val_loss: 0.5014 - val_accuracy: 0.7895\n",
      "Epoch 1445/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4267 - accuracy: 0.8124 - val_loss: 0.5026 - val_accuracy: 0.7976\n",
      "Epoch 1446/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4292 - accuracy: 0.8124 - val_loss: 0.5044 - val_accuracy: 0.7935\n",
      "Epoch 1447/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4312 - accuracy: 0.7964 - val_loss: 0.5002 - val_accuracy: 0.8016\n",
      "Epoch 1448/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4268 - accuracy: 0.7924 - val_loss: 0.5027 - val_accuracy: 0.7976\n",
      "Epoch 1449/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4254 - accuracy: 0.8004 - val_loss: 0.4969 - val_accuracy: 0.7976\n",
      "Epoch 1450/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4232 - accuracy: 0.8004 - val_loss: 0.5022 - val_accuracy: 0.7976\n",
      "Epoch 1451/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4332 - accuracy: 0.7984 - val_loss: 0.5002 - val_accuracy: 0.7976\n",
      "Epoch 1452/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4228 - accuracy: 0.8004 - val_loss: 0.4984 - val_accuracy: 0.7976\n",
      "Epoch 1453/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4310 - accuracy: 0.7924 - val_loss: 0.5033 - val_accuracy: 0.7976\n",
      "Epoch 1454/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4289 - accuracy: 0.7964 - val_loss: 0.5026 - val_accuracy: 0.7935\n",
      "Epoch 1455/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4286 - accuracy: 0.8024 - val_loss: 0.4981 - val_accuracy: 0.7935\n",
      "Epoch 1456/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4291 - accuracy: 0.7964 - val_loss: 0.5018 - val_accuracy: 0.7854\n",
      "Epoch 1457/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4311 - accuracy: 0.7964 - val_loss: 0.5010 - val_accuracy: 0.7895\n",
      "Epoch 1458/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4267 - accuracy: 0.8084 - val_loss: 0.5051 - val_accuracy: 0.7935\n",
      "Epoch 1459/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4285 - accuracy: 0.7964 - val_loss: 0.4992 - val_accuracy: 0.7935\n",
      "Epoch 1460/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4273 - accuracy: 0.7944 - val_loss: 0.5028 - val_accuracy: 0.7976\n",
      "Epoch 1461/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4273 - accuracy: 0.7944 - val_loss: 0.4996 - val_accuracy: 0.7976\n",
      "Epoch 1462/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4253 - accuracy: 0.8044 - val_loss: 0.4987 - val_accuracy: 0.7935\n",
      "Epoch 1463/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4330 - accuracy: 0.8064 - val_loss: 0.5060 - val_accuracy: 0.7935\n",
      "Epoch 1464/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4276 - accuracy: 0.7964 - val_loss: 0.5039 - val_accuracy: 0.7976\n",
      "Epoch 1465/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4295 - accuracy: 0.7964 - val_loss: 0.5035 - val_accuracy: 0.7976\n",
      "Epoch 1466/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4268 - accuracy: 0.8104 - val_loss: 0.5044 - val_accuracy: 0.7935\n",
      "Epoch 1467/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4261 - accuracy: 0.8084 - val_loss: 0.5049 - val_accuracy: 0.7935\n",
      "Epoch 1468/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4191 - accuracy: 0.8064 - val_loss: 0.5001 - val_accuracy: 0.7976\n",
      "Epoch 1469/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4257 - accuracy: 0.8004 - val_loss: 0.5036 - val_accuracy: 0.7935\n",
      "Epoch 1470/1600\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4271 - accuracy: 0.7944 - val_loss: 0.5028 - val_accuracy: 0.7935\n",
      "Epoch 1471/1600\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4246 - accuracy: 0.8084 - val_loss: 0.5027 - val_accuracy: 0.7895\n",
      "Epoch 1472/1600\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4231 - accuracy: 0.7984 - val_loss: 0.4992 - val_accuracy: 0.7935\n",
      "Epoch 1473/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4293 - accuracy: 0.8024 - val_loss: 0.5084 - val_accuracy: 0.7895\n",
      "Epoch 1474/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4351 - accuracy: 0.7964 - val_loss: 0.5045 - val_accuracy: 0.7935\n",
      "Epoch 1475/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4278 - accuracy: 0.8084 - val_loss: 0.5050 - val_accuracy: 0.8016\n",
      "Epoch 1476/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4342 - accuracy: 0.7944 - val_loss: 0.5096 - val_accuracy: 0.7935\n",
      "Epoch 1477/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4352 - accuracy: 0.7904 - val_loss: 0.5009 - val_accuracy: 0.7976\n",
      "Epoch 1478/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4226 - accuracy: 0.7944 - val_loss: 0.5053 - val_accuracy: 0.7935\n",
      "Epoch 1479/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4329 - accuracy: 0.7984 - val_loss: 0.5017 - val_accuracy: 0.7895\n",
      "Epoch 1480/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4197 - accuracy: 0.8064 - val_loss: 0.5030 - val_accuracy: 0.7895\n",
      "Epoch 1481/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4251 - accuracy: 0.8024 - val_loss: 0.5042 - val_accuracy: 0.7895\n",
      "Epoch 1482/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4255 - accuracy: 0.8024 - val_loss: 0.5009 - val_accuracy: 0.7935\n",
      "Epoch 1483/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4240 - accuracy: 0.7904 - val_loss: 0.4996 - val_accuracy: 0.7976\n",
      "Epoch 1484/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4255 - accuracy: 0.8084 - val_loss: 0.5041 - val_accuracy: 0.7976\n",
      "Epoch 1485/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4346 - accuracy: 0.7944 - val_loss: 0.4997 - val_accuracy: 0.7976\n",
      "Epoch 1486/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4239 - accuracy: 0.8144 - val_loss: 0.5074 - val_accuracy: 0.7895\n",
      "Epoch 1487/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4283 - accuracy: 0.8024 - val_loss: 0.5025 - val_accuracy: 0.8016\n",
      "Epoch 1488/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4250 - accuracy: 0.8024 - val_loss: 0.5046 - val_accuracy: 0.7895\n",
      "Epoch 1489/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4269 - accuracy: 0.7964 - val_loss: 0.5032 - val_accuracy: 0.8016\n",
      "Epoch 1490/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4265 - accuracy: 0.8064 - val_loss: 0.4985 - val_accuracy: 0.7935\n",
      "Epoch 1491/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4290 - accuracy: 0.8104 - val_loss: 0.5073 - val_accuracy: 0.7935\n",
      "Epoch 1492/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4241 - accuracy: 0.8024 - val_loss: 0.4982 - val_accuracy: 0.7976\n",
      "Epoch 1493/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4247 - accuracy: 0.7964 - val_loss: 0.4961 - val_accuracy: 0.8016\n",
      "Epoch 1494/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4333 - accuracy: 0.8044 - val_loss: 0.5065 - val_accuracy: 0.7854\n",
      "Epoch 1495/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4244 - accuracy: 0.7964 - val_loss: 0.5021 - val_accuracy: 0.7976\n",
      "Epoch 1496/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4237 - accuracy: 0.8144 - val_loss: 0.5014 - val_accuracy: 0.7895\n",
      "Epoch 1497/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4269 - accuracy: 0.8124 - val_loss: 0.5042 - val_accuracy: 0.7935\n",
      "Epoch 1498/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4227 - accuracy: 0.8004 - val_loss: 0.5071 - val_accuracy: 0.7935\n",
      "Epoch 1499/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4338 - accuracy: 0.8044 - val_loss: 0.4975 - val_accuracy: 0.7895\n",
      "Epoch 1500/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4305 - accuracy: 0.7964 - val_loss: 0.4997 - val_accuracy: 0.7935\n",
      "Epoch 1501/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4285 - accuracy: 0.7904 - val_loss: 0.5113 - val_accuracy: 0.7895\n",
      "Epoch 1502/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4341 - accuracy: 0.8064 - val_loss: 0.5012 - val_accuracy: 0.7895\n",
      "Epoch 1503/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4253 - accuracy: 0.7964 - val_loss: 0.5020 - val_accuracy: 0.7935\n",
      "Epoch 1504/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4264 - accuracy: 0.7964 - val_loss: 0.4995 - val_accuracy: 0.7935\n",
      "Epoch 1505/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4229 - accuracy: 0.8084 - val_loss: 0.4959 - val_accuracy: 0.7854\n",
      "Epoch 1506/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4284 - accuracy: 0.8064 - val_loss: 0.4964 - val_accuracy: 0.7895\n",
      "Epoch 1507/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4231 - accuracy: 0.7944 - val_loss: 0.4980 - val_accuracy: 0.7895\n",
      "Epoch 1508/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4214 - accuracy: 0.7904 - val_loss: 0.4995 - val_accuracy: 0.7895\n",
      "Epoch 1509/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4175 - accuracy: 0.8024 - val_loss: 0.4977 - val_accuracy: 0.7935\n",
      "Epoch 1510/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4334 - accuracy: 0.7964 - val_loss: 0.5015 - val_accuracy: 0.7854\n",
      "Epoch 1511/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4315 - accuracy: 0.8044 - val_loss: 0.5034 - val_accuracy: 0.7976\n",
      "Epoch 1512/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4247 - accuracy: 0.7984 - val_loss: 0.5042 - val_accuracy: 0.7895\n",
      "Epoch 1513/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4189 - accuracy: 0.8144 - val_loss: 0.5114 - val_accuracy: 0.7854\n",
      "Epoch 1514/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4300 - accuracy: 0.7984 - val_loss: 0.5040 - val_accuracy: 0.7895\n",
      "Epoch 1515/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4256 - accuracy: 0.7984 - val_loss: 0.5064 - val_accuracy: 0.7895\n",
      "Epoch 1516/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4233 - accuracy: 0.8084 - val_loss: 0.5040 - val_accuracy: 0.7976\n",
      "Epoch 1517/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4313 - accuracy: 0.8044 - val_loss: 0.4957 - val_accuracy: 0.7895\n",
      "Epoch 1518/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4226 - accuracy: 0.8064 - val_loss: 0.5119 - val_accuracy: 0.7854\n",
      "Epoch 1519/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4314 - accuracy: 0.7964 - val_loss: 0.5090 - val_accuracy: 0.7814\n",
      "Epoch 1520/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4385 - accuracy: 0.8064 - val_loss: 0.5067 - val_accuracy: 0.7814\n",
      "Epoch 1521/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4294 - accuracy: 0.8024 - val_loss: 0.5085 - val_accuracy: 0.7814\n",
      "Epoch 1522/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4267 - accuracy: 0.8144 - val_loss: 0.5035 - val_accuracy: 0.7895\n",
      "Epoch 1523/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4271 - accuracy: 0.7964 - val_loss: 0.4995 - val_accuracy: 0.7895\n",
      "Epoch 1524/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4335 - accuracy: 0.8004 - val_loss: 0.5047 - val_accuracy: 0.7935\n",
      "Epoch 1525/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4257 - accuracy: 0.8164 - val_loss: 0.5093 - val_accuracy: 0.7935\n",
      "Epoch 1526/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4225 - accuracy: 0.8044 - val_loss: 0.5034 - val_accuracy: 0.8016\n",
      "Epoch 1527/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4249 - accuracy: 0.8024 - val_loss: 0.5013 - val_accuracy: 0.7854\n",
      "Epoch 1528/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4327 - accuracy: 0.8064 - val_loss: 0.5057 - val_accuracy: 0.7935\n",
      "Epoch 1529/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4315 - accuracy: 0.7924 - val_loss: 0.5059 - val_accuracy: 0.7895\n",
      "Epoch 1530/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4229 - accuracy: 0.8024 - val_loss: 0.5040 - val_accuracy: 0.7976\n",
      "Epoch 1531/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4239 - accuracy: 0.8084 - val_loss: 0.5064 - val_accuracy: 0.7895\n",
      "Epoch 1532/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4307 - accuracy: 0.8004 - val_loss: 0.5056 - val_accuracy: 0.7935\n",
      "Epoch 1533/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4275 - accuracy: 0.8024 - val_loss: 0.4985 - val_accuracy: 0.7976\n",
      "Epoch 1534/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4144 - accuracy: 0.8104 - val_loss: 0.5053 - val_accuracy: 0.7976\n",
      "Epoch 1535/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4341 - accuracy: 0.7964 - val_loss: 0.5066 - val_accuracy: 0.7976\n",
      "Epoch 1536/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4266 - accuracy: 0.8024 - val_loss: 0.5010 - val_accuracy: 0.7935\n",
      "Epoch 1537/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4259 - accuracy: 0.8064 - val_loss: 0.5058 - val_accuracy: 0.7854\n",
      "Epoch 1538/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4263 - accuracy: 0.8044 - val_loss: 0.5070 - val_accuracy: 0.7854\n",
      "Epoch 1539/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4265 - accuracy: 0.8084 - val_loss: 0.5005 - val_accuracy: 0.7935\n",
      "Epoch 1540/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4219 - accuracy: 0.8084 - val_loss: 0.5030 - val_accuracy: 0.7814\n",
      "Epoch 1541/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4317 - accuracy: 0.8024 - val_loss: 0.5043 - val_accuracy: 0.7814\n",
      "Epoch 1542/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4299 - accuracy: 0.8064 - val_loss: 0.5017 - val_accuracy: 0.7854\n",
      "Epoch 1543/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4229 - accuracy: 0.8164 - val_loss: 0.5042 - val_accuracy: 0.7935\n",
      "Epoch 1544/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4338 - accuracy: 0.7904 - val_loss: 0.5004 - val_accuracy: 0.7895\n",
      "Epoch 1545/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4323 - accuracy: 0.8004 - val_loss: 0.5046 - val_accuracy: 0.7814\n",
      "Epoch 1546/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4284 - accuracy: 0.8044 - val_loss: 0.5026 - val_accuracy: 0.7935\n",
      "Epoch 1547/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4270 - accuracy: 0.8064 - val_loss: 0.5034 - val_accuracy: 0.7935\n",
      "Epoch 1548/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4233 - accuracy: 0.8144 - val_loss: 0.5027 - val_accuracy: 0.7935\n",
      "Epoch 1549/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4307 - accuracy: 0.8004 - val_loss: 0.5056 - val_accuracy: 0.7895\n",
      "Epoch 1550/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4274 - accuracy: 0.7924 - val_loss: 0.5048 - val_accuracy: 0.7976\n",
      "Epoch 1551/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4234 - accuracy: 0.7964 - val_loss: 0.4996 - val_accuracy: 0.7976\n",
      "Epoch 1552/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4229 - accuracy: 0.8104 - val_loss: 0.5040 - val_accuracy: 0.7935\n",
      "Epoch 1553/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4159 - accuracy: 0.8204 - val_loss: 0.5056 - val_accuracy: 0.7854\n",
      "Epoch 1554/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4262 - accuracy: 0.8084 - val_loss: 0.5049 - val_accuracy: 0.7935\n",
      "Epoch 1555/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4398 - accuracy: 0.7924 - val_loss: 0.5004 - val_accuracy: 0.7895\n",
      "Epoch 1556/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4220 - accuracy: 0.8004 - val_loss: 0.5035 - val_accuracy: 0.7935\n",
      "Epoch 1557/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4244 - accuracy: 0.8044 - val_loss: 0.4987 - val_accuracy: 0.7976\n",
      "Epoch 1558/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4322 - accuracy: 0.8064 - val_loss: 0.5040 - val_accuracy: 0.7976\n",
      "Epoch 1559/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4304 - accuracy: 0.7924 - val_loss: 0.5010 - val_accuracy: 0.7976\n",
      "Epoch 1560/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4278 - accuracy: 0.7924 - val_loss: 0.5072 - val_accuracy: 0.7854\n",
      "Epoch 1561/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4256 - accuracy: 0.8124 - val_loss: 0.5052 - val_accuracy: 0.7814\n",
      "Epoch 1562/1600\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4284 - accuracy: 0.8124 - val_loss: 0.5032 - val_accuracy: 0.7854\n",
      "Epoch 1563/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4296 - accuracy: 0.8084 - val_loss: 0.5083 - val_accuracy: 0.7895\n",
      "Epoch 1564/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4245 - accuracy: 0.7964 - val_loss: 0.5119 - val_accuracy: 0.7814\n",
      "Epoch 1565/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4261 - accuracy: 0.8184 - val_loss: 0.4966 - val_accuracy: 0.7895\n",
      "Epoch 1566/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4222 - accuracy: 0.7984 - val_loss: 0.5060 - val_accuracy: 0.7733\n",
      "Epoch 1567/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4259 - accuracy: 0.8004 - val_loss: 0.5011 - val_accuracy: 0.7854\n",
      "Epoch 1568/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4261 - accuracy: 0.8004 - val_loss: 0.5062 - val_accuracy: 0.7814\n",
      "Epoch 1569/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4219 - accuracy: 0.8044 - val_loss: 0.5069 - val_accuracy: 0.7854\n",
      "Epoch 1570/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4238 - accuracy: 0.8064 - val_loss: 0.5030 - val_accuracy: 0.7895\n",
      "Epoch 1571/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4231 - accuracy: 0.7984 - val_loss: 0.5028 - val_accuracy: 0.7976\n",
      "Epoch 1572/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4270 - accuracy: 0.8044 - val_loss: 0.5015 - val_accuracy: 0.7976\n",
      "Epoch 1573/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4289 - accuracy: 0.8084 - val_loss: 0.5044 - val_accuracy: 0.7935\n",
      "Epoch 1574/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4186 - accuracy: 0.8004 - val_loss: 0.5049 - val_accuracy: 0.7935\n",
      "Epoch 1575/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4237 - accuracy: 0.8024 - val_loss: 0.5038 - val_accuracy: 0.7895\n",
      "Epoch 1576/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4224 - accuracy: 0.8104 - val_loss: 0.5046 - val_accuracy: 0.7976\n",
      "Epoch 1577/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4331 - accuracy: 0.8064 - val_loss: 0.5030 - val_accuracy: 0.7854\n",
      "Epoch 1578/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4285 - accuracy: 0.8144 - val_loss: 0.5041 - val_accuracy: 0.7814\n",
      "Epoch 1579/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4221 - accuracy: 0.8004 - val_loss: 0.5026 - val_accuracy: 0.7854\n",
      "Epoch 1580/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4196 - accuracy: 0.8104 - val_loss: 0.5051 - val_accuracy: 0.7935\n",
      "Epoch 1581/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4134 - accuracy: 0.7984 - val_loss: 0.5039 - val_accuracy: 0.7895\n",
      "Epoch 1582/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4202 - accuracy: 0.8064 - val_loss: 0.5075 - val_accuracy: 0.7895\n",
      "Epoch 1583/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4189 - accuracy: 0.8104 - val_loss: 0.5055 - val_accuracy: 0.7814\n",
      "Epoch 1584/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4238 - accuracy: 0.8024 - val_loss: 0.5031 - val_accuracy: 0.7814\n",
      "Epoch 1585/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4300 - accuracy: 0.7884 - val_loss: 0.5077 - val_accuracy: 0.7854\n",
      "Epoch 1586/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4264 - accuracy: 0.8084 - val_loss: 0.5036 - val_accuracy: 0.7895\n",
      "Epoch 1587/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4159 - accuracy: 0.8164 - val_loss: 0.5065 - val_accuracy: 0.7814\n",
      "Epoch 1588/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4217 - accuracy: 0.8104 - val_loss: 0.5050 - val_accuracy: 0.7935\n",
      "Epoch 1589/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4231 - accuracy: 0.8104 - val_loss: 0.5039 - val_accuracy: 0.7976\n",
      "Epoch 1590/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4195 - accuracy: 0.8004 - val_loss: 0.5049 - val_accuracy: 0.7935\n",
      "Epoch 1591/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4300 - accuracy: 0.8064 - val_loss: 0.5035 - val_accuracy: 0.7854\n",
      "Epoch 1592/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4314 - accuracy: 0.7984 - val_loss: 0.5124 - val_accuracy: 0.7814\n",
      "Epoch 1593/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4199 - accuracy: 0.7984 - val_loss: 0.5010 - val_accuracy: 0.7854\n",
      "Epoch 1594/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4262 - accuracy: 0.8044 - val_loss: 0.5124 - val_accuracy: 0.7895\n",
      "Epoch 1595/1600\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4199 - accuracy: 0.8064 - val_loss: 0.5071 - val_accuracy: 0.7773\n",
      "Epoch 1596/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4279 - accuracy: 0.8124 - val_loss: 0.5080 - val_accuracy: 0.7814\n",
      "Epoch 1597/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4245 - accuracy: 0.8144 - val_loss: 0.5060 - val_accuracy: 0.7854\n",
      "Epoch 1598/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4266 - accuracy: 0.8024 - val_loss: 0.5041 - val_accuracy: 0.7773\n",
      "Epoch 1599/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4207 - accuracy: 0.8084 - val_loss: 0.5059 - val_accuracy: 0.7854\n",
      "Epoch 1600/1600\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4194 - accuracy: 0.8064 - val_loss: 0.5062 - val_accuracy: 0.7895\n"
     ]
    }
   ],
   "source": [
    "#fitting the model\n",
    "#validation data: passing in the test data\n",
    "history = model.fit(X_train,y_train,epochs=1600,validation_data = (X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.676552</td>\n",
       "      <td>0.748503</td>\n",
       "      <td>0.654318</td>\n",
       "      <td>0.744939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.638256</td>\n",
       "      <td>0.770459</td>\n",
       "      <td>0.614378</td>\n",
       "      <td>0.744939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.596349</td>\n",
       "      <td>0.770459</td>\n",
       "      <td>0.575753</td>\n",
       "      <td>0.744939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.558301</td>\n",
       "      <td>0.770459</td>\n",
       "      <td>0.550881</td>\n",
       "      <td>0.744939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.535621</td>\n",
       "      <td>0.770459</td>\n",
       "      <td>0.541099</td>\n",
       "      <td>0.744939</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss  accuracy  val_loss  val_accuracy\n",
       "0  0.676552  0.748503  0.654318      0.744939\n",
       "1  0.638256  0.770459  0.614378      0.744939\n",
       "2  0.596349  0.770459  0.575753      0.744939\n",
       "3  0.558301  0.770459  0.550881      0.744939\n",
       "4  0.535621  0.770459  0.541099      0.744939"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking the model loss\n",
    "losses = pd.DataFrame(history.history)\n",
    "losses.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x170427b2188>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plotting the loss\n",
    "plt.figure(figsize =(12,6))\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "losses['loss'].plot(label = 'Training Loss')\n",
    "losses['val_loss'].plot(label = 'Validation Loss')\n",
    "plt.legend(loc = 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice from the above plot that the model is overfitting as the validation loss is increasing as the training loss decreases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 1ms/step - loss: 0.5062 - accuracy: 0.7895\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5062319040298462, 0.7894737124443054]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#evaluating the model\n",
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making predictions\n",
    "predictions = model.predict(X_test)\n",
    "import numpy as np\n",
    "np.set_printoptions(suppress = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df = pd.DataFrame(predictions.round(3),columns = ['Prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Prediction\n",
       "0       0.086\n",
       "1       0.555\n",
       "2       0.043\n",
       "3       0.052\n",
       "4       0.100"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 247 entries, 0 to 246\n",
      "Data columns (total 1 columns):\n",
      "Prediction    247 non-null float32\n",
      "dtypes: float32(1)\n",
      "memory usage: 1.1 KB\n"
     ]
    }
   ],
   "source": [
    "predictions_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Row_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>242</td>\n",
       "      <td>0.328</td>\n",
       "      <td>242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>243</td>\n",
       "      <td>0.069</td>\n",
       "      <td>243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>244</td>\n",
       "      <td>0.196</td>\n",
       "      <td>244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>245</td>\n",
       "      <td>0.546</td>\n",
       "      <td>245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>246</td>\n",
       "      <td>0.072</td>\n",
       "      <td>246</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Prediction  Row_ID\n",
       "242       0.328     242\n",
       "243       0.069     243\n",
       "244       0.196     244\n",
       "245       0.546     245\n",
       "246       0.072     246"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#adding rows to the df\n",
    "predictions_df['Row_ID'] = range(0,247)\n",
    "predictions_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6179999709129333"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking the maximum prediction\n",
    "predictions_df['Prediction'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking the minimum prediction\n",
    "predictions_df['Prediction'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making the predictions true or false\n",
    "predictions = predictions>0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.91      0.82      0.87       204\n",
      "        True       0.43      0.63      0.51        43\n",
      "\n",
      "    accuracy                           0.79       247\n",
      "   macro avg       0.67      0.73      0.69       247\n",
      "weighted avg       0.83      0.79      0.80       247\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#checking the performance metric\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(predictions.reshape(247,),y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Keras Callbacks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Recency (months)</th>\n",
       "      <th>Frequency (times)</th>\n",
       "      <th>Monetary (c.c. blood)</th>\n",
       "      <th>Time (months)</th>\n",
       "      <th>whether he/she donated blood in March 2007</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>12500</td>\n",
       "      <td>98</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>3250</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>4000</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>5000</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>6000</td>\n",
       "      <td>77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Recency (months)  Frequency (times)  Monetary (c.c. blood)  Time (months)  \\\n",
       "0                 2                 50                  12500             98   \n",
       "1                 0                 13                   3250             28   \n",
       "2                 1                 16                   4000             35   \n",
       "3                 2                 20                   5000             45   \n",
       "4                 1                 24                   6000             77   \n",
       "\n",
       "   whether he/she donated blood in March 2007  \n",
       "0                                           1  \n",
       "1                                           1  \n",
       "2                                           1  \n",
       "3                                           1  \n",
       "4                                           0  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing the dataset\n",
    "df = pd.read_csv('transfusion.data')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data preparation - creating features and target variable\n",
    "X = df.drop(['whether he/she donated blood in March 2007'],axis = 1)\n",
    "y = df['whether he/she donated blood in March 2007']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Datapreprocessing using StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#The StandardScaler tranforms the values to have a mean of 0 and a standard deviation of 1\n",
    "sc = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transforming the data\n",
    "X_train= sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building the model\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense,Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    #passing layer- passing 20 neurons,input_dim: equal to number of features\n",
    "    Dense(20,activation = 'relu',input_dim=4),\n",
    "    Dense(15,activation = 'relu'),\n",
    "    #means dropping 20% of the layers at this point to prevent overfitting\n",
    "    Dropout(0.2),\n",
    "    #creating output layer with actibvation using relu\n",
    "    Dense(25,activation = 'relu'),\n",
    "    #using sigmoid becaose we are predicting two classes\n",
    "    Dense(1,activation=keras.activations.sigmoid)\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compiling the model\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How to stop the model when its no longer learning; improving with increasing epochs\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "#list of callback, patience: checks that the model has stopped improving after 20 epochs\n",
    "#mode: to maximize accuracy\n",
    "callbacks = [\n",
    "    EarlyStopping(patience = 20,mode = 'max',monitor='accuracy')    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/600\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.6937 - accuracy: 0.5277 - val_loss: 0.6460 - val_accuracy: 0.7333\n",
      "Epoch 2/600\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6389 - accuracy: 0.7380 - val_loss: 0.5985 - val_accuracy: 0.7467\n",
      "Epoch 3/600\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5972 - accuracy: 0.7648 - val_loss: 0.5647 - val_accuracy: 0.7467\n",
      "Epoch 4/600\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5571 - accuracy: 0.7686 - val_loss: 0.5323 - val_accuracy: 0.7467\n",
      "Epoch 5/600\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5165 - accuracy: 0.7667 - val_loss: 0.5134 - val_accuracy: 0.7467\n",
      "Epoch 6/600\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5107 - accuracy: 0.7686 - val_loss: 0.5008 - val_accuracy: 0.7467\n",
      "Epoch 7/600\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.4911 - accuracy: 0.7763 - val_loss: 0.4945 - val_accuracy: 0.7467\n",
      "Epoch 8/600\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.4898 - accuracy: 0.7859 - val_loss: 0.4873 - val_accuracy: 0.7556\n",
      "Epoch 9/600\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.4844 - accuracy: 0.7916 - val_loss: 0.4828 - val_accuracy: 0.7600\n",
      "Epoch 10/600\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.4829 - accuracy: 0.7801 - val_loss: 0.4786 - val_accuracy: 0.7644\n",
      "Epoch 11/600\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.4741 - accuracy: 0.7839 - val_loss: 0.4763 - val_accuracy: 0.7644\n",
      "Epoch 12/600\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.4805 - accuracy: 0.7878 - val_loss: 0.4764 - val_accuracy: 0.7600\n",
      "Epoch 13/600\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.4743 - accuracy: 0.7782 - val_loss: 0.4750 - val_accuracy: 0.7644\n",
      "Epoch 14/600\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.4723 - accuracy: 0.7782 - val_loss: 0.4735 - val_accuracy: 0.7644\n",
      "Epoch 15/600\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.4719 - accuracy: 0.7725 - val_loss: 0.4729 - val_accuracy: 0.7733\n",
      "Epoch 16/600\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.4722 - accuracy: 0.7744 - val_loss: 0.4707 - val_accuracy: 0.7778\n",
      "Epoch 17/600\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.4678 - accuracy: 0.7839 - val_loss: 0.4692 - val_accuracy: 0.7778\n",
      "Epoch 18/600\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.4692 - accuracy: 0.7839 - val_loss: 0.4674 - val_accuracy: 0.7822\n",
      "Epoch 19/600\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.4641 - accuracy: 0.7801 - val_loss: 0.4686 - val_accuracy: 0.7778\n",
      "Epoch 20/600\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.4716 - accuracy: 0.7839 - val_loss: 0.4674 - val_accuracy: 0.7689\n",
      "Epoch 21/600\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.4800 - accuracy: 0.7801 - val_loss: 0.4682 - val_accuracy: 0.7778\n",
      "Epoch 22/600\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.4641 - accuracy: 0.7782 - val_loss: 0.4676 - val_accuracy: 0.7778\n",
      "Epoch 23/600\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.4655 - accuracy: 0.7839 - val_loss: 0.4681 - val_accuracy: 0.7822\n",
      "Epoch 24/600\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.4714 - accuracy: 0.7839 - val_loss: 0.4679 - val_accuracy: 0.7822\n",
      "Epoch 25/600\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.4715 - accuracy: 0.7916 - val_loss: 0.4659 - val_accuracy: 0.7822\n",
      "Epoch 26/600\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.4746 - accuracy: 0.7839 - val_loss: 0.4652 - val_accuracy: 0.7822\n",
      "Epoch 27/600\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.4713 - accuracy: 0.7763 - val_loss: 0.4647 - val_accuracy: 0.7822\n",
      "Epoch 28/600\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.4639 - accuracy: 0.7839 - val_loss: 0.4656 - val_accuracy: 0.7822\n",
      "Epoch 29/600\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.4706 - accuracy: 0.7859 - val_loss: 0.4643 - val_accuracy: 0.7822\n"
     ]
    }
   ],
   "source": [
    "#fitting the model\n",
    "history = model.fit(X_train,y_train,epochs=600,validation_data=(X_test,y_test),\n",
    "                   callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.693689</td>\n",
       "      <td>0.527725</td>\n",
       "      <td>0.646032</td>\n",
       "      <td>0.733333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.638917</td>\n",
       "      <td>0.738050</td>\n",
       "      <td>0.598541</td>\n",
       "      <td>0.746667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.597234</td>\n",
       "      <td>0.764818</td>\n",
       "      <td>0.564715</td>\n",
       "      <td>0.746667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.557101</td>\n",
       "      <td>0.768642</td>\n",
       "      <td>0.532275</td>\n",
       "      <td>0.746667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.516542</td>\n",
       "      <td>0.766730</td>\n",
       "      <td>0.513408</td>\n",
       "      <td>0.746667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss  accuracy  val_loss  val_accuracy\n",
       "0  0.693689  0.527725  0.646032      0.733333\n",
       "1  0.638917  0.738050  0.598541      0.746667\n",
       "2  0.597234  0.764818  0.564715      0.746667\n",
       "3  0.557101  0.768642  0.532275      0.746667\n",
       "4  0.516542  0.766730  0.513408      0.746667"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking the model loss\n",
    "losses = pd.DataFrame(history.history)\n",
    "losses.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x17044ee6748>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plotting the loss\n",
    "plt.figure(figsize =(12,6))\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "losses['loss'].plot(label = 'Training Loss')\n",
    "losses['val_loss'].plot(label = 'Validation Loss')\n",
    "plt.legend(loc = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making predictions\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making the predictions true or false\n",
    "predictions = predictions>0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(225, 1)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(225,)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.98      0.78      0.87       211\n",
      "        True       0.19      0.79      0.31        14\n",
      "\n",
      "    accuracy                           0.78       225\n",
      "   macro avg       0.59      0.78      0.59       225\n",
      "weighted avg       0.93      0.78      0.84       225\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#checking the performance metric\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(predictions.reshape(225,),y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Custom Keras Callbacks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCallBack(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.metrics = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.metrics.append(logs.get('mse'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "custom_callback = CustomCallBack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "            custom_callback,\n",
    "            EarlyStopping(patience = 30,mode='min',monitor='val_loss')\n",
    "\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/600\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 0.4842 - accuracy: 0.7751 - val_loss: 0.3709 - val_accuracy: 0.8381\n",
      "Epoch 2/600\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.4804 - accuracy: 0.7895 - val_loss: 0.3867 - val_accuracy: 0.8381\n",
      "Epoch 3/600\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.5015 - accuracy: 0.7727 - val_loss: 0.3830 - val_accuracy: 0.8381\n",
      "Epoch 4/600\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4807 - accuracy: 0.7679 - val_loss: 0.3782 - val_accuracy: 0.8381\n",
      "Epoch 5/600\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4967 - accuracy: 0.7751 - val_loss: 0.3750 - val_accuracy: 0.8381\n",
      "Epoch 6/600\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4814 - accuracy: 0.7847 - val_loss: 0.3891 - val_accuracy: 0.8381\n",
      "Epoch 7/600\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3643 - accuracy: 0.81 - 0s 4ms/step - loss: 0.4874 - accuracy: 0.7775 - val_loss: 0.3870 - val_accuracy: 0.8381\n",
      "Epoch 8/600\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4879 - accuracy: 0.7823 - val_loss: 0.3830 - val_accuracy: 0.8381\n",
      "Epoch 9/600\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4843 - accuracy: 0.7656 - val_loss: 0.3793 - val_accuracy: 0.8381\n",
      "Epoch 10/600\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4867 - accuracy: 0.7823 - val_loss: 0.3857 - val_accuracy: 0.8381\n",
      "Epoch 11/600\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4906 - accuracy: 0.7751 - val_loss: 0.3827 - val_accuracy: 0.8381\n",
      "Epoch 12/600\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4845 - accuracy: 0.7727 - val_loss: 0.3854 - val_accuracy: 0.8381\n",
      "Epoch 13/600\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4820 - accuracy: 0.7751 - val_loss: 0.3846 - val_accuracy: 0.8381\n",
      "Epoch 14/600\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4822 - accuracy: 0.7823 - val_loss: 0.3870 - val_accuracy: 0.8381\n",
      "Epoch 15/600\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4830 - accuracy: 0.7727 - val_loss: 0.3849 - val_accuracy: 0.8381\n",
      "Epoch 16/600\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4889 - accuracy: 0.7871 - val_loss: 0.3829 - val_accuracy: 0.8381\n",
      "Epoch 17/600\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4852 - accuracy: 0.7823 - val_loss: 0.3846 - val_accuracy: 0.8381\n",
      "Epoch 18/600\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4830 - accuracy: 0.7751 - val_loss: 0.3812 - val_accuracy: 0.8381\n",
      "Epoch 19/600\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.4845 - accuracy: 0.7727 - val_loss: 0.3823 - val_accuracy: 0.8381\n",
      "Epoch 20/600\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4861 - accuracy: 0.7679 - val_loss: 0.3843 - val_accuracy: 0.8381\n",
      "Epoch 21/600\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4838 - accuracy: 0.7799 - val_loss: 0.3812 - val_accuracy: 0.8381\n",
      "Epoch 22/600\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4815 - accuracy: 0.7823 - val_loss: 0.3737 - val_accuracy: 0.8381\n",
      "Epoch 23/600\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4855 - accuracy: 0.7703 - val_loss: 0.3799 - val_accuracy: 0.8381\n",
      "Epoch 24/600\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4868 - accuracy: 0.7751 - val_loss: 0.3854 - val_accuracy: 0.8381\n",
      "Epoch 25/600\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4846 - accuracy: 0.7751 - val_loss: 0.3968 - val_accuracy: 0.8476\n",
      "Epoch 26/600\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4830 - accuracy: 0.7871 - val_loss: 0.3887 - val_accuracy: 0.8286\n",
      "Epoch 27/600\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4877 - accuracy: 0.7823 - val_loss: 0.3865 - val_accuracy: 0.8381\n",
      "Epoch 28/600\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4822 - accuracy: 0.7799 - val_loss: 0.3893 - val_accuracy: 0.8286\n",
      "Epoch 29/600\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4743 - accuracy: 0.7919 - val_loss: 0.3835 - val_accuracy: 0.8381\n",
      "Epoch 30/600\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4801 - accuracy: 0.7847 - val_loss: 0.3856 - val_accuracy: 0.8381\n",
      "Epoch 31/600\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4863 - accuracy: 0.7751 - val_loss: 0.3838 - val_accuracy: 0.8381\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train,y_train,epochs=600,validation_split = 0.2,callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0\n",
       "0  None\n",
       "1  None\n",
       "2  None\n",
       "3  None\n",
       "4  None"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(custom_callback.metrics).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TF Syntax Basics**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning DeepDive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
